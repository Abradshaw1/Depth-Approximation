{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d7cdf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abradshaw/Marigold/venv/marigold/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "TraceError",
     "evalue": "Proxy object cannot be iterated. This can be attempted when the Proxy is used in a loop or as a *args or **kwargs function argument. See the torch.fx docs on pytorch.org for a more detailed explanation of what types of control flow can be traced, and check out the Proxy docstring for help troubleshooting Proxy iteration errors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTraceError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mis_leaf_module(m, qualname)\n\u001b[1;32m     24\u001b[0m tracer \u001b[38;5;241m=\u001b[39m DiffusersTracer()\n\u001b[0;32m---> 25\u001b[0m gm \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43munet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m gm\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mprint_tabular()          \u001b[38;5;66;03m# quick visual check\u001b[39;00m\n\u001b[1;32m     28\u001b[0m gm\u001b[38;5;241m.\u001b[39mrecompile()                    \u001b[38;5;66;03m# be sure the new GraphModule runs\u001b[39;00m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:822\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[1;32m    816\u001b[0m             _autowrap_check(\n\u001b[1;32m    817\u001b[0m                 patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    818\u001b[0m             )\n\u001b[1;32m    819\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    820\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    821\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 822\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    823\u001b[0m             {},\n\u001b[1;32m    824\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    825\u001b[0m         )\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py:1108\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m   1105\u001b[0m forward_upsample_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m upsample_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m sample\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]:\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;241m%\u001b[39m default_overall_up_factor \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1110\u001b[0m         \u001b[38;5;66;03m# Forward upsample size to force interpolation output size.\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m         forward_upsample_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/fx/proxy.py:416\u001b[0m, in \u001b[0;36mProxy.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUNPACK_SEQUENCE\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(inst\u001b[38;5;241m.\u001b[39margval))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 416\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/fx/proxy.py:316\u001b[0m, in \u001b[0;36mTracerBase.iter\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;129m@compatibility\u001b[39m(is_backward_compatible\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21miter\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProxy\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator:\n\u001b[1;32m    311\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called when a proxy object is being iterated over, such as\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03m    when used in control flow.  Normally we don't know what to do because\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    we don't know the value of the proxy, but a custom tracer can attach more\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    information to the graph node using create_node and can choose to return an iterator.\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TraceError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProxy object cannot be iterated. This can be \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    317\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattempted when the Proxy is used in a loop or\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    318\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as a *args or **kwargs function argument. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    319\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the torch.fx docs on pytorch.org for a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    320\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmore detailed explanation of what types of \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    321\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontrol flow can be traced, and check out the\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    322\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Proxy docstring for help troubleshooting \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    323\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProxy iteration errors\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTraceError\u001b[0m: Proxy object cannot be iterated. This can be attempted when the Proxy is used in a loop or as a *args or **kwargs function argument. See the torch.fx docs on pytorch.org for a more detailed explanation of what types of control flow can be traced, and check out the Proxy docstring for help troubleshooting Proxy iteration errors"
     ]
    }
   ],
   "source": [
    "from diffusers import UNet2DConditionModel\n",
    "import torch, torch.fx as fx\n",
    "\n",
    "CKPT = \"prs-eth/marigold-depth-v1-1\"\n",
    "\n",
    "# 1️⃣  Disable every fused/compiled attention path\n",
    "import torch.backends.cuda as bk\n",
    "bk.enable_flash_sdp(False); bk.enable_mem_efficient_sdp(False)\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "          CKPT, subfolder=\"unet\",\n",
    "          low_cpu_mem_usage=True).cpu()\n",
    "\n",
    "# diffusers >=0.23 has an explicit helper too\n",
    "unet.disable_xformers_memory_efficient_attention()   # no-op if xformers absent :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "# 2️⃣  Optional: treat Transformer blocks as leafs so the graph stays small\n",
    "class DiffusersTracer(fx.Tracer):\n",
    "    def is_leaf_module(self, m, qualname):\n",
    "        if \"Transformer2DModel\" in m.__class__.__name__:\n",
    "            return True\n",
    "        return super().is_leaf_module(m, qualname)\n",
    "\n",
    "tracer = DiffusersTracer()\n",
    "gm = tracer.trace(unet)\n",
    "\n",
    "gm.graph.print_tabular()          # quick visual check\n",
    "gm.recompile()                    # be sure the new GraphModule runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e97f5aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.4.1+cu121\n",
      "(model: torch.fx.graph_module.GraphModule, quantizer: torch.ao.quantization.quantizer.quantizer.Quantizer) -> torch.fx.graph_module.GraphModule\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/inspect.py:1700\u001b[0m, in \u001b[0;36m_check_class\u001b[0;34m(klass, attr)\u001b[0m\n\u001b[1;32m   1699\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: '_torchdynamo_inline'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     15\u001b[0m unet\u001b[38;5;241m.\u001b[39mdisable_xformers_memory_efficient_attention()\n\u001b[1;32m     17\u001b[0m example \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m),     \u001b[38;5;66;03m# latent\u001b[39;00m\n\u001b[1;32m     18\u001b[0m            torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m]),             \u001b[38;5;66;03m# timestep\u001b[39;00m\n\u001b[1;32m     19\u001b[0m            torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m77\u001b[39m, \u001b[38;5;241m1024\u001b[39m))      \u001b[38;5;66;03m# text enc\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m gm \u001b[38;5;241m=\u001b[39m \u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43munet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m                \u001b[38;5;66;03m# records the loop as guards\u001b[39;00m\n\u001b[1;32m     22\u001b[0m qmap \u001b[38;5;241m=\u001b[39m get_default_qat_qconfig_mapping(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx86\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m gm_qat \u001b[38;5;241m=\u001b[39m prepare_qat_pt2e(gm, qmap)\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/export/__init__.py:174\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `mod` to be an instance of `torch.nn.Module`, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(mod)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m     )\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/export/_trace.py:928\u001b[0m, in \u001b[0;36m_log_export_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    927\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 928\u001b[0m     ep \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    929\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    930\u001b[0m     log_export_usage(\n\u001b[1;32m    931\u001b[0m         event\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexport.time\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    932\u001b[0m         metrics\u001b[38;5;241m=\u001b[39mend \u001b[38;5;241m-\u001b[39m start,\n\u001b[1;32m    933\u001b[0m         flags\u001b[38;5;241m=\u001b[39m_EXPORT_FLAGS,\n\u001b[1;32m    934\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mget_ep_stats(ep),\n\u001b[1;32m    935\u001b[0m     )\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/export/exported_program.py:89\u001b[0m, in \u001b[0;36m_disable_prexisiting_fake_mode.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m maybe_disable_fake_tensor_mode():\n\u001b[0;32m---> 89\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/export/_trace.py:1455\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature, pre_dispatch, _allow_complex_guards_as_runtime_asserts, _disable_forced_specializations, _is_torch_jit_trace)\u001b[0m\n\u001b[1;32m   1453\u001b[0m \u001b[38;5;66;03m# Call the appropriate export function based on the strictness of tracing.\u001b[39;00m\n\u001b[1;32m   1454\u001b[0m export_func \u001b[38;5;241m=\u001b[39m _strict_export \u001b[38;5;28;01mif\u001b[39;00m strict \u001b[38;5;28;01melse\u001b[39;00m _non_strict_export\n\u001b[0;32m-> 1455\u001b[0m aten_export_artifact \u001b[38;5;241m=\u001b[39m \u001b[43mexport_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1457\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m    \u001b[49m\u001b[43moriginal_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m    \u001b[49m\u001b[43morig_in_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_allow_complex_guards_as_runtime_asserts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_disable_forced_specializations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_torch_jit_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;66;03m# Decompose here for readability.\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m gm \u001b[38;5;241m=\u001b[39m aten_export_artifact\u001b[38;5;241m.\u001b[39mgm\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/export/_trace.py:1060\u001b[0m, in \u001b[0;36m_strict_export\u001b[0;34m(mod, args, kwargs, dynamic_shapes, preserve_module_call_signature, pre_dispatch, original_state_dict, orig_in_spec, _allow_complex_guards_as_runtime_asserts, _disable_forced_specializations, _is_torch_jit_trace)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_strict_export\u001b[39m(\n\u001b[1;32m   1048\u001b[0m     mod: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m   1049\u001b[0m     args: Tuple[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     _is_torch_jit_trace: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m   1059\u001b[0m ):\n\u001b[0;32m-> 1060\u001b[0m     gm_torch_level \u001b[38;5;241m=\u001b[39m \u001b[43m_export_to_torch_ir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrestore_fqn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# don't need to restore because we will do it later\u001b[39;49;00m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_allow_complex_guards_as_runtime_asserts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_allow_complex_guards_as_runtime_asserts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_log_export_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1071\u001b[0m     \u001b[38;5;66;03m# We detect the fake_mode by looking at gm_torch_level's placeholders, this is the fake_mode created in dynamo.\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m     (\n\u001b[1;32m   1073\u001b[0m         fake_args,\n\u001b[1;32m   1074\u001b[0m         fake_kwargs,\n\u001b[1;32m   1075\u001b[0m         fake_params_buffers,\n\u001b[1;32m   1076\u001b[0m         dynamo_fake_mode,\n\u001b[1;32m   1077\u001b[0m     ) \u001b[38;5;241m=\u001b[39m _convert_input_to_fake(gm_torch_level, args, kwargs)\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/export/_trace.py:512\u001b[0m, in \u001b[0;36m_export_to_torch_ir\u001b[0;34m(f, args, kwargs, dynamic_shapes, preserve_module_call_signature, disable_constraint_solver, _allow_complex_guards_as_runtime_asserts, restore_fqn, _log_export_usage, same_signature)\u001b[0m\n\u001b[1;32m    508\u001b[0m     module_call_specs: Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, pytree\u001b[38;5;241m.\u001b[39mTreeSpec]] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _wrap_submodules(\n\u001b[1;32m    510\u001b[0m         f, preserve_module_call_signature, module_call_specs\n\u001b[1;32m    511\u001b[0m     ), _ignore_backend_decomps():\n\u001b[0;32m--> 512\u001b[0m         gm_torch_level, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    515\u001b[0m \u001b[43m            \u001b[49m\u001b[43massume_static_by_default\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtracing_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msymbolic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_constraint_solver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_constraint_solver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# currently the following 2 flags are tied together for export purposes,\u001b[39;49;00m\n\u001b[1;32m    519\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# but untangle for sake of dynamo export api\u001b[39;49;00m\n\u001b[1;32m    520\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprefer_deferred_runtime_asserts_over_guards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_allow_complex_guards_as_runtime_asserts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_allow_complex_guards_as_runtime_asserts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_allow_complex_guards_as_runtime_asserts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_log_export_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_log_export_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m            \u001b[49m\u001b[43msame_signature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msame_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ConstraintViolationError, ValueRangeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UserError(UserErrorType\u001b[38;5;241m.\u001b[39mCONSTRAINT_VIOLATION, \u001b[38;5;28mstr\u001b[39m(e))  \u001b[38;5;66;03m# noqa: B904\u001b[39;00m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1379\u001b[0m, in \u001b[0;36mexport.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;66;03m# TODO(voz): We may have instances of `f` that mutate inputs, we should track sideeffects and reject.\u001b[39;00m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1379\u001b[0m     result_traced \u001b[38;5;241m=\u001b[39m \u001b[43mopt_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConstraintViolationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1381\u001b[0m     constraint_violation_error \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:433\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    429\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    430\u001b[0m )\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    437\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    438\u001b[0m     )\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:1116\u001b[0m, in \u001b[0;36mCatchErrorsWrapper.__call__\u001b[0;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[1;32m   1111\u001b[0m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state\n\u001b[1;32m   1112\u001b[0m             )\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:472\u001b[0m, in \u001b[0;36mConvertFrameAssert.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    458\u001b[0m compile_id \u001b[38;5;241m=\u001b[39m CompileId(frame_id, frame_compile_id)\n\u001b[1;32m    460\u001b[0m signpost_event(\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m     },\n\u001b[1;32m    470\u001b[0m )\n\u001b[0;32m--> 472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_utils_internal.py:84\u001b[0m, in \u001b[0;36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m     83\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStrobelightCompileTimeProfiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofile_compile_time\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_strobelight/compile_time_profiler.py:129\u001b[0m, in \u001b[0;36mStrobelightCompileTimeProfiler.profile_compile_time\u001b[0;34m(cls, func, phase_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprofile_compile_time\u001b[39m(\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mcls\u001b[39m, func: Any, phase_name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m--> 129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprofiler is not set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:817\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    815\u001b[0m guarded_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 817\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    820\u001b[0m     Unsupported,\n\u001b[1;32m    821\u001b[0m     TorchRuntimeError,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    828\u001b[0m     BisectValidationException,\n\u001b[1;32m    829\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/utils.py:231\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    230\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 231\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    233\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:636\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    634\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 636\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1185\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1182\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m   1183\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m-> 1185\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:178\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m cleanup \u001b[38;5;241m=\u001b[39m setup_compile_debug()\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:582\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[0;32m--> 582\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[1;32m    584\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2451\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2451\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:893\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 893\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    894\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:805\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 805\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException:\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:499\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_context_manager_depth \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1512\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION_KW\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1510\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(argnames, kwargs_list))\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(argnames)\n\u001b[0;32m-> 1512\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:743\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 743\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py:437\u001b[0m, in \u001b[0;36mNNModuleVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m istype(fn, types\u001b[38;5;241m.\u001b[39mFunctionType)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_user_function_return\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUserFunctionVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_source\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:749\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.inline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minline_user_function_return\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m    746\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;124;03m    A call to some user defined function by inlining it.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInliningInstructionTranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2666\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2663\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   2664\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minline_call\u001b[39m(\u001b[38;5;28mcls\u001b[39m, parent, func, args, kwargs):\n\u001b[1;32m   2665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mdict(counters, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munimplemented\u001b[39m\u001b[38;5;124m\"\u001b[39m: counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minline_call\u001b[39m\u001b[38;5;124m\"\u001b[39m]}):\n\u001b[0;32m-> 2666\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2782\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call_\u001b[0;34m(parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2780\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2781\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m strict_ctx:\n\u001b[0;32m-> 2782\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2783\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2784\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObserved exception DURING INLING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:893\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 893\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    894\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:805\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 805\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException:\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:499\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_context_manager_depth \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1500\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION_EX\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;66;03m# Map to a dictionary of str -> VariableTracker\u001b[39;00m\n\u001b[1;32m   1499\u001b[0m kwargsvars \u001b[38;5;241m=\u001b[39m kwargsvars\u001b[38;5;241m.\u001b[39mkeys_as_python_constant()\n\u001b[0;32m-> 1500\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margsvars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargsvars\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:743\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 743\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:344\u001b[0m, in \u001b[0;36mUserMethodVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mvalue, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invoke_and_store_as_constant(tx, fn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_name(), args, kwargs)\n\u001b[0;32m--> 344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:293\u001b[0m, in \u001b[0;36mUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_constant:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invoke_and_store_as_constant(\n\u001b[1;32m    290\u001b[0m         tx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_name(), args, kwargs\n\u001b[1;32m    291\u001b[0m     )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:90\u001b[0m, in \u001b[0;36mBaseUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_function\u001b[39m(\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m, tx, args: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mList[VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m, kwargs: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDict[str, VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariableTracker\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_user_function_return\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:749\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.inline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minline_user_function_return\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m    746\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;124;03m    A call to some user defined function by inlining it.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInliningInstructionTranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2666\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2663\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   2664\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minline_call\u001b[39m(\u001b[38;5;28mcls\u001b[39m, parent, func, args, kwargs):\n\u001b[1;32m   2665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mdict(counters, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munimplemented\u001b[39m\u001b[38;5;124m\"\u001b[39m: counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minline_call\u001b[39m\u001b[38;5;124m\"\u001b[39m]}):\n\u001b[0;32m-> 2666\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2782\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call_\u001b[0;34m(parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2780\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2781\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m strict_ctx:\n\u001b[0;32m-> 2782\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2783\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2784\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObserved exception DURING INLING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:893\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 893\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    894\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:805\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 805\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException:\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:499\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_context_manager_depth \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1512\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION_KW\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1510\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(argnames, kwargs_list))\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(argnames)\n\u001b[0;32m-> 1512\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:743\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 743\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py:437\u001b[0m, in \u001b[0;36mNNModuleVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m istype(fn, types\u001b[38;5;241m.\u001b[39mFunctionType)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_user_function_return\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUserFunctionVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_source\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:749\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.inline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minline_user_function_return\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m    746\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;124;03m    A call to some user defined function by inlining it.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInliningInstructionTranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2666\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2663\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   2664\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minline_call\u001b[39m(\u001b[38;5;28mcls\u001b[39m, parent, func, args, kwargs):\n\u001b[1;32m   2665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mdict(counters, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munimplemented\u001b[39m\u001b[38;5;124m\"\u001b[39m: counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minline_call\u001b[39m\u001b[38;5;124m\"\u001b[39m]}):\n\u001b[0;32m-> 2666\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2782\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call_\u001b[0;34m(parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2780\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2781\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m strict_ctx:\n\u001b[0;32m-> 2782\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2783\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2784\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObserved exception DURING INLING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:893\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 893\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    894\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:805\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 805\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException:\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:499\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_context_manager_depth \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1500\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION_EX\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;66;03m# Map to a dictionary of str -> VariableTracker\u001b[39;00m\n\u001b[1;32m   1499\u001b[0m kwargsvars \u001b[38;5;241m=\u001b[39m kwargsvars\u001b[38;5;241m.\u001b[39mkeys_as_python_constant()\n\u001b[0;32m-> 1500\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margsvars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargsvars\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:743\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 743\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:344\u001b[0m, in \u001b[0;36mUserMethodVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mvalue, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invoke_and_store_as_constant(tx, fn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_name(), args, kwargs)\n\u001b[0;32m--> 344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:293\u001b[0m, in \u001b[0;36mUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_constant:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invoke_and_store_as_constant(\n\u001b[1;32m    290\u001b[0m         tx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_name(), args, kwargs\n\u001b[1;32m    291\u001b[0m     )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:90\u001b[0m, in \u001b[0;36mBaseUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_function\u001b[39m(\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m, tx, args: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mList[VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m, kwargs: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDict[str, VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariableTracker\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_user_function_return\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:749\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.inline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minline_user_function_return\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m    746\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;124;03m    A call to some user defined function by inlining it.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInliningInstructionTranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2666\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2663\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   2664\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minline_call\u001b[39m(\u001b[38;5;28mcls\u001b[39m, parent, func, args, kwargs):\n\u001b[1;32m   2665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mdict(counters, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munimplemented\u001b[39m\u001b[38;5;124m\"\u001b[39m: counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minline_call\u001b[39m\u001b[38;5;124m\"\u001b[39m]}):\n\u001b[0;32m-> 2666\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2782\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call_\u001b[0;34m(parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2780\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2781\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m strict_ctx:\n\u001b[0;32m-> 2782\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2783\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2784\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObserved exception DURING INLING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:893\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 893\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    894\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:805\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 805\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException:\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:499\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_context_manager_depth \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1512\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION_KW\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1510\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(argnames, kwargs_list))\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(argnames)\n\u001b[0;32m-> 1512\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:743\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 743\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py:437\u001b[0m, in \u001b[0;36mNNModuleVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m istype(fn, types\u001b[38;5;241m.\u001b[39mFunctionType)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_user_function_return\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUserFunctionVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_source\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:749\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.inline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minline_user_function_return\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m    746\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;124;03m    A call to some user defined function by inlining it.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInliningInstructionTranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2666\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2663\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   2664\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minline_call\u001b[39m(\u001b[38;5;28mcls\u001b[39m, parent, func, args, kwargs):\n\u001b[1;32m   2665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mdict(counters, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munimplemented\u001b[39m\u001b[38;5;124m\"\u001b[39m: counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minline_call\u001b[39m\u001b[38;5;124m\"\u001b[39m]}):\n\u001b[0;32m-> 2666\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2782\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call_\u001b[0;34m(parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2780\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2781\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m strict_ctx:\n\u001b[0;32m-> 2782\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2783\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2784\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObserved exception DURING INLING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:893\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 893\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    894\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:805\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 805\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException:\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:499\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_context_manager_depth \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1500\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION_EX\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;66;03m# Map to a dictionary of str -> VariableTracker\u001b[39;00m\n\u001b[1;32m   1499\u001b[0m kwargsvars \u001b[38;5;241m=\u001b[39m kwargsvars\u001b[38;5;241m.\u001b[39mkeys_as_python_constant()\n\u001b[0;32m-> 1500\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margsvars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargsvars\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:743\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 743\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:344\u001b[0m, in \u001b[0;36mUserMethodVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mvalue, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invoke_and_store_as_constant(tx, fn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_name(), args, kwargs)\n\u001b[0;32m--> 344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:293\u001b[0m, in \u001b[0;36mUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_constant:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invoke_and_store_as_constant(\n\u001b[1;32m    290\u001b[0m         tx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_name(), args, kwargs\n\u001b[1;32m    291\u001b[0m     )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:90\u001b[0m, in \u001b[0;36mBaseUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_function\u001b[39m(\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m, tx, args: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mList[VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m, kwargs: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDict[str, VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariableTracker\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_user_function_return\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:749\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.inline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minline_user_function_return\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m    746\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;124;03m    A call to some user defined function by inlining it.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInliningInstructionTranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2666\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2663\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   2664\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minline_call\u001b[39m(\u001b[38;5;28mcls\u001b[39m, parent, func, args, kwargs):\n\u001b[1;32m   2665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mdict(counters, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munimplemented\u001b[39m\u001b[38;5;124m\"\u001b[39m: counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minline_call\u001b[39m\u001b[38;5;124m\"\u001b[39m]}):\n\u001b[0;32m-> 2666\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2782\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call_\u001b[0;34m(parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2780\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2781\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m strict_ctx:\n\u001b[0;32m-> 2782\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2783\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2784\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObserved exception DURING INLING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:893\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 893\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    894\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:805\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 805\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException:\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:499\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_context_manager_depth \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1459\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1457\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopn(inst\u001b[38;5;241m.\u001b[39margval)\n\u001b[1;32m   1458\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m-> 1459\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:743\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 743\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py:438\u001b[0m, in \u001b[0;36mNNModuleVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m istype(fn, types\u001b[38;5;241m.\u001b[39mFunctionType)\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tx\u001b[38;5;241m.\u001b[39minline_user_function_return(\n\u001b[0;32m--> 438\u001b[0m     \u001b[43mvariables\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUserFunctionVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_source\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    439\u001b[0m     args,\n\u001b[1;32m    440\u001b[0m     kwargs,\n\u001b[1;32m    441\u001b[0m )\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:138\u001b[0m, in \u001b[0;36mUserFunctionVariable.__init__\u001b[0;34m(self, fn, is_constant, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    135\u001b[0m     fn, (types\u001b[38;5;241m.\u001b[39mFunctionType, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptFunction)\n\u001b[1;32m    136\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected FunctionType found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypestr(fn)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# unpack @torch._dynamo.optimize()(fn) wrapped function\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetattr_static\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_torchdynamo_inline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# unpack torch.jit.script_if_tracing\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mgetattr_static(fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__script_if_tracing_wrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/inspect.py:1747\u001b[0m, in \u001b[0;36mgetattr_static\u001b[0;34m(obj, attr, default)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     klass \u001b[38;5;241m=\u001b[39m obj\n\u001b[0;32m-> 1747\u001b[0m klass_result \u001b[38;5;241m=\u001b[39m \u001b[43m_check_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mklass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _sentinel \u001b[38;5;129;01mand\u001b[39;00m klass_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _sentinel:\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (_check_class(\u001b[38;5;28mtype\u001b[39m(klass_result), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__get__\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _sentinel \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m         _check_class(\u001b[38;5;28mtype\u001b[39m(klass_result), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__set__\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _sentinel):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/inspect.py:1700\u001b[0m, in \u001b[0;36m_check_class\u001b[0;34m(klass, attr)\u001b[0m\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _shadowed_dict(\u001b[38;5;28mtype\u001b[39m(entry)) \u001b[38;5;129;01mis\u001b[39;00m _sentinel:\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1700\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1701\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m   1702\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import UNet2DConditionModel\n",
    "from torch.export import export\n",
    "from torch.ao.quantization.qconfig_mapping import (\n",
    "    get_default_qat_qconfig_mapping)\n",
    "from torch.ao.quantization.quantize_pt2e import (\n",
    "    prepare_qat_pt2e, convert_pt2e)\n",
    "import torch, inspect\n",
    "from torch.ao.quantization.quantize_pt2e import prepare_qat_pt2e\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(inspect.signature(prepare_qat_pt2e))\n",
    "\n",
    "CKPT = \"prs-eth/marigold-depth-v1-1\"\n",
    "unet = UNet2DConditionModel.from_pretrained(CKPT, subfolder=\"unet\").cpu()\n",
    "unet.disable_xformers_memory_efficient_attention()\n",
    "\n",
    "example = (torch.randn(1, 8, 64, 64),     # latent\n",
    "           torch.tensor([0]),             # timestep\n",
    "           torch.randn(1, 77, 1024))      # text enc\n",
    "\n",
    "gm = export(unet, example)                # records the loop as guards\n",
    "qmap = get_default_qat_qconfig_mapping(\"x86\")\n",
    "gm_qat = prepare_qat_pt2e(gm, qmap)\n",
    "# fine-tune ...\n",
    "gm_int8 = convert_pt2e(gm_qat)\n",
    "gm_int8.save(\"unet_int8.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a728be5",
   "metadata": {},
   "source": [
    "## torch export for unet graph tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd240e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abradshaw/Marigold/venv/marigold/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name                                                                     target                                                                   args                                                                                                                                                           kwargs\n",
      "-------------  -----------------------------------------------------------------------  -----------------------------------------------------------------------  -------------------------------------------------------------------------------------------------------------------------------------------------------------  ---------------------------------------------------------------------------\n",
      "placeholder    p_time_embedding_linear_1_weight                                         p_time_embedding_linear_1_weight                                         ()                                                                                                                                                             {}\n",
      "placeholder    p_time_embedding_linear_1_bias                                           p_time_embedding_linear_1_bias                                           ()                                                                                                                                                             {}\n",
      "placeholder    p_time_embedding_linear_2_weight                                         p_time_embedding_linear_2_weight                                         ()                                                                                                                                                             {}\n",
      "placeholder    p_time_embedding_linear_2_bias                                           p_time_embedding_linear_2_bias                                           ()                                                                                                                                                             {}\n",
      "placeholder    p_conv_in_weight                                                         p_conv_in_weight                                                         ()                                                                                                                                                             {}\n",
      "placeholder    p_conv_in_bias                                                           p_conv_in_bias                                                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_norm1_weight                                   p_down_blocks_0_resnets_0_norm1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_norm1_bias                                     p_down_blocks_0_resnets_0_norm1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_conv1_weight                                   p_down_blocks_0_resnets_0_conv1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_conv1_bias                                     p_down_blocks_0_resnets_0_conv1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_time_emb_proj_weight                           p_down_blocks_0_resnets_0_time_emb_proj_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_time_emb_proj_bias                             p_down_blocks_0_resnets_0_time_emb_proj_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_norm2_weight                                   p_down_blocks_0_resnets_0_norm2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_norm2_bias                                     p_down_blocks_0_resnets_0_norm2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_conv2_weight                                   p_down_blocks_0_resnets_0_conv2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_conv2_bias                                     p_down_blocks_0_resnets_0_conv2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_norm_weight                                 p_down_blocks_0_attentions_0_norm_weight                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_norm_bias                                   p_down_blocks_0_attentions_0_norm_bias                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_proj_in_weight                              p_down_blocks_0_attentions_0_proj_in_weight                              ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_proj_in_bias                                p_down_blocks_0_attentions_0_proj_in_bias                                ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_norm1_weight           p_down_blocks_0_attentions_0_transformer_blocks_0_norm1_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_norm1_bias             p_down_blocks_0_attentions_0_transformer_blocks_0_norm1_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q_weight      p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k_weight      p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v_weight      p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_weight  p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias    p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_norm2_weight           p_down_blocks_0_attentions_0_transformer_blocks_0_norm2_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_norm2_bias             p_down_blocks_0_attentions_0_transformer_blocks_0_norm2_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q_weight      p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k_weight      p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v_weight      p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_weight  p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_bias    p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_norm3_weight           p_down_blocks_0_attentions_0_transformer_blocks_0_norm3_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_norm3_bias             p_down_blocks_0_attentions_0_transformer_blocks_0_norm3_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_weight   p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_weight   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_bias     p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_bias     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_weight        p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias          p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias          ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_proj_out_weight                             p_down_blocks_0_attentions_0_proj_out_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_proj_out_bias                               p_down_blocks_0_attentions_0_proj_out_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_norm1_weight                                   p_down_blocks_0_resnets_1_norm1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_norm1_bias                                     p_down_blocks_0_resnets_1_norm1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_conv1_weight                                   p_down_blocks_0_resnets_1_conv1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_conv1_bias                                     p_down_blocks_0_resnets_1_conv1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_time_emb_proj_weight                           p_down_blocks_0_resnets_1_time_emb_proj_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_time_emb_proj_bias                             p_down_blocks_0_resnets_1_time_emb_proj_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_norm2_weight                                   p_down_blocks_0_resnets_1_norm2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_norm2_bias                                     p_down_blocks_0_resnets_1_norm2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_conv2_weight                                   p_down_blocks_0_resnets_1_conv2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_conv2_bias                                     p_down_blocks_0_resnets_1_conv2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_norm_weight                                 p_down_blocks_0_attentions_1_norm_weight                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_norm_bias                                   p_down_blocks_0_attentions_1_norm_bias                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_proj_in_weight                              p_down_blocks_0_attentions_1_proj_in_weight                              ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_proj_in_bias                                p_down_blocks_0_attentions_1_proj_in_bias                                ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_norm1_weight           p_down_blocks_0_attentions_1_transformer_blocks_0_norm1_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_norm1_bias             p_down_blocks_0_attentions_1_transformer_blocks_0_norm1_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q_weight      p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k_weight      p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v_weight      p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_weight  p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_bias    p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_norm2_weight           p_down_blocks_0_attentions_1_transformer_blocks_0_norm2_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_norm2_bias             p_down_blocks_0_attentions_1_transformer_blocks_0_norm2_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q_weight      p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k_weight      p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v_weight      p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_weight  p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_bias    p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_norm3_weight           p_down_blocks_0_attentions_1_transformer_blocks_0_norm3_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_norm3_bias             p_down_blocks_0_attentions_1_transformer_blocks_0_norm3_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_weight   p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_weight   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_bias     p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_bias     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_weight        p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_bias          p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_bias          ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_proj_out_weight                             p_down_blocks_0_attentions_1_proj_out_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_proj_out_bias                               p_down_blocks_0_attentions_1_proj_out_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_downsamplers_0_conv_weight                               p_down_blocks_0_downsamplers_0_conv_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_downsamplers_0_conv_bias                                 p_down_blocks_0_downsamplers_0_conv_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_norm1_weight                                   p_down_blocks_1_resnets_0_norm1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_norm1_bias                                     p_down_blocks_1_resnets_0_norm1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_conv1_weight                                   p_down_blocks_1_resnets_0_conv1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_conv1_bias                                     p_down_blocks_1_resnets_0_conv1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_time_emb_proj_weight                           p_down_blocks_1_resnets_0_time_emb_proj_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_time_emb_proj_bias                             p_down_blocks_1_resnets_0_time_emb_proj_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_norm2_weight                                   p_down_blocks_1_resnets_0_norm2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_norm2_bias                                     p_down_blocks_1_resnets_0_norm2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_conv2_weight                                   p_down_blocks_1_resnets_0_conv2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_conv2_bias                                     p_down_blocks_1_resnets_0_conv2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_conv_shortcut_weight                           p_down_blocks_1_resnets_0_conv_shortcut_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_conv_shortcut_bias                             p_down_blocks_1_resnets_0_conv_shortcut_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_norm_weight                                 p_down_blocks_1_attentions_0_norm_weight                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_norm_bias                                   p_down_blocks_1_attentions_0_norm_bias                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_proj_in_weight                              p_down_blocks_1_attentions_0_proj_in_weight                              ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_proj_in_bias                                p_down_blocks_1_attentions_0_proj_in_bias                                ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_norm1_weight           p_down_blocks_1_attentions_0_transformer_blocks_0_norm1_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_norm1_bias             p_down_blocks_1_attentions_0_transformer_blocks_0_norm1_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight      p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight      p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight      p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight  p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias    p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_norm2_weight           p_down_blocks_1_attentions_0_transformer_blocks_0_norm2_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_norm2_bias             p_down_blocks_1_attentions_0_transformer_blocks_0_norm2_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight      p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight      p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight      p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight  p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias    p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_norm3_weight           p_down_blocks_1_attentions_0_transformer_blocks_0_norm3_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_norm3_bias             p_down_blocks_1_attentions_0_transformer_blocks_0_norm3_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight   p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias     p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight        p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias          p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias          ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_proj_out_weight                             p_down_blocks_1_attentions_0_proj_out_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_proj_out_bias                               p_down_blocks_1_attentions_0_proj_out_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_norm1_weight                                   p_down_blocks_1_resnets_1_norm1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_norm1_bias                                     p_down_blocks_1_resnets_1_norm1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_conv1_weight                                   p_down_blocks_1_resnets_1_conv1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_conv1_bias                                     p_down_blocks_1_resnets_1_conv1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_time_emb_proj_weight                           p_down_blocks_1_resnets_1_time_emb_proj_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_time_emb_proj_bias                             p_down_blocks_1_resnets_1_time_emb_proj_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_norm2_weight                                   p_down_blocks_1_resnets_1_norm2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_norm2_bias                                     p_down_blocks_1_resnets_1_norm2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_conv2_weight                                   p_down_blocks_1_resnets_1_conv2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_conv2_bias                                     p_down_blocks_1_resnets_1_conv2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_norm_weight                                 p_down_blocks_1_attentions_1_norm_weight                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_norm_bias                                   p_down_blocks_1_attentions_1_norm_bias                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_proj_in_weight                              p_down_blocks_1_attentions_1_proj_in_weight                              ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_proj_in_bias                                p_down_blocks_1_attentions_1_proj_in_bias                                ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_norm1_weight           p_down_blocks_1_attentions_1_transformer_blocks_0_norm1_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_norm1_bias             p_down_blocks_1_attentions_1_transformer_blocks_0_norm1_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight      p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight      p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight      p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight  p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias    p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_norm2_weight           p_down_blocks_1_attentions_1_transformer_blocks_0_norm2_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_norm2_bias             p_down_blocks_1_attentions_1_transformer_blocks_0_norm2_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight      p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight      p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight      p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight  p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias    p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_norm3_weight           p_down_blocks_1_attentions_1_transformer_blocks_0_norm3_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_norm3_bias             p_down_blocks_1_attentions_1_transformer_blocks_0_norm3_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight   p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias     p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight        p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias          p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias          ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_proj_out_weight                             p_down_blocks_1_attentions_1_proj_out_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_proj_out_bias                               p_down_blocks_1_attentions_1_proj_out_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_downsamplers_0_conv_weight                               p_down_blocks_1_downsamplers_0_conv_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_downsamplers_0_conv_bias                                 p_down_blocks_1_downsamplers_0_conv_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_norm1_weight                                   p_down_blocks_2_resnets_0_norm1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_norm1_bias                                     p_down_blocks_2_resnets_0_norm1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_conv1_weight                                   p_down_blocks_2_resnets_0_conv1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_conv1_bias                                     p_down_blocks_2_resnets_0_conv1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_time_emb_proj_weight                           p_down_blocks_2_resnets_0_time_emb_proj_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_time_emb_proj_bias                             p_down_blocks_2_resnets_0_time_emb_proj_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_norm2_weight                                   p_down_blocks_2_resnets_0_norm2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_norm2_bias                                     p_down_blocks_2_resnets_0_norm2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_conv2_weight                                   p_down_blocks_2_resnets_0_conv2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_conv2_bias                                     p_down_blocks_2_resnets_0_conv2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_conv_shortcut_weight                           p_down_blocks_2_resnets_0_conv_shortcut_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_conv_shortcut_bias                             p_down_blocks_2_resnets_0_conv_shortcut_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_norm_weight                                 p_down_blocks_2_attentions_0_norm_weight                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_norm_bias                                   p_down_blocks_2_attentions_0_norm_bias                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_proj_in_weight                              p_down_blocks_2_attentions_0_proj_in_weight                              ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_proj_in_bias                                p_down_blocks_2_attentions_0_proj_in_bias                                ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_norm1_weight           p_down_blocks_2_attentions_0_transformer_blocks_0_norm1_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_norm1_bias             p_down_blocks_2_attentions_0_transformer_blocks_0_norm1_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight      p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight      p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight      p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight  p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias    p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_norm2_weight           p_down_blocks_2_attentions_0_transformer_blocks_0_norm2_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_norm2_bias             p_down_blocks_2_attentions_0_transformer_blocks_0_norm2_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight      p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight      p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight      p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight  p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias    p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_norm3_weight           p_down_blocks_2_attentions_0_transformer_blocks_0_norm3_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_norm3_bias             p_down_blocks_2_attentions_0_transformer_blocks_0_norm3_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight   p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias     p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight        p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias          p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias          ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_proj_out_weight                             p_down_blocks_2_attentions_0_proj_out_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_proj_out_bias                               p_down_blocks_2_attentions_0_proj_out_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_norm1_weight                                   p_down_blocks_2_resnets_1_norm1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_norm1_bias                                     p_down_blocks_2_resnets_1_norm1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_conv1_weight                                   p_down_blocks_2_resnets_1_conv1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_conv1_bias                                     p_down_blocks_2_resnets_1_conv1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_time_emb_proj_weight                           p_down_blocks_2_resnets_1_time_emb_proj_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_time_emb_proj_bias                             p_down_blocks_2_resnets_1_time_emb_proj_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_norm2_weight                                   p_down_blocks_2_resnets_1_norm2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_norm2_bias                                     p_down_blocks_2_resnets_1_norm2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_conv2_weight                                   p_down_blocks_2_resnets_1_conv2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_conv2_bias                                     p_down_blocks_2_resnets_1_conv2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_norm_weight                                 p_down_blocks_2_attentions_1_norm_weight                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_norm_bias                                   p_down_blocks_2_attentions_1_norm_bias                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_proj_in_weight                              p_down_blocks_2_attentions_1_proj_in_weight                              ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_proj_in_bias                                p_down_blocks_2_attentions_1_proj_in_bias                                ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_norm1_weight           p_down_blocks_2_attentions_1_transformer_blocks_0_norm1_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_norm1_bias             p_down_blocks_2_attentions_1_transformer_blocks_0_norm1_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight      p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight      p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight      p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight  p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias    p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_norm2_weight           p_down_blocks_2_attentions_1_transformer_blocks_0_norm2_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_norm2_bias             p_down_blocks_2_attentions_1_transformer_blocks_0_norm2_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight      p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight      p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight      p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight  p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias    p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_norm3_weight           p_down_blocks_2_attentions_1_transformer_blocks_0_norm3_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_norm3_bias             p_down_blocks_2_attentions_1_transformer_blocks_0_norm3_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight   p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias     p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight        p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias          p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias          ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_proj_out_weight                             p_down_blocks_2_attentions_1_proj_out_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_proj_out_bias                               p_down_blocks_2_attentions_1_proj_out_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_downsamplers_0_conv_weight                               p_down_blocks_2_downsamplers_0_conv_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_downsamplers_0_conv_bias                                 p_down_blocks_2_downsamplers_0_conv_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_norm1_weight                                   p_down_blocks_3_resnets_0_norm1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_norm1_bias                                     p_down_blocks_3_resnets_0_norm1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_conv1_weight                                   p_down_blocks_3_resnets_0_conv1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_conv1_bias                                     p_down_blocks_3_resnets_0_conv1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_time_emb_proj_weight                           p_down_blocks_3_resnets_0_time_emb_proj_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_time_emb_proj_bias                             p_down_blocks_3_resnets_0_time_emb_proj_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_norm2_weight                                   p_down_blocks_3_resnets_0_norm2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_norm2_bias                                     p_down_blocks_3_resnets_0_norm2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_conv2_weight                                   p_down_blocks_3_resnets_0_conv2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_conv2_bias                                     p_down_blocks_3_resnets_0_conv2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_norm1_weight                                   p_down_blocks_3_resnets_1_norm1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_norm1_bias                                     p_down_blocks_3_resnets_1_norm1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_conv1_weight                                   p_down_blocks_3_resnets_1_conv1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_conv1_bias                                     p_down_blocks_3_resnets_1_conv1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_time_emb_proj_weight                           p_down_blocks_3_resnets_1_time_emb_proj_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_time_emb_proj_bias                             p_down_blocks_3_resnets_1_time_emb_proj_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_norm2_weight                                   p_down_blocks_3_resnets_1_norm2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_norm2_bias                                     p_down_blocks_3_resnets_1_norm2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_conv2_weight                                   p_down_blocks_3_resnets_1_conv2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_conv2_bias                                     p_down_blocks_3_resnets_1_conv2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_norm1_weight                                       p_mid_block_resnets_0_norm1_weight                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_norm1_bias                                         p_mid_block_resnets_0_norm1_bias                                         ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_conv1_weight                                       p_mid_block_resnets_0_conv1_weight                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_conv1_bias                                         p_mid_block_resnets_0_conv1_bias                                         ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_time_emb_proj_weight                               p_mid_block_resnets_0_time_emb_proj_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_time_emb_proj_bias                                 p_mid_block_resnets_0_time_emb_proj_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_norm2_weight                                       p_mid_block_resnets_0_norm2_weight                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_norm2_bias                                         p_mid_block_resnets_0_norm2_bias                                         ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_conv2_weight                                       p_mid_block_resnets_0_conv2_weight                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_conv2_bias                                         p_mid_block_resnets_0_conv2_bias                                         ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_norm_weight                                     p_mid_block_attentions_0_norm_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_norm_bias                                       p_mid_block_attentions_0_norm_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_proj_in_weight                                  p_mid_block_attentions_0_proj_in_weight                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_proj_in_bias                                    p_mid_block_attentions_0_proj_in_bias                                    ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_norm1_weight               p_mid_block_attentions_0_transformer_blocks_0_norm1_weight               ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_norm1_bias                 p_mid_block_attentions_0_transformer_blocks_0_norm1_bias                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn1_to_q_weight          p_mid_block_attentions_0_transformer_blocks_0_attn1_to_q_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn1_to_k_weight          p_mid_block_attentions_0_transformer_blocks_0_attn1_to_k_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn1_to_v_weight          p_mid_block_attentions_0_transformer_blocks_0_attn1_to_v_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_weight      p_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias        p_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias        ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_norm2_weight               p_mid_block_attentions_0_transformer_blocks_0_norm2_weight               ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_norm2_bias                 p_mid_block_attentions_0_transformer_blocks_0_norm2_bias                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn2_to_q_weight          p_mid_block_attentions_0_transformer_blocks_0_attn2_to_q_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn2_to_k_weight          p_mid_block_attentions_0_transformer_blocks_0_attn2_to_k_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn2_to_v_weight          p_mid_block_attentions_0_transformer_blocks_0_attn2_to_v_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_weight      p_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_bias        p_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_bias        ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_norm3_weight               p_mid_block_attentions_0_transformer_blocks_0_norm3_weight               ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_norm3_bias                 p_mid_block_attentions_0_transformer_blocks_0_norm3_bias                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_weight       p_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_weight       ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_bias         p_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_bias         ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_ff_net_2_weight            p_mid_block_attentions_0_transformer_blocks_0_ff_net_2_weight            ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias              p_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias              ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_proj_out_weight                                 p_mid_block_attentions_0_proj_out_weight                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_proj_out_bias                                   p_mid_block_attentions_0_proj_out_bias                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_norm1_weight                 p_mid_block_resnets_slice_1__none__none___0_norm1_weight                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_norm1_bias                   p_mid_block_resnets_slice_1__none__none___0_norm1_bias                   ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_conv1_weight                 p_mid_block_resnets_slice_1__none__none___0_conv1_weight                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_conv1_bias                   p_mid_block_resnets_slice_1__none__none___0_conv1_bias                   ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_time_emb_proj_weight         p_mid_block_resnets_slice_1__none__none___0_time_emb_proj_weight         ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_time_emb_proj_bias           p_mid_block_resnets_slice_1__none__none___0_time_emb_proj_bias           ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_norm2_weight                 p_mid_block_resnets_slice_1__none__none___0_norm2_weight                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_norm2_bias                   p_mid_block_resnets_slice_1__none__none___0_norm2_bias                   ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_conv2_weight                 p_mid_block_resnets_slice_1__none__none___0_conv2_weight                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_conv2_bias                   p_mid_block_resnets_slice_1__none__none___0_conv2_bias                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_norm1_weight                                     p_up_blocks_0_resnets_0_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_norm1_bias                                       p_up_blocks_0_resnets_0_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_conv1_weight                                     p_up_blocks_0_resnets_0_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_conv1_bias                                       p_up_blocks_0_resnets_0_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_time_emb_proj_weight                             p_up_blocks_0_resnets_0_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_time_emb_proj_bias                               p_up_blocks_0_resnets_0_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_norm2_weight                                     p_up_blocks_0_resnets_0_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_norm2_bias                                       p_up_blocks_0_resnets_0_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_conv2_weight                                     p_up_blocks_0_resnets_0_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_conv2_bias                                       p_up_blocks_0_resnets_0_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_conv_shortcut_weight                             p_up_blocks_0_resnets_0_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_conv_shortcut_bias                               p_up_blocks_0_resnets_0_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_norm1_weight                                     p_up_blocks_0_resnets_1_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_norm1_bias                                       p_up_blocks_0_resnets_1_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_conv1_weight                                     p_up_blocks_0_resnets_1_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_conv1_bias                                       p_up_blocks_0_resnets_1_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_time_emb_proj_weight                             p_up_blocks_0_resnets_1_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_time_emb_proj_bias                               p_up_blocks_0_resnets_1_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_norm2_weight                                     p_up_blocks_0_resnets_1_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_norm2_bias                                       p_up_blocks_0_resnets_1_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_conv2_weight                                     p_up_blocks_0_resnets_1_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_conv2_bias                                       p_up_blocks_0_resnets_1_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_conv_shortcut_weight                             p_up_blocks_0_resnets_1_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_conv_shortcut_bias                               p_up_blocks_0_resnets_1_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_norm1_weight                                     p_up_blocks_0_resnets_2_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_norm1_bias                                       p_up_blocks_0_resnets_2_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_conv1_weight                                     p_up_blocks_0_resnets_2_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_conv1_bias                                       p_up_blocks_0_resnets_2_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_time_emb_proj_weight                             p_up_blocks_0_resnets_2_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_time_emb_proj_bias                               p_up_blocks_0_resnets_2_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_norm2_weight                                     p_up_blocks_0_resnets_2_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_norm2_bias                                       p_up_blocks_0_resnets_2_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_conv2_weight                                     p_up_blocks_0_resnets_2_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_conv2_bias                                       p_up_blocks_0_resnets_2_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_conv_shortcut_weight                             p_up_blocks_0_resnets_2_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_conv_shortcut_bias                               p_up_blocks_0_resnets_2_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_upsamplers_0_conv_weight                                   p_up_blocks_0_upsamplers_0_conv_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_upsamplers_0_conv_bias                                     p_up_blocks_0_upsamplers_0_conv_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_norm1_weight                                     p_up_blocks_1_resnets_0_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_norm1_bias                                       p_up_blocks_1_resnets_0_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_conv1_weight                                     p_up_blocks_1_resnets_0_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_conv1_bias                                       p_up_blocks_1_resnets_0_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_time_emb_proj_weight                             p_up_blocks_1_resnets_0_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_time_emb_proj_bias                               p_up_blocks_1_resnets_0_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_norm2_weight                                     p_up_blocks_1_resnets_0_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_norm2_bias                                       p_up_blocks_1_resnets_0_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_conv2_weight                                     p_up_blocks_1_resnets_0_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_conv2_bias                                       p_up_blocks_1_resnets_0_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_conv_shortcut_weight                             p_up_blocks_1_resnets_0_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_conv_shortcut_bias                               p_up_blocks_1_resnets_0_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_norm_weight                                   p_up_blocks_1_attentions_0_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_norm_bias                                     p_up_blocks_1_attentions_0_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_proj_in_weight                                p_up_blocks_1_attentions_0_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_proj_in_bias                                  p_up_blocks_1_attentions_0_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_norm1_weight             p_up_blocks_1_attentions_0_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_norm1_bias               p_up_blocks_1_attentions_0_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_norm2_weight             p_up_blocks_1_attentions_0_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_norm2_bias               p_up_blocks_1_attentions_0_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_norm3_weight             p_up_blocks_1_attentions_0_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_norm3_bias               p_up_blocks_1_attentions_0_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight          p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias            p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_proj_out_weight                               p_up_blocks_1_attentions_0_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_proj_out_bias                                 p_up_blocks_1_attentions_0_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_norm1_weight                                     p_up_blocks_1_resnets_1_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_norm1_bias                                       p_up_blocks_1_resnets_1_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_conv1_weight                                     p_up_blocks_1_resnets_1_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_conv1_bias                                       p_up_blocks_1_resnets_1_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_time_emb_proj_weight                             p_up_blocks_1_resnets_1_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_time_emb_proj_bias                               p_up_blocks_1_resnets_1_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_norm2_weight                                     p_up_blocks_1_resnets_1_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_norm2_bias                                       p_up_blocks_1_resnets_1_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_conv2_weight                                     p_up_blocks_1_resnets_1_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_conv2_bias                                       p_up_blocks_1_resnets_1_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_conv_shortcut_weight                             p_up_blocks_1_resnets_1_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_conv_shortcut_bias                               p_up_blocks_1_resnets_1_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_norm_weight                                   p_up_blocks_1_attentions_1_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_norm_bias                                     p_up_blocks_1_attentions_1_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_proj_in_weight                                p_up_blocks_1_attentions_1_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_proj_in_bias                                  p_up_blocks_1_attentions_1_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_norm1_weight             p_up_blocks_1_attentions_1_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_norm1_bias               p_up_blocks_1_attentions_1_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_norm2_weight             p_up_blocks_1_attentions_1_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_norm2_bias               p_up_blocks_1_attentions_1_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_norm3_weight             p_up_blocks_1_attentions_1_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_norm3_bias               p_up_blocks_1_attentions_1_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight          p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias            p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_proj_out_weight                               p_up_blocks_1_attentions_1_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_proj_out_bias                                 p_up_blocks_1_attentions_1_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_norm1_weight                                     p_up_blocks_1_resnets_2_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_norm1_bias                                       p_up_blocks_1_resnets_2_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_conv1_weight                                     p_up_blocks_1_resnets_2_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_conv1_bias                                       p_up_blocks_1_resnets_2_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_time_emb_proj_weight                             p_up_blocks_1_resnets_2_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_time_emb_proj_bias                               p_up_blocks_1_resnets_2_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_norm2_weight                                     p_up_blocks_1_resnets_2_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_norm2_bias                                       p_up_blocks_1_resnets_2_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_conv2_weight                                     p_up_blocks_1_resnets_2_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_conv2_bias                                       p_up_blocks_1_resnets_2_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_conv_shortcut_weight                             p_up_blocks_1_resnets_2_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_conv_shortcut_bias                               p_up_blocks_1_resnets_2_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_norm_weight                                   p_up_blocks_1_attentions_2_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_norm_bias                                     p_up_blocks_1_attentions_2_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_proj_in_weight                                p_up_blocks_1_attentions_2_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_proj_in_bias                                  p_up_blocks_1_attentions_2_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_norm1_weight             p_up_blocks_1_attentions_2_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_norm1_bias               p_up_blocks_1_attentions_2_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_norm2_weight             p_up_blocks_1_attentions_2_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_norm2_bias               p_up_blocks_1_attentions_2_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_norm3_weight             p_up_blocks_1_attentions_2_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_norm3_bias               p_up_blocks_1_attentions_2_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_weight          p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_bias            p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_proj_out_weight                               p_up_blocks_1_attentions_2_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_proj_out_bias                                 p_up_blocks_1_attentions_2_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_upsamplers_0_conv_weight                                   p_up_blocks_1_upsamplers_0_conv_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_upsamplers_0_conv_bias                                     p_up_blocks_1_upsamplers_0_conv_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_norm1_weight                                     p_up_blocks_2_resnets_0_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_norm1_bias                                       p_up_blocks_2_resnets_0_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_conv1_weight                                     p_up_blocks_2_resnets_0_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_conv1_bias                                       p_up_blocks_2_resnets_0_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_time_emb_proj_weight                             p_up_blocks_2_resnets_0_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_time_emb_proj_bias                               p_up_blocks_2_resnets_0_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_norm2_weight                                     p_up_blocks_2_resnets_0_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_norm2_bias                                       p_up_blocks_2_resnets_0_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_conv2_weight                                     p_up_blocks_2_resnets_0_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_conv2_bias                                       p_up_blocks_2_resnets_0_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_conv_shortcut_weight                             p_up_blocks_2_resnets_0_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_conv_shortcut_bias                               p_up_blocks_2_resnets_0_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_norm_weight                                   p_up_blocks_2_attentions_0_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_norm_bias                                     p_up_blocks_2_attentions_0_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_proj_in_weight                                p_up_blocks_2_attentions_0_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_proj_in_bias                                  p_up_blocks_2_attentions_0_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_norm1_weight             p_up_blocks_2_attentions_0_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_norm1_bias               p_up_blocks_2_attentions_0_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_norm2_weight             p_up_blocks_2_attentions_0_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_norm2_bias               p_up_blocks_2_attentions_0_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_norm3_weight             p_up_blocks_2_attentions_0_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_norm3_bias               p_up_blocks_2_attentions_0_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight          p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias            p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_proj_out_weight                               p_up_blocks_2_attentions_0_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_proj_out_bias                                 p_up_blocks_2_attentions_0_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_norm1_weight                                     p_up_blocks_2_resnets_1_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_norm1_bias                                       p_up_blocks_2_resnets_1_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_conv1_weight                                     p_up_blocks_2_resnets_1_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_conv1_bias                                       p_up_blocks_2_resnets_1_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_time_emb_proj_weight                             p_up_blocks_2_resnets_1_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_time_emb_proj_bias                               p_up_blocks_2_resnets_1_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_norm2_weight                                     p_up_blocks_2_resnets_1_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_norm2_bias                                       p_up_blocks_2_resnets_1_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_conv2_weight                                     p_up_blocks_2_resnets_1_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_conv2_bias                                       p_up_blocks_2_resnets_1_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_conv_shortcut_weight                             p_up_blocks_2_resnets_1_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_conv_shortcut_bias                               p_up_blocks_2_resnets_1_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_norm_weight                                   p_up_blocks_2_attentions_1_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_norm_bias                                     p_up_blocks_2_attentions_1_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_proj_in_weight                                p_up_blocks_2_attentions_1_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_proj_in_bias                                  p_up_blocks_2_attentions_1_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_norm1_weight             p_up_blocks_2_attentions_1_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_norm1_bias               p_up_blocks_2_attentions_1_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_norm2_weight             p_up_blocks_2_attentions_1_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_norm2_bias               p_up_blocks_2_attentions_1_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_norm3_weight             p_up_blocks_2_attentions_1_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_norm3_bias               p_up_blocks_2_attentions_1_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight          p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias            p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_proj_out_weight                               p_up_blocks_2_attentions_1_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_proj_out_bias                                 p_up_blocks_2_attentions_1_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_norm1_weight                                     p_up_blocks_2_resnets_2_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_norm1_bias                                       p_up_blocks_2_resnets_2_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_conv1_weight                                     p_up_blocks_2_resnets_2_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_conv1_bias                                       p_up_blocks_2_resnets_2_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_time_emb_proj_weight                             p_up_blocks_2_resnets_2_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_time_emb_proj_bias                               p_up_blocks_2_resnets_2_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_norm2_weight                                     p_up_blocks_2_resnets_2_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_norm2_bias                                       p_up_blocks_2_resnets_2_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_conv2_weight                                     p_up_blocks_2_resnets_2_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_conv2_bias                                       p_up_blocks_2_resnets_2_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_conv_shortcut_weight                             p_up_blocks_2_resnets_2_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_conv_shortcut_bias                               p_up_blocks_2_resnets_2_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_norm_weight                                   p_up_blocks_2_attentions_2_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_norm_bias                                     p_up_blocks_2_attentions_2_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_proj_in_weight                                p_up_blocks_2_attentions_2_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_proj_in_bias                                  p_up_blocks_2_attentions_2_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_norm1_weight             p_up_blocks_2_attentions_2_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_norm1_bias               p_up_blocks_2_attentions_2_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_norm2_weight             p_up_blocks_2_attentions_2_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_norm2_bias               p_up_blocks_2_attentions_2_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_norm3_weight             p_up_blocks_2_attentions_2_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_norm3_bias               p_up_blocks_2_attentions_2_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_weight          p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_bias            p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_proj_out_weight                               p_up_blocks_2_attentions_2_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_proj_out_bias                                 p_up_blocks_2_attentions_2_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_upsamplers_0_conv_weight                                   p_up_blocks_2_upsamplers_0_conv_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_upsamplers_0_conv_bias                                     p_up_blocks_2_upsamplers_0_conv_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_norm1_weight                                     p_up_blocks_3_resnets_0_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_norm1_bias                                       p_up_blocks_3_resnets_0_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_conv1_weight                                     p_up_blocks_3_resnets_0_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_conv1_bias                                       p_up_blocks_3_resnets_0_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_time_emb_proj_weight                             p_up_blocks_3_resnets_0_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_time_emb_proj_bias                               p_up_blocks_3_resnets_0_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_norm2_weight                                     p_up_blocks_3_resnets_0_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_norm2_bias                                       p_up_blocks_3_resnets_0_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_conv2_weight                                     p_up_blocks_3_resnets_0_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_conv2_bias                                       p_up_blocks_3_resnets_0_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_conv_shortcut_weight                             p_up_blocks_3_resnets_0_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_conv_shortcut_bias                               p_up_blocks_3_resnets_0_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_norm_weight                                   p_up_blocks_3_attentions_0_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_norm_bias                                     p_up_blocks_3_attentions_0_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_proj_in_weight                                p_up_blocks_3_attentions_0_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_proj_in_bias                                  p_up_blocks_3_attentions_0_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_norm1_weight             p_up_blocks_3_attentions_0_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_norm1_bias               p_up_blocks_3_attentions_0_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_norm2_weight             p_up_blocks_3_attentions_0_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_norm2_bias               p_up_blocks_3_attentions_0_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_norm3_weight             p_up_blocks_3_attentions_0_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_norm3_bias               p_up_blocks_3_attentions_0_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_weight          p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_bias            p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_proj_out_weight                               p_up_blocks_3_attentions_0_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_proj_out_bias                                 p_up_blocks_3_attentions_0_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_norm1_weight                                     p_up_blocks_3_resnets_1_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_norm1_bias                                       p_up_blocks_3_resnets_1_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_conv1_weight                                     p_up_blocks_3_resnets_1_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_conv1_bias                                       p_up_blocks_3_resnets_1_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_time_emb_proj_weight                             p_up_blocks_3_resnets_1_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_time_emb_proj_bias                               p_up_blocks_3_resnets_1_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_norm2_weight                                     p_up_blocks_3_resnets_1_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_norm2_bias                                       p_up_blocks_3_resnets_1_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_conv2_weight                                     p_up_blocks_3_resnets_1_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_conv2_bias                                       p_up_blocks_3_resnets_1_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_conv_shortcut_weight                             p_up_blocks_3_resnets_1_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_conv_shortcut_bias                               p_up_blocks_3_resnets_1_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_norm_weight                                   p_up_blocks_3_attentions_1_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_norm_bias                                     p_up_blocks_3_attentions_1_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_proj_in_weight                                p_up_blocks_3_attentions_1_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_proj_in_bias                                  p_up_blocks_3_attentions_1_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_norm1_weight             p_up_blocks_3_attentions_1_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_norm1_bias               p_up_blocks_3_attentions_1_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_norm2_weight             p_up_blocks_3_attentions_1_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_norm2_bias               p_up_blocks_3_attentions_1_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_norm3_weight             p_up_blocks_3_attentions_1_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_norm3_bias               p_up_blocks_3_attentions_1_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_weight          p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_bias            p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_proj_out_weight                               p_up_blocks_3_attentions_1_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_proj_out_bias                                 p_up_blocks_3_attentions_1_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_norm1_weight                                     p_up_blocks_3_resnets_2_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_norm1_bias                                       p_up_blocks_3_resnets_2_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_conv1_weight                                     p_up_blocks_3_resnets_2_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_conv1_bias                                       p_up_blocks_3_resnets_2_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_time_emb_proj_weight                             p_up_blocks_3_resnets_2_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_time_emb_proj_bias                               p_up_blocks_3_resnets_2_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_norm2_weight                                     p_up_blocks_3_resnets_2_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_norm2_bias                                       p_up_blocks_3_resnets_2_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_conv2_weight                                     p_up_blocks_3_resnets_2_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_conv2_bias                                       p_up_blocks_3_resnets_2_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_conv_shortcut_weight                             p_up_blocks_3_resnets_2_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_conv_shortcut_bias                               p_up_blocks_3_resnets_2_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_norm_weight                                   p_up_blocks_3_attentions_2_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_norm_bias                                     p_up_blocks_3_attentions_2_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_proj_in_weight                                p_up_blocks_3_attentions_2_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_proj_in_bias                                  p_up_blocks_3_attentions_2_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_norm1_weight             p_up_blocks_3_attentions_2_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_norm1_bias               p_up_blocks_3_attentions_2_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_norm2_weight             p_up_blocks_3_attentions_2_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_norm2_bias               p_up_blocks_3_attentions_2_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_norm3_weight             p_up_blocks_3_attentions_2_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_norm3_bias               p_up_blocks_3_attentions_2_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_weight          p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_bias            p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_proj_out_weight                               p_up_blocks_3_attentions_2_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_proj_out_bias                                 p_up_blocks_3_attentions_2_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_conv_norm_out_weight                                                   p_conv_norm_out_weight                                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_conv_norm_out_bias                                                     p_conv_norm_out_bias                                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_conv_out_weight                                                        p_conv_out_weight                                                        ()                                                                                                                                                             {}\n",
      "placeholder    p_conv_out_bias                                                          p_conv_out_bias                                                          ()                                                                                                                                                             {}\n",
      "placeholder    sample                                                                   sample                                                                   ()                                                                                                                                                             {}\n",
      "placeholder    timestep                                                                 timestep                                                                 ()                                                                                                                                                             {}\n",
      "placeholder    encoder_hidden_states                                                    encoder_hidden_states                                                    ()                                                                                                                                                             {}\n",
      "call_function  expand                                                                   aten.expand.default                                                      (timestep, [1])                                                                                                                                                {}\n",
      "call_function  arange                                                                   aten.arange.start                                                        (0, 160)                                                                                                                                                       {'dtype': torch.float32, 'device': device(type='cpu'), 'pin_memory': False}\n",
      "call_function  mul                                                                      aten.mul.Tensor                                                          (arange, -9.210340371976184)                                                                                                                                   {}\n",
      "call_function  div                                                                      aten.div.Tensor                                                          (mul, 160)                                                                                                                                                     {}\n",
      "call_function  exp                                                                      aten.exp.default                                                         (div,)                                                                                                                                                         {}\n",
      "call_function  slice_1                                                                  aten.slice.Tensor                                                        (expand, 0, 0, 9223372036854775807)                                                                                                                            {}\n",
      "call_function  unsqueeze                                                                aten.unsqueeze.default                                                   (slice_1, 1)                                                                                                                                                   {}\n",
      "call_function  _to_copy                                                                 aten._to_copy.default                                                    (unsqueeze,)                                                                                                                                                   {'dtype': torch.float32}\n",
      "call_function  unsqueeze_1                                                              aten.unsqueeze.default                                                   (exp, 0)                                                                                                                                                       {}\n",
      "call_function  slice_2                                                                  aten.slice.Tensor                                                        (unsqueeze_1, 1, 0, 9223372036854775807)                                                                                                                       {}\n",
      "call_function  mul_1                                                                    aten.mul.Tensor                                                          (_to_copy, slice_2)                                                                                                                                            {}\n",
      "call_function  mul_2                                                                    aten.mul.Tensor                                                          (mul_1, 1)                                                                                                                                                     {}\n",
      "call_function  sin                                                                      aten.sin.default                                                         (mul_2,)                                                                                                                                                       {}\n",
      "call_function  cos                                                                      aten.cos.default                                                         (mul_2,)                                                                                                                                                       {}\n",
      "call_function  cat                                                                      aten.cat.default                                                         ([sin, cos], -1)                                                                                                                                               {}\n",
      "call_function  slice_3                                                                  aten.slice.Tensor                                                        (cat, 0, 0, 9223372036854775807)                                                                                                                               {}\n",
      "call_function  slice_4                                                                  aten.slice.Tensor                                                        (slice_3, 1, 160, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_5                                                                  aten.slice.Tensor                                                        (cat, 0, 0, 9223372036854775807)                                                                                                                               {}\n",
      "call_function  slice_6                                                                  aten.slice.Tensor                                                        (slice_5, 1, 0, 160)                                                                                                                                           {}\n",
      "call_function  cat_1                                                                    aten.cat.default                                                         ([slice_4, slice_6], -1)                                                                                                                                       {}\n",
      "call_function  _to_copy_1                                                               aten._to_copy.default                                                    (cat_1,)                                                                                                                                                       {'dtype': torch.float32}\n",
      "call_function  linear                                                                   aten.linear.default                                                      (_to_copy_1, p_time_embedding_linear_1_weight, p_time_embedding_linear_1_bias)                                                                                 {}\n",
      "call_function  silu                                                                     aten.silu.default                                                        (linear,)                                                                                                                                                      {}\n",
      "call_function  linear_1                                                                 aten.linear.default                                                      (silu, p_time_embedding_linear_2_weight, p_time_embedding_linear_2_bias)                                                                                       {}\n",
      "call_function  conv2d                                                                   aten.conv2d.default                                                      (sample, p_conv_in_weight, p_conv_in_bias, [1, 1], [1, 1])                                                                                                     {}\n",
      "call_function  group_norm                                                               aten.group_norm.default                                                  (conv2d, 32, p_down_blocks_0_resnets_0_norm1_weight, p_down_blocks_0_resnets_0_norm1_bias)                                                                     {}\n",
      "call_function  silu_1                                                                   aten.silu.default                                                        (group_norm,)                                                                                                                                                  {}\n",
      "call_function  conv2d_1                                                                 aten.conv2d.default                                                      (silu_1, p_down_blocks_0_resnets_0_conv1_weight, p_down_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  silu_2                                                                   aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_2                                                                 aten.linear.default                                                      (silu_2, p_down_blocks_0_resnets_0_time_emb_proj_weight, p_down_blocks_0_resnets_0_time_emb_proj_bias)                                                         {}\n",
      "call_function  slice_7                                                                  aten.slice.Tensor                                                        (linear_2, 0, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  slice_8                                                                  aten.slice.Tensor                                                        (slice_7, 1, 0, 9223372036854775807)                                                                                                                           {}\n",
      "call_function  unsqueeze_2                                                              aten.unsqueeze.default                                                   (slice_8, 2)                                                                                                                                                   {}\n",
      "call_function  unsqueeze_3                                                              aten.unsqueeze.default                                                   (unsqueeze_2, 3)                                                                                                                                               {}\n",
      "call_function  add                                                                      aten.add.Tensor                                                          (conv2d_1, unsqueeze_3)                                                                                                                                        {}\n",
      "call_function  group_norm_1                                                             aten.group_norm.default                                                  (add, 32, p_down_blocks_0_resnets_0_norm2_weight, p_down_blocks_0_resnets_0_norm2_bias)                                                                        {}\n",
      "call_function  silu_3                                                                   aten.silu.default                                                        (group_norm_1,)                                                                                                                                                {}\n",
      "call_function  dropout                                                                  aten.dropout.default                                                     (silu_3, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_2                                                                 aten.conv2d.default                                                      (dropout, p_down_blocks_0_resnets_0_conv2_weight, p_down_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1])                                                        {}\n",
      "call_function  add_1                                                                    aten.add.Tensor                                                          (conv2d, conv2d_2)                                                                                                                                             {}\n",
      "call_function  div_1                                                                    aten.div.Tensor                                                          (add_1, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_2                                                             aten.group_norm.default                                                  (div_1, 32, p_down_blocks_0_attentions_0_norm_weight, p_down_blocks_0_attentions_0_norm_bias, 1e-06)                                                           {}\n",
      "call_function  permute                                                                  aten.permute.default                                                     (group_norm_2, [0, 2, 3, 1])                                                                                                                                   {}\n",
      "call_function  view                                                                     aten.view.default                                                        (permute, [1, 4096, 320])                                                                                                                                      {}\n",
      "call_function  linear_3                                                                 aten.linear.default                                                      (view, p_down_blocks_0_attentions_0_proj_in_weight, p_down_blocks_0_attentions_0_proj_in_bias)                                                                 {}\n",
      "call_function  layer_norm                                                               aten.layer_norm.default                                                  (linear_3, [320], p_down_blocks_0_attentions_0_transformer_blocks_0_norm1_weight, p_down_blocks_0_attentions_0_transformer_blocks_0_norm1_bias)                {}\n",
      "call_function  linear_4                                                                 aten.linear.default                                                      (layer_norm, p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q_weight)                                                                              {}\n",
      "call_function  linear_5                                                                 aten.linear.default                                                      (layer_norm, p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k_weight)                                                                              {}\n",
      "call_function  linear_6                                                                 aten.linear.default                                                      (layer_norm, p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v_weight)                                                                              {}\n",
      "call_function  view_1                                                                   aten.view.default                                                        (linear_4, [1, -1, 5, 64])                                                                                                                                     {}\n",
      "call_function  transpose                                                                aten.transpose.int                                                       (view_1, 1, 2)                                                                                                                                                 {}\n",
      "call_function  view_2                                                                   aten.view.default                                                        (linear_5, [1, -1, 5, 64])                                                                                                                                     {}\n",
      "call_function  transpose_1                                                              aten.transpose.int                                                       (view_2, 1, 2)                                                                                                                                                 {}\n",
      "call_function  view_3                                                                   aten.view.default                                                        (linear_6, [1, -1, 5, 64])                                                                                                                                     {}\n",
      "call_function  transpose_2                                                              aten.transpose.int                                                       (view_3, 1, 2)                                                                                                                                                 {}\n",
      "call_function  scaled_dot_product_attention                                             aten.scaled_dot_product_attention.default                                (transpose, transpose_1, transpose_2)                                                                                                                          {}\n",
      "call_function  transpose_3                                                              aten.transpose.int                                                       (scaled_dot_product_attention, 1, 2)                                                                                                                           {}\n",
      "call_function  view_4                                                                   aten.view.default                                                        (transpose_3, [1, -1, 320])                                                                                                                                    {}\n",
      "call_function  _to_copy_2                                                               aten._to_copy.default                                                    (view_4,)                                                                                                                                                      {'dtype': torch.float32}\n",
      "call_function  linear_7                                                                 aten.linear.default                                                      (_to_copy_2, p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias)   {}\n",
      "call_function  dropout_1                                                                aten.dropout.default                                                     (linear_7, 0.0, False)                                                                                                                                         {}\n",
      "call_function  div_2                                                                    aten.div.Tensor                                                          (dropout_1, 1.0)                                                                                                                                               {}\n",
      "call_function  add_2                                                                    aten.add.Tensor                                                          (div_2, linear_3)                                                                                                                                              {}\n",
      "call_function  layer_norm_1                                                             aten.layer_norm.default                                                  (add_2, [320], p_down_blocks_0_attentions_0_transformer_blocks_0_norm2_weight, p_down_blocks_0_attentions_0_transformer_blocks_0_norm2_bias)                   {}\n",
      "call_function  linear_8                                                                 aten.linear.default                                                      (layer_norm_1, p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q_weight)                                                                            {}\n",
      "call_function  linear_9                                                                 aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k_weight)                                                                   {}\n",
      "call_function  linear_10                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v_weight)                                                                   {}\n",
      "call_function  view_5                                                                   aten.view.default                                                        (linear_8, [1, -1, 5, 64])                                                                                                                                     {}\n",
      "call_function  transpose_4                                                              aten.transpose.int                                                       (view_5, 1, 2)                                                                                                                                                 {}\n",
      "call_function  view_6                                                                   aten.view.default                                                        (linear_9, [1, -1, 5, 64])                                                                                                                                     {}\n",
      "call_function  transpose_5                                                              aten.transpose.int                                                       (view_6, 1, 2)                                                                                                                                                 {}\n",
      "call_function  view_7                                                                   aten.view.default                                                        (linear_10, [1, -1, 5, 64])                                                                                                                                    {}\n",
      "call_function  transpose_6                                                              aten.transpose.int                                                       (view_7, 1, 2)                                                                                                                                                 {}\n",
      "call_function  scaled_dot_product_attention_1                                           aten.scaled_dot_product_attention.default                                (transpose_4, transpose_5, transpose_6)                                                                                                                        {}\n",
      "call_function  transpose_7                                                              aten.transpose.int                                                       (scaled_dot_product_attention_1, 1, 2)                                                                                                                         {}\n",
      "call_function  view_8                                                                   aten.view.default                                                        (transpose_7, [1, -1, 320])                                                                                                                                    {}\n",
      "call_function  _to_copy_3                                                               aten._to_copy.default                                                    (view_8,)                                                                                                                                                      {'dtype': torch.float32}\n",
      "call_function  linear_11                                                                aten.linear.default                                                      (_to_copy_3, p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_bias)   {}\n",
      "call_function  dropout_2                                                                aten.dropout.default                                                     (linear_11, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_3                                                                    aten.div.Tensor                                                          (dropout_2, 1.0)                                                                                                                                               {}\n",
      "call_function  add_3                                                                    aten.add.Tensor                                                          (div_3, add_2)                                                                                                                                                 {}\n",
      "call_function  layer_norm_2                                                             aten.layer_norm.default                                                  (add_3, [320], p_down_blocks_0_attentions_0_transformer_blocks_0_norm3_weight, p_down_blocks_0_attentions_0_transformer_blocks_0_norm3_bias)                   {}\n",
      "call_function  linear_12                                                                aten.linear.default                                                      (layer_norm_2, p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_bias)   {}\n",
      "call_function  split                                                                    aten.split.Tensor                                                        (linear_12, 1280, -1)                                                                                                                                          {}\n",
      "call_function  getitem                                                                  <built-in function getitem>                                              (split, 0)                                                                                                                                                     {}\n",
      "call_function  getitem_1                                                                <built-in function getitem>                                              (split, 1)                                                                                                                                                     {}\n",
      "call_function  gelu                                                                     aten.gelu.default                                                        (getitem_1,)                                                                                                                                                   {}\n",
      "call_function  mul_3                                                                    aten.mul.Tensor                                                          (getitem, gelu)                                                                                                                                                {}\n",
      "call_function  dropout_3                                                                aten.dropout.default                                                     (mul_3, 0.0, False)                                                                                                                                            {}\n",
      "call_function  linear_13                                                                aten.linear.default                                                      (dropout_3, p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_weight, p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias)                {}\n",
      "call_function  add_4                                                                    aten.add.Tensor                                                          (linear_13, add_3)                                                                                                                                             {}\n",
      "call_function  linear_14                                                                aten.linear.default                                                      (add_4, p_down_blocks_0_attentions_0_proj_out_weight, p_down_blocks_0_attentions_0_proj_out_bias)                                                              {}\n",
      "call_function  view_9                                                                   aten.view.default                                                        (linear_14, [1, 64, 64, 320])                                                                                                                                  {}\n",
      "call_function  permute_1                                                                aten.permute.default                                                     (view_9, [0, 3, 1, 2])                                                                                                                                         {}\n",
      "call_function  clone                                                                    aten.clone.default                                                       (permute_1,)                                                                                                                                                   {'memory_format': torch.contiguous_format}\n",
      "call_function  add_5                                                                    aten.add.Tensor                                                          (clone, div_1)                                                                                                                                                 {}\n",
      "call_function  group_norm_3                                                             aten.group_norm.default                                                  (add_5, 32, p_down_blocks_0_resnets_1_norm1_weight, p_down_blocks_0_resnets_1_norm1_bias)                                                                      {}\n",
      "call_function  silu_4                                                                   aten.silu.default                                                        (group_norm_3,)                                                                                                                                                {}\n",
      "call_function  conv2d_3                                                                 aten.conv2d.default                                                      (silu_4, p_down_blocks_0_resnets_1_conv1_weight, p_down_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  silu_5                                                                   aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_15                                                                aten.linear.default                                                      (silu_5, p_down_blocks_0_resnets_1_time_emb_proj_weight, p_down_blocks_0_resnets_1_time_emb_proj_bias)                                                         {}\n",
      "call_function  slice_9                                                                  aten.slice.Tensor                                                        (linear_15, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_10                                                                 aten.slice.Tensor                                                        (slice_9, 1, 0, 9223372036854775807)                                                                                                                           {}\n",
      "call_function  unsqueeze_4                                                              aten.unsqueeze.default                                                   (slice_10, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_5                                                              aten.unsqueeze.default                                                   (unsqueeze_4, 3)                                                                                                                                               {}\n",
      "call_function  add_6                                                                    aten.add.Tensor                                                          (conv2d_3, unsqueeze_5)                                                                                                                                        {}\n",
      "call_function  group_norm_4                                                             aten.group_norm.default                                                  (add_6, 32, p_down_blocks_0_resnets_1_norm2_weight, p_down_blocks_0_resnets_1_norm2_bias)                                                                      {}\n",
      "call_function  silu_6                                                                   aten.silu.default                                                        (group_norm_4,)                                                                                                                                                {}\n",
      "call_function  dropout_4                                                                aten.dropout.default                                                     (silu_6, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_4                                                                 aten.conv2d.default                                                      (dropout_4, p_down_blocks_0_resnets_1_conv2_weight, p_down_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1])                                                      {}\n",
      "call_function  add_7                                                                    aten.add.Tensor                                                          (add_5, conv2d_4)                                                                                                                                              {}\n",
      "call_function  div_4                                                                    aten.div.Tensor                                                          (add_7, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_5                                                             aten.group_norm.default                                                  (div_4, 32, p_down_blocks_0_attentions_1_norm_weight, p_down_blocks_0_attentions_1_norm_bias, 1e-06)                                                           {}\n",
      "call_function  permute_2                                                                aten.permute.default                                                     (group_norm_5, [0, 2, 3, 1])                                                                                                                                   {}\n",
      "call_function  view_10                                                                  aten.view.default                                                        (permute_2, [1, 4096, 320])                                                                                                                                    {}\n",
      "call_function  linear_16                                                                aten.linear.default                                                      (view_10, p_down_blocks_0_attentions_1_proj_in_weight, p_down_blocks_0_attentions_1_proj_in_bias)                                                              {}\n",
      "call_function  layer_norm_3                                                             aten.layer_norm.default                                                  (linear_16, [320], p_down_blocks_0_attentions_1_transformer_blocks_0_norm1_weight, p_down_blocks_0_attentions_1_transformer_blocks_0_norm1_bias)               {}\n",
      "call_function  linear_17                                                                aten.linear.default                                                      (layer_norm_3, p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q_weight)                                                                            {}\n",
      "call_function  linear_18                                                                aten.linear.default                                                      (layer_norm_3, p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k_weight)                                                                            {}\n",
      "call_function  linear_19                                                                aten.linear.default                                                      (layer_norm_3, p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v_weight)                                                                            {}\n",
      "call_function  view_11                                                                  aten.view.default                                                        (linear_17, [1, -1, 5, 64])                                                                                                                                    {}\n",
      "call_function  transpose_8                                                              aten.transpose.int                                                       (view_11, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_12                                                                  aten.view.default                                                        (linear_18, [1, -1, 5, 64])                                                                                                                                    {}\n",
      "call_function  transpose_9                                                              aten.transpose.int                                                       (view_12, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_13                                                                  aten.view.default                                                        (linear_19, [1, -1, 5, 64])                                                                                                                                    {}\n",
      "call_function  transpose_10                                                             aten.transpose.int                                                       (view_13, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_2                                           aten.scaled_dot_product_attention.default                                (transpose_8, transpose_9, transpose_10)                                                                                                                       {}\n",
      "call_function  transpose_11                                                             aten.transpose.int                                                       (scaled_dot_product_attention_2, 1, 2)                                                                                                                         {}\n",
      "call_function  view_14                                                                  aten.view.default                                                        (transpose_11, [1, -1, 320])                                                                                                                                   {}\n",
      "call_function  _to_copy_4                                                               aten._to_copy.default                                                    (view_14,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_20                                                                aten.linear.default                                                      (_to_copy_4, p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_bias)   {}\n",
      "call_function  dropout_5                                                                aten.dropout.default                                                     (linear_20, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_5                                                                    aten.div.Tensor                                                          (dropout_5, 1.0)                                                                                                                                               {}\n",
      "call_function  add_8                                                                    aten.add.Tensor                                                          (div_5, linear_16)                                                                                                                                             {}\n",
      "call_function  layer_norm_4                                                             aten.layer_norm.default                                                  (add_8, [320], p_down_blocks_0_attentions_1_transformer_blocks_0_norm2_weight, p_down_blocks_0_attentions_1_transformer_blocks_0_norm2_bias)                   {}\n",
      "call_function  linear_21                                                                aten.linear.default                                                      (layer_norm_4, p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q_weight)                                                                            {}\n",
      "call_function  linear_22                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k_weight)                                                                   {}\n",
      "call_function  linear_23                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v_weight)                                                                   {}\n",
      "call_function  view_15                                                                  aten.view.default                                                        (linear_21, [1, -1, 5, 64])                                                                                                                                    {}\n",
      "call_function  transpose_12                                                             aten.transpose.int                                                       (view_15, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_16                                                                  aten.view.default                                                        (linear_22, [1, -1, 5, 64])                                                                                                                                    {}\n",
      "call_function  transpose_13                                                             aten.transpose.int                                                       (view_16, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_17                                                                  aten.view.default                                                        (linear_23, [1, -1, 5, 64])                                                                                                                                    {}\n",
      "call_function  transpose_14                                                             aten.transpose.int                                                       (view_17, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_3                                           aten.scaled_dot_product_attention.default                                (transpose_12, transpose_13, transpose_14)                                                                                                                     {}\n",
      "call_function  transpose_15                                                             aten.transpose.int                                                       (scaled_dot_product_attention_3, 1, 2)                                                                                                                         {}\n",
      "call_function  view_18                                                                  aten.view.default                                                        (transpose_15, [1, -1, 320])                                                                                                                                   {}\n",
      "call_function  _to_copy_5                                                               aten._to_copy.default                                                    (view_18,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_24                                                                aten.linear.default                                                      (_to_copy_5, p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_bias)   {}\n",
      "call_function  dropout_6                                                                aten.dropout.default                                                     (linear_24, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_6                                                                    aten.div.Tensor                                                          (dropout_6, 1.0)                                                                                                                                               {}\n",
      "call_function  add_9                                                                    aten.add.Tensor                                                          (div_6, add_8)                                                                                                                                                 {}\n",
      "call_function  layer_norm_5                                                             aten.layer_norm.default                                                  (add_9, [320], p_down_blocks_0_attentions_1_transformer_blocks_0_norm3_weight, p_down_blocks_0_attentions_1_transformer_blocks_0_norm3_bias)                   {}\n",
      "call_function  linear_25                                                                aten.linear.default                                                      (layer_norm_5, p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_bias)   {}\n",
      "call_function  split_1                                                                  aten.split.Tensor                                                        (linear_25, 1280, -1)                                                                                                                                          {}\n",
      "call_function  getitem_2                                                                <built-in function getitem>                                              (split_1, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_3                                                                <built-in function getitem>                                              (split_1, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_1                                                                   aten.gelu.default                                                        (getitem_3,)                                                                                                                                                   {}\n",
      "call_function  mul_4                                                                    aten.mul.Tensor                                                          (getitem_2, gelu_1)                                                                                                                                            {}\n",
      "call_function  dropout_7                                                                aten.dropout.default                                                     (mul_4, 0.0, False)                                                                                                                                            {}\n",
      "call_function  linear_26                                                                aten.linear.default                                                      (dropout_7, p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_weight, p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_bias)                {}\n",
      "call_function  add_10                                                                   aten.add.Tensor                                                          (linear_26, add_9)                                                                                                                                             {}\n",
      "call_function  linear_27                                                                aten.linear.default                                                      (add_10, p_down_blocks_0_attentions_1_proj_out_weight, p_down_blocks_0_attentions_1_proj_out_bias)                                                             {}\n",
      "call_function  view_19                                                                  aten.view.default                                                        (linear_27, [1, 64, 64, 320])                                                                                                                                  {}\n",
      "call_function  permute_3                                                                aten.permute.default                                                     (view_19, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_1                                                                  aten.clone.default                                                       (permute_3,)                                                                                                                                                   {'memory_format': torch.contiguous_format}\n",
      "call_function  add_11                                                                   aten.add.Tensor                                                          (clone_1, div_4)                                                                                                                                               {}\n",
      "call_function  conv2d_5                                                                 aten.conv2d.default                                                      (add_11, p_down_blocks_0_downsamplers_0_conv_weight, p_down_blocks_0_downsamplers_0_conv_bias, [2, 2], [1, 1])                                                 {}\n",
      "call_function  group_norm_6                                                             aten.group_norm.default                                                  (conv2d_5, 32, p_down_blocks_1_resnets_0_norm1_weight, p_down_blocks_1_resnets_0_norm1_bias)                                                                   {}\n",
      "call_function  silu_7                                                                   aten.silu.default                                                        (group_norm_6,)                                                                                                                                                {}\n",
      "call_function  conv2d_6                                                                 aten.conv2d.default                                                      (silu_7, p_down_blocks_1_resnets_0_conv1_weight, p_down_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  silu_8                                                                   aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_28                                                                aten.linear.default                                                      (silu_8, p_down_blocks_1_resnets_0_time_emb_proj_weight, p_down_blocks_1_resnets_0_time_emb_proj_bias)                                                         {}\n",
      "call_function  slice_11                                                                 aten.slice.Tensor                                                        (linear_28, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_12                                                                 aten.slice.Tensor                                                        (slice_11, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_6                                                              aten.unsqueeze.default                                                   (slice_12, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_7                                                              aten.unsqueeze.default                                                   (unsqueeze_6, 3)                                                                                                                                               {}\n",
      "call_function  add_12                                                                   aten.add.Tensor                                                          (conv2d_6, unsqueeze_7)                                                                                                                                        {}\n",
      "call_function  group_norm_7                                                             aten.group_norm.default                                                  (add_12, 32, p_down_blocks_1_resnets_0_norm2_weight, p_down_blocks_1_resnets_0_norm2_bias)                                                                     {}\n",
      "call_function  silu_9                                                                   aten.silu.default                                                        (group_norm_7,)                                                                                                                                                {}\n",
      "call_function  dropout_8                                                                aten.dropout.default                                                     (silu_9, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_7                                                                 aten.conv2d.default                                                      (dropout_8, p_down_blocks_1_resnets_0_conv2_weight, p_down_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1])                                                      {}\n",
      "call_function  conv2d_8                                                                 aten.conv2d.default                                                      (conv2d_5, p_down_blocks_1_resnets_0_conv_shortcut_weight, p_down_blocks_1_resnets_0_conv_shortcut_bias)                                                       {}\n",
      "call_function  add_13                                                                   aten.add.Tensor                                                          (conv2d_8, conv2d_7)                                                                                                                                           {}\n",
      "call_function  div_7                                                                    aten.div.Tensor                                                          (add_13, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_8                                                             aten.group_norm.default                                                  (div_7, 32, p_down_blocks_1_attentions_0_norm_weight, p_down_blocks_1_attentions_0_norm_bias, 1e-06)                                                           {}\n",
      "call_function  permute_4                                                                aten.permute.default                                                     (group_norm_8, [0, 2, 3, 1])                                                                                                                                   {}\n",
      "call_function  view_20                                                                  aten.view.default                                                        (permute_4, [1, 1024, 640])                                                                                                                                    {}\n",
      "call_function  linear_29                                                                aten.linear.default                                                      (view_20, p_down_blocks_1_attentions_0_proj_in_weight, p_down_blocks_1_attentions_0_proj_in_bias)                                                              {}\n",
      "call_function  layer_norm_6                                                             aten.layer_norm.default                                                  (linear_29, [640], p_down_blocks_1_attentions_0_transformer_blocks_0_norm1_weight, p_down_blocks_1_attentions_0_transformer_blocks_0_norm1_bias)               {}\n",
      "call_function  linear_30                                                                aten.linear.default                                                      (layer_norm_6, p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight)                                                                            {}\n",
      "call_function  linear_31                                                                aten.linear.default                                                      (layer_norm_6, p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight)                                                                            {}\n",
      "call_function  linear_32                                                                aten.linear.default                                                      (layer_norm_6, p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight)                                                                            {}\n",
      "call_function  view_21                                                                  aten.view.default                                                        (linear_30, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_16                                                             aten.transpose.int                                                       (view_21, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_22                                                                  aten.view.default                                                        (linear_31, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_17                                                             aten.transpose.int                                                       (view_22, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_23                                                                  aten.view.default                                                        (linear_32, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_18                                                             aten.transpose.int                                                       (view_23, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_4                                           aten.scaled_dot_product_attention.default                                (transpose_16, transpose_17, transpose_18)                                                                                                                     {}\n",
      "call_function  transpose_19                                                             aten.transpose.int                                                       (scaled_dot_product_attention_4, 1, 2)                                                                                                                         {}\n",
      "call_function  view_24                                                                  aten.view.default                                                        (transpose_19, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_6                                                               aten._to_copy.default                                                    (view_24,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_33                                                                aten.linear.default                                                      (_to_copy_6, p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias)   {}\n",
      "call_function  dropout_9                                                                aten.dropout.default                                                     (linear_33, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_8                                                                    aten.div.Tensor                                                          (dropout_9, 1.0)                                                                                                                                               {}\n",
      "call_function  add_14                                                                   aten.add.Tensor                                                          (div_8, linear_29)                                                                                                                                             {}\n",
      "call_function  layer_norm_7                                                             aten.layer_norm.default                                                  (add_14, [640], p_down_blocks_1_attentions_0_transformer_blocks_0_norm2_weight, p_down_blocks_1_attentions_0_transformer_blocks_0_norm2_bias)                  {}\n",
      "call_function  linear_34                                                                aten.linear.default                                                      (layer_norm_7, p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight)                                                                            {}\n",
      "call_function  linear_35                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight)                                                                   {}\n",
      "call_function  linear_36                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight)                                                                   {}\n",
      "call_function  view_25                                                                  aten.view.default                                                        (linear_34, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_20                                                             aten.transpose.int                                                       (view_25, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_26                                                                  aten.view.default                                                        (linear_35, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_21                                                             aten.transpose.int                                                       (view_26, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_27                                                                  aten.view.default                                                        (linear_36, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_22                                                             aten.transpose.int                                                       (view_27, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_5                                           aten.scaled_dot_product_attention.default                                (transpose_20, transpose_21, transpose_22)                                                                                                                     {}\n",
      "call_function  transpose_23                                                             aten.transpose.int                                                       (scaled_dot_product_attention_5, 1, 2)                                                                                                                         {}\n",
      "call_function  view_28                                                                  aten.view.default                                                        (transpose_23, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_7                                                               aten._to_copy.default                                                    (view_28,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_37                                                                aten.linear.default                                                      (_to_copy_7, p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias)   {}\n",
      "call_function  dropout_10                                                               aten.dropout.default                                                     (linear_37, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_9                                                                    aten.div.Tensor                                                          (dropout_10, 1.0)                                                                                                                                              {}\n",
      "call_function  add_15                                                                   aten.add.Tensor                                                          (div_9, add_14)                                                                                                                                                {}\n",
      "call_function  layer_norm_8                                                             aten.layer_norm.default                                                  (add_15, [640], p_down_blocks_1_attentions_0_transformer_blocks_0_norm3_weight, p_down_blocks_1_attentions_0_transformer_blocks_0_norm3_bias)                  {}\n",
      "call_function  linear_38                                                                aten.linear.default                                                      (layer_norm_8, p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias)   {}\n",
      "call_function  split_2                                                                  aten.split.Tensor                                                        (linear_38, 2560, -1)                                                                                                                                          {}\n",
      "call_function  getitem_4                                                                <built-in function getitem>                                              (split_2, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_5                                                                <built-in function getitem>                                              (split_2, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_2                                                                   aten.gelu.default                                                        (getitem_5,)                                                                                                                                                   {}\n",
      "call_function  mul_5                                                                    aten.mul.Tensor                                                          (getitem_4, gelu_2)                                                                                                                                            {}\n",
      "call_function  dropout_11                                                               aten.dropout.default                                                     (mul_5, 0.0, False)                                                                                                                                            {}\n",
      "call_function  linear_39                                                                aten.linear.default                                                      (dropout_11, p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight, p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias)               {}\n",
      "call_function  add_16                                                                   aten.add.Tensor                                                          (linear_39, add_15)                                                                                                                                            {}\n",
      "call_function  linear_40                                                                aten.linear.default                                                      (add_16, p_down_blocks_1_attentions_0_proj_out_weight, p_down_blocks_1_attentions_0_proj_out_bias)                                                             {}\n",
      "call_function  view_29                                                                  aten.view.default                                                        (linear_40, [1, 32, 32, 640])                                                                                                                                  {}\n",
      "call_function  permute_5                                                                aten.permute.default                                                     (view_29, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_2                                                                  aten.clone.default                                                       (permute_5,)                                                                                                                                                   {'memory_format': torch.contiguous_format}\n",
      "call_function  add_17                                                                   aten.add.Tensor                                                          (clone_2, div_7)                                                                                                                                               {}\n",
      "call_function  group_norm_9                                                             aten.group_norm.default                                                  (add_17, 32, p_down_blocks_1_resnets_1_norm1_weight, p_down_blocks_1_resnets_1_norm1_bias)                                                                     {}\n",
      "call_function  silu_10                                                                  aten.silu.default                                                        (group_norm_9,)                                                                                                                                                {}\n",
      "call_function  conv2d_9                                                                 aten.conv2d.default                                                      (silu_10, p_down_blocks_1_resnets_1_conv1_weight, p_down_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1])                                                        {}\n",
      "call_function  silu_11                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_41                                                                aten.linear.default                                                      (silu_11, p_down_blocks_1_resnets_1_time_emb_proj_weight, p_down_blocks_1_resnets_1_time_emb_proj_bias)                                                        {}\n",
      "call_function  slice_13                                                                 aten.slice.Tensor                                                        (linear_41, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_14                                                                 aten.slice.Tensor                                                        (slice_13, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_8                                                              aten.unsqueeze.default                                                   (slice_14, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_9                                                              aten.unsqueeze.default                                                   (unsqueeze_8, 3)                                                                                                                                               {}\n",
      "call_function  add_18                                                                   aten.add.Tensor                                                          (conv2d_9, unsqueeze_9)                                                                                                                                        {}\n",
      "call_function  group_norm_10                                                            aten.group_norm.default                                                  (add_18, 32, p_down_blocks_1_resnets_1_norm2_weight, p_down_blocks_1_resnets_1_norm2_bias)                                                                     {}\n",
      "call_function  silu_12                                                                  aten.silu.default                                                        (group_norm_10,)                                                                                                                                               {}\n",
      "call_function  dropout_12                                                               aten.dropout.default                                                     (silu_12, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_10                                                                aten.conv2d.default                                                      (dropout_12, p_down_blocks_1_resnets_1_conv2_weight, p_down_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1])                                                     {}\n",
      "call_function  add_19                                                                   aten.add.Tensor                                                          (add_17, conv2d_10)                                                                                                                                            {}\n",
      "call_function  div_10                                                                   aten.div.Tensor                                                          (add_19, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_11                                                            aten.group_norm.default                                                  (div_10, 32, p_down_blocks_1_attentions_1_norm_weight, p_down_blocks_1_attentions_1_norm_bias, 1e-06)                                                          {}\n",
      "call_function  permute_6                                                                aten.permute.default                                                     (group_norm_11, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_30                                                                  aten.view.default                                                        (permute_6, [1, 1024, 640])                                                                                                                                    {}\n",
      "call_function  linear_42                                                                aten.linear.default                                                      (view_30, p_down_blocks_1_attentions_1_proj_in_weight, p_down_blocks_1_attentions_1_proj_in_bias)                                                              {}\n",
      "call_function  layer_norm_9                                                             aten.layer_norm.default                                                  (linear_42, [640], p_down_blocks_1_attentions_1_transformer_blocks_0_norm1_weight, p_down_blocks_1_attentions_1_transformer_blocks_0_norm1_bias)               {}\n",
      "call_function  linear_43                                                                aten.linear.default                                                      (layer_norm_9, p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight)                                                                            {}\n",
      "call_function  linear_44                                                                aten.linear.default                                                      (layer_norm_9, p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight)                                                                            {}\n",
      "call_function  linear_45                                                                aten.linear.default                                                      (layer_norm_9, p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight)                                                                            {}\n",
      "call_function  view_31                                                                  aten.view.default                                                        (linear_43, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_24                                                             aten.transpose.int                                                       (view_31, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_32                                                                  aten.view.default                                                        (linear_44, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_25                                                             aten.transpose.int                                                       (view_32, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_33                                                                  aten.view.default                                                        (linear_45, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_26                                                             aten.transpose.int                                                       (view_33, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_6                                           aten.scaled_dot_product_attention.default                                (transpose_24, transpose_25, transpose_26)                                                                                                                     {}\n",
      "call_function  transpose_27                                                             aten.transpose.int                                                       (scaled_dot_product_attention_6, 1, 2)                                                                                                                         {}\n",
      "call_function  view_34                                                                  aten.view.default                                                        (transpose_27, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_8                                                               aten._to_copy.default                                                    (view_34,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_46                                                                aten.linear.default                                                      (_to_copy_8, p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias)   {}\n",
      "call_function  dropout_13                                                               aten.dropout.default                                                     (linear_46, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_11                                                                   aten.div.Tensor                                                          (dropout_13, 1.0)                                                                                                                                              {}\n",
      "call_function  add_20                                                                   aten.add.Tensor                                                          (div_11, linear_42)                                                                                                                                            {}\n",
      "call_function  layer_norm_10                                                            aten.layer_norm.default                                                  (add_20, [640], p_down_blocks_1_attentions_1_transformer_blocks_0_norm2_weight, p_down_blocks_1_attentions_1_transformer_blocks_0_norm2_bias)                  {}\n",
      "call_function  linear_47                                                                aten.linear.default                                                      (layer_norm_10, p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight)                                                                           {}\n",
      "call_function  linear_48                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight)                                                                   {}\n",
      "call_function  linear_49                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight)                                                                   {}\n",
      "call_function  view_35                                                                  aten.view.default                                                        (linear_47, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_28                                                             aten.transpose.int                                                       (view_35, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_36                                                                  aten.view.default                                                        (linear_48, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_29                                                             aten.transpose.int                                                       (view_36, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_37                                                                  aten.view.default                                                        (linear_49, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_30                                                             aten.transpose.int                                                       (view_37, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_7                                           aten.scaled_dot_product_attention.default                                (transpose_28, transpose_29, transpose_30)                                                                                                                     {}\n",
      "call_function  transpose_31                                                             aten.transpose.int                                                       (scaled_dot_product_attention_7, 1, 2)                                                                                                                         {}\n",
      "call_function  view_38                                                                  aten.view.default                                                        (transpose_31, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_9                                                               aten._to_copy.default                                                    (view_38,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_50                                                                aten.linear.default                                                      (_to_copy_9, p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias)   {}\n",
      "call_function  dropout_14                                                               aten.dropout.default                                                     (linear_50, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_12                                                                   aten.div.Tensor                                                          (dropout_14, 1.0)                                                                                                                                              {}\n",
      "call_function  add_21                                                                   aten.add.Tensor                                                          (div_12, add_20)                                                                                                                                               {}\n",
      "call_function  layer_norm_11                                                            aten.layer_norm.default                                                  (add_21, [640], p_down_blocks_1_attentions_1_transformer_blocks_0_norm3_weight, p_down_blocks_1_attentions_1_transformer_blocks_0_norm3_bias)                  {}\n",
      "call_function  linear_51                                                                aten.linear.default                                                      (layer_norm_11, p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias)  {}\n",
      "call_function  split_3                                                                  aten.split.Tensor                                                        (linear_51, 2560, -1)                                                                                                                                          {}\n",
      "call_function  getitem_6                                                                <built-in function getitem>                                              (split_3, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_7                                                                <built-in function getitem>                                              (split_3, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_3                                                                   aten.gelu.default                                                        (getitem_7,)                                                                                                                                                   {}\n",
      "call_function  mul_6                                                                    aten.mul.Tensor                                                          (getitem_6, gelu_3)                                                                                                                                            {}\n",
      "call_function  dropout_15                                                               aten.dropout.default                                                     (mul_6, 0.0, False)                                                                                                                                            {}\n",
      "call_function  linear_52                                                                aten.linear.default                                                      (dropout_15, p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight, p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias)               {}\n",
      "call_function  add_22                                                                   aten.add.Tensor                                                          (linear_52, add_21)                                                                                                                                            {}\n",
      "call_function  linear_53                                                                aten.linear.default                                                      (add_22, p_down_blocks_1_attentions_1_proj_out_weight, p_down_blocks_1_attentions_1_proj_out_bias)                                                             {}\n",
      "call_function  view_39                                                                  aten.view.default                                                        (linear_53, [1, 32, 32, 640])                                                                                                                                  {}\n",
      "call_function  permute_7                                                                aten.permute.default                                                     (view_39, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_3                                                                  aten.clone.default                                                       (permute_7,)                                                                                                                                                   {'memory_format': torch.contiguous_format}\n",
      "call_function  add_23                                                                   aten.add.Tensor                                                          (clone_3, div_10)                                                                                                                                              {}\n",
      "call_function  conv2d_11                                                                aten.conv2d.default                                                      (add_23, p_down_blocks_1_downsamplers_0_conv_weight, p_down_blocks_1_downsamplers_0_conv_bias, [2, 2], [1, 1])                                                 {}\n",
      "call_function  group_norm_12                                                            aten.group_norm.default                                                  (conv2d_11, 32, p_down_blocks_2_resnets_0_norm1_weight, p_down_blocks_2_resnets_0_norm1_bias)                                                                  {}\n",
      "call_function  silu_13                                                                  aten.silu.default                                                        (group_norm_12,)                                                                                                                                               {}\n",
      "call_function  conv2d_12                                                                aten.conv2d.default                                                      (silu_13, p_down_blocks_2_resnets_0_conv1_weight, p_down_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1])                                                        {}\n",
      "call_function  silu_14                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_54                                                                aten.linear.default                                                      (silu_14, p_down_blocks_2_resnets_0_time_emb_proj_weight, p_down_blocks_2_resnets_0_time_emb_proj_bias)                                                        {}\n",
      "call_function  slice_15                                                                 aten.slice.Tensor                                                        (linear_54, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_16                                                                 aten.slice.Tensor                                                        (slice_15, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_10                                                             aten.unsqueeze.default                                                   (slice_16, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_11                                                             aten.unsqueeze.default                                                   (unsqueeze_10, 3)                                                                                                                                              {}\n",
      "call_function  add_24                                                                   aten.add.Tensor                                                          (conv2d_12, unsqueeze_11)                                                                                                                                      {}\n",
      "call_function  group_norm_13                                                            aten.group_norm.default                                                  (add_24, 32, p_down_blocks_2_resnets_0_norm2_weight, p_down_blocks_2_resnets_0_norm2_bias)                                                                     {}\n",
      "call_function  silu_15                                                                  aten.silu.default                                                        (group_norm_13,)                                                                                                                                               {}\n",
      "call_function  dropout_16                                                               aten.dropout.default                                                     (silu_15, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_13                                                                aten.conv2d.default                                                      (dropout_16, p_down_blocks_2_resnets_0_conv2_weight, p_down_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1])                                                     {}\n",
      "call_function  conv2d_14                                                                aten.conv2d.default                                                      (conv2d_11, p_down_blocks_2_resnets_0_conv_shortcut_weight, p_down_blocks_2_resnets_0_conv_shortcut_bias)                                                      {}\n",
      "call_function  add_25                                                                   aten.add.Tensor                                                          (conv2d_14, conv2d_13)                                                                                                                                         {}\n",
      "call_function  div_13                                                                   aten.div.Tensor                                                          (add_25, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_14                                                            aten.group_norm.default                                                  (div_13, 32, p_down_blocks_2_attentions_0_norm_weight, p_down_blocks_2_attentions_0_norm_bias, 1e-06)                                                          {}\n",
      "call_function  permute_8                                                                aten.permute.default                                                     (group_norm_14, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_40                                                                  aten.view.default                                                        (permute_8, [1, 256, 1280])                                                                                                                                    {}\n",
      "call_function  linear_55                                                                aten.linear.default                                                      (view_40, p_down_blocks_2_attentions_0_proj_in_weight, p_down_blocks_2_attentions_0_proj_in_bias)                                                              {}\n",
      "call_function  layer_norm_12                                                            aten.layer_norm.default                                                  (linear_55, [1280], p_down_blocks_2_attentions_0_transformer_blocks_0_norm1_weight, p_down_blocks_2_attentions_0_transformer_blocks_0_norm1_bias)              {}\n",
      "call_function  linear_56                                                                aten.linear.default                                                      (layer_norm_12, p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight)                                                                           {}\n",
      "call_function  linear_57                                                                aten.linear.default                                                      (layer_norm_12, p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight)                                                                           {}\n",
      "call_function  linear_58                                                                aten.linear.default                                                      (layer_norm_12, p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight)                                                                           {}\n",
      "call_function  view_41                                                                  aten.view.default                                                        (linear_56, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_32                                                             aten.transpose.int                                                       (view_41, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_42                                                                  aten.view.default                                                        (linear_57, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_33                                                             aten.transpose.int                                                       (view_42, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_43                                                                  aten.view.default                                                        (linear_58, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_34                                                             aten.transpose.int                                                       (view_43, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_8                                           aten.scaled_dot_product_attention.default                                (transpose_32, transpose_33, transpose_34)                                                                                                                     {}\n",
      "call_function  transpose_35                                                             aten.transpose.int                                                       (scaled_dot_product_attention_8, 1, 2)                                                                                                                         {}\n",
      "call_function  view_44                                                                  aten.view.default                                                        (transpose_35, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_10                                                              aten._to_copy.default                                                    (view_44,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_59                                                                aten.linear.default                                                      (_to_copy_10, p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias)  {}\n",
      "call_function  dropout_17                                                               aten.dropout.default                                                     (linear_59, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_14                                                                   aten.div.Tensor                                                          (dropout_17, 1.0)                                                                                                                                              {}\n",
      "call_function  add_26                                                                   aten.add.Tensor                                                          (div_14, linear_55)                                                                                                                                            {}\n",
      "call_function  layer_norm_13                                                            aten.layer_norm.default                                                  (add_26, [1280], p_down_blocks_2_attentions_0_transformer_blocks_0_norm2_weight, p_down_blocks_2_attentions_0_transformer_blocks_0_norm2_bias)                 {}\n",
      "call_function  linear_60                                                                aten.linear.default                                                      (layer_norm_13, p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight)                                                                           {}\n",
      "call_function  linear_61                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight)                                                                   {}\n",
      "call_function  linear_62                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight)                                                                   {}\n",
      "call_function  view_45                                                                  aten.view.default                                                        (linear_60, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_36                                                             aten.transpose.int                                                       (view_45, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_46                                                                  aten.view.default                                                        (linear_61, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_37                                                             aten.transpose.int                                                       (view_46, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_47                                                                  aten.view.default                                                        (linear_62, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_38                                                             aten.transpose.int                                                       (view_47, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_9                                           aten.scaled_dot_product_attention.default                                (transpose_36, transpose_37, transpose_38)                                                                                                                     {}\n",
      "call_function  transpose_39                                                             aten.transpose.int                                                       (scaled_dot_product_attention_9, 1, 2)                                                                                                                         {}\n",
      "call_function  view_48                                                                  aten.view.default                                                        (transpose_39, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_11                                                              aten._to_copy.default                                                    (view_48,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_63                                                                aten.linear.default                                                      (_to_copy_11, p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias)  {}\n",
      "call_function  dropout_18                                                               aten.dropout.default                                                     (linear_63, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_15                                                                   aten.div.Tensor                                                          (dropout_18, 1.0)                                                                                                                                              {}\n",
      "call_function  add_27                                                                   aten.add.Tensor                                                          (div_15, add_26)                                                                                                                                               {}\n",
      "call_function  layer_norm_14                                                            aten.layer_norm.default                                                  (add_27, [1280], p_down_blocks_2_attentions_0_transformer_blocks_0_norm3_weight, p_down_blocks_2_attentions_0_transformer_blocks_0_norm3_bias)                 {}\n",
      "call_function  linear_64                                                                aten.linear.default                                                      (layer_norm_14, p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias)  {}\n",
      "call_function  split_4                                                                  aten.split.Tensor                                                        (linear_64, 5120, -1)                                                                                                                                          {}\n",
      "call_function  getitem_8                                                                <built-in function getitem>                                              (split_4, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_9                                                                <built-in function getitem>                                              (split_4, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_4                                                                   aten.gelu.default                                                        (getitem_9,)                                                                                                                                                   {}\n",
      "call_function  mul_7                                                                    aten.mul.Tensor                                                          (getitem_8, gelu_4)                                                                                                                                            {}\n",
      "call_function  dropout_19                                                               aten.dropout.default                                                     (mul_7, 0.0, False)                                                                                                                                            {}\n",
      "call_function  linear_65                                                                aten.linear.default                                                      (dropout_19, p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight, p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias)               {}\n",
      "call_function  add_28                                                                   aten.add.Tensor                                                          (linear_65, add_27)                                                                                                                                            {}\n",
      "call_function  linear_66                                                                aten.linear.default                                                      (add_28, p_down_blocks_2_attentions_0_proj_out_weight, p_down_blocks_2_attentions_0_proj_out_bias)                                                             {}\n",
      "call_function  view_49                                                                  aten.view.default                                                        (linear_66, [1, 16, 16, 1280])                                                                                                                                 {}\n",
      "call_function  permute_9                                                                aten.permute.default                                                     (view_49, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_4                                                                  aten.clone.default                                                       (permute_9,)                                                                                                                                                   {'memory_format': torch.contiguous_format}\n",
      "call_function  add_29                                                                   aten.add.Tensor                                                          (clone_4, div_13)                                                                                                                                              {}\n",
      "call_function  group_norm_15                                                            aten.group_norm.default                                                  (add_29, 32, p_down_blocks_2_resnets_1_norm1_weight, p_down_blocks_2_resnets_1_norm1_bias)                                                                     {}\n",
      "call_function  silu_16                                                                  aten.silu.default                                                        (group_norm_15,)                                                                                                                                               {}\n",
      "call_function  conv2d_15                                                                aten.conv2d.default                                                      (silu_16, p_down_blocks_2_resnets_1_conv1_weight, p_down_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1])                                                        {}\n",
      "call_function  silu_17                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_67                                                                aten.linear.default                                                      (silu_17, p_down_blocks_2_resnets_1_time_emb_proj_weight, p_down_blocks_2_resnets_1_time_emb_proj_bias)                                                        {}\n",
      "call_function  slice_17                                                                 aten.slice.Tensor                                                        (linear_67, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_18                                                                 aten.slice.Tensor                                                        (slice_17, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_12                                                             aten.unsqueeze.default                                                   (slice_18, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_13                                                             aten.unsqueeze.default                                                   (unsqueeze_12, 3)                                                                                                                                              {}\n",
      "call_function  add_30                                                                   aten.add.Tensor                                                          (conv2d_15, unsqueeze_13)                                                                                                                                      {}\n",
      "call_function  group_norm_16                                                            aten.group_norm.default                                                  (add_30, 32, p_down_blocks_2_resnets_1_norm2_weight, p_down_blocks_2_resnets_1_norm2_bias)                                                                     {}\n",
      "call_function  silu_18                                                                  aten.silu.default                                                        (group_norm_16,)                                                                                                                                               {}\n",
      "call_function  dropout_20                                                               aten.dropout.default                                                     (silu_18, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_16                                                                aten.conv2d.default                                                      (dropout_20, p_down_blocks_2_resnets_1_conv2_weight, p_down_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1])                                                     {}\n",
      "call_function  add_31                                                                   aten.add.Tensor                                                          (add_29, conv2d_16)                                                                                                                                            {}\n",
      "call_function  div_16                                                                   aten.div.Tensor                                                          (add_31, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_17                                                            aten.group_norm.default                                                  (div_16, 32, p_down_blocks_2_attentions_1_norm_weight, p_down_blocks_2_attentions_1_norm_bias, 1e-06)                                                          {}\n",
      "call_function  permute_10                                                               aten.permute.default                                                     (group_norm_17, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_50                                                                  aten.view.default                                                        (permute_10, [1, 256, 1280])                                                                                                                                   {}\n",
      "call_function  linear_68                                                                aten.linear.default                                                      (view_50, p_down_blocks_2_attentions_1_proj_in_weight, p_down_blocks_2_attentions_1_proj_in_bias)                                                              {}\n",
      "call_function  layer_norm_15                                                            aten.layer_norm.default                                                  (linear_68, [1280], p_down_blocks_2_attentions_1_transformer_blocks_0_norm1_weight, p_down_blocks_2_attentions_1_transformer_blocks_0_norm1_bias)              {}\n",
      "call_function  linear_69                                                                aten.linear.default                                                      (layer_norm_15, p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight)                                                                           {}\n",
      "call_function  linear_70                                                                aten.linear.default                                                      (layer_norm_15, p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight)                                                                           {}\n",
      "call_function  linear_71                                                                aten.linear.default                                                      (layer_norm_15, p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight)                                                                           {}\n",
      "call_function  view_51                                                                  aten.view.default                                                        (linear_69, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_40                                                             aten.transpose.int                                                       (view_51, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_52                                                                  aten.view.default                                                        (linear_70, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_41                                                             aten.transpose.int                                                       (view_52, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_53                                                                  aten.view.default                                                        (linear_71, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_42                                                             aten.transpose.int                                                       (view_53, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_10                                          aten.scaled_dot_product_attention.default                                (transpose_40, transpose_41, transpose_42)                                                                                                                     {}\n",
      "call_function  transpose_43                                                             aten.transpose.int                                                       (scaled_dot_product_attention_10, 1, 2)                                                                                                                        {}\n",
      "call_function  view_54                                                                  aten.view.default                                                        (transpose_43, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_12                                                              aten._to_copy.default                                                    (view_54,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_72                                                                aten.linear.default                                                      (_to_copy_12, p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias)  {}\n",
      "call_function  dropout_21                                                               aten.dropout.default                                                     (linear_72, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_17                                                                   aten.div.Tensor                                                          (dropout_21, 1.0)                                                                                                                                              {}\n",
      "call_function  add_32                                                                   aten.add.Tensor                                                          (div_17, linear_68)                                                                                                                                            {}\n",
      "call_function  layer_norm_16                                                            aten.layer_norm.default                                                  (add_32, [1280], p_down_blocks_2_attentions_1_transformer_blocks_0_norm2_weight, p_down_blocks_2_attentions_1_transformer_blocks_0_norm2_bias)                 {}\n",
      "call_function  linear_73                                                                aten.linear.default                                                      (layer_norm_16, p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight)                                                                           {}\n",
      "call_function  linear_74                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight)                                                                   {}\n",
      "call_function  linear_75                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight)                                                                   {}\n",
      "call_function  view_55                                                                  aten.view.default                                                        (linear_73, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_44                                                             aten.transpose.int                                                       (view_55, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_56                                                                  aten.view.default                                                        (linear_74, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_45                                                             aten.transpose.int                                                       (view_56, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_57                                                                  aten.view.default                                                        (linear_75, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_46                                                             aten.transpose.int                                                       (view_57, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_11                                          aten.scaled_dot_product_attention.default                                (transpose_44, transpose_45, transpose_46)                                                                                                                     {}\n",
      "call_function  transpose_47                                                             aten.transpose.int                                                       (scaled_dot_product_attention_11, 1, 2)                                                                                                                        {}\n",
      "call_function  view_58                                                                  aten.view.default                                                        (transpose_47, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_13                                                              aten._to_copy.default                                                    (view_58,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_76                                                                aten.linear.default                                                      (_to_copy_13, p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias)  {}\n",
      "call_function  dropout_22                                                               aten.dropout.default                                                     (linear_76, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_18                                                                   aten.div.Tensor                                                          (dropout_22, 1.0)                                                                                                                                              {}\n",
      "call_function  add_33                                                                   aten.add.Tensor                                                          (div_18, add_32)                                                                                                                                               {}\n",
      "call_function  layer_norm_17                                                            aten.layer_norm.default                                                  (add_33, [1280], p_down_blocks_2_attentions_1_transformer_blocks_0_norm3_weight, p_down_blocks_2_attentions_1_transformer_blocks_0_norm3_bias)                 {}\n",
      "call_function  linear_77                                                                aten.linear.default                                                      (layer_norm_17, p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias)  {}\n",
      "call_function  split_5                                                                  aten.split.Tensor                                                        (linear_77, 5120, -1)                                                                                                                                          {}\n",
      "call_function  getitem_10                                                               <built-in function getitem>                                              (split_5, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_11                                                               <built-in function getitem>                                              (split_5, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_5                                                                   aten.gelu.default                                                        (getitem_11,)                                                                                                                                                  {}\n",
      "call_function  mul_8                                                                    aten.mul.Tensor                                                          (getitem_10, gelu_5)                                                                                                                                           {}\n",
      "call_function  dropout_23                                                               aten.dropout.default                                                     (mul_8, 0.0, False)                                                                                                                                            {}\n",
      "call_function  linear_78                                                                aten.linear.default                                                      (dropout_23, p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight, p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias)               {}\n",
      "call_function  add_34                                                                   aten.add.Tensor                                                          (linear_78, add_33)                                                                                                                                            {}\n",
      "call_function  linear_79                                                                aten.linear.default                                                      (add_34, p_down_blocks_2_attentions_1_proj_out_weight, p_down_blocks_2_attentions_1_proj_out_bias)                                                             {}\n",
      "call_function  view_59                                                                  aten.view.default                                                        (linear_79, [1, 16, 16, 1280])                                                                                                                                 {}\n",
      "call_function  permute_11                                                               aten.permute.default                                                     (view_59, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_5                                                                  aten.clone.default                                                       (permute_11,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_35                                                                   aten.add.Tensor                                                          (clone_5, div_16)                                                                                                                                              {}\n",
      "call_function  conv2d_17                                                                aten.conv2d.default                                                      (add_35, p_down_blocks_2_downsamplers_0_conv_weight, p_down_blocks_2_downsamplers_0_conv_bias, [2, 2], [1, 1])                                                 {}\n",
      "call_function  group_norm_18                                                            aten.group_norm.default                                                  (conv2d_17, 32, p_down_blocks_3_resnets_0_norm1_weight, p_down_blocks_3_resnets_0_norm1_bias)                                                                  {}\n",
      "call_function  silu_19                                                                  aten.silu.default                                                        (group_norm_18,)                                                                                                                                               {}\n",
      "call_function  conv2d_18                                                                aten.conv2d.default                                                      (silu_19, p_down_blocks_3_resnets_0_conv1_weight, p_down_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1])                                                        {}\n",
      "call_function  silu_20                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_80                                                                aten.linear.default                                                      (silu_20, p_down_blocks_3_resnets_0_time_emb_proj_weight, p_down_blocks_3_resnets_0_time_emb_proj_bias)                                                        {}\n",
      "call_function  slice_19                                                                 aten.slice.Tensor                                                        (linear_80, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_20                                                                 aten.slice.Tensor                                                        (slice_19, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_14                                                             aten.unsqueeze.default                                                   (slice_20, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_15                                                             aten.unsqueeze.default                                                   (unsqueeze_14, 3)                                                                                                                                              {}\n",
      "call_function  add_36                                                                   aten.add.Tensor                                                          (conv2d_18, unsqueeze_15)                                                                                                                                      {}\n",
      "call_function  group_norm_19                                                            aten.group_norm.default                                                  (add_36, 32, p_down_blocks_3_resnets_0_norm2_weight, p_down_blocks_3_resnets_0_norm2_bias)                                                                     {}\n",
      "call_function  silu_21                                                                  aten.silu.default                                                        (group_norm_19,)                                                                                                                                               {}\n",
      "call_function  dropout_24                                                               aten.dropout.default                                                     (silu_21, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_19                                                                aten.conv2d.default                                                      (dropout_24, p_down_blocks_3_resnets_0_conv2_weight, p_down_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1])                                                     {}\n",
      "call_function  add_37                                                                   aten.add.Tensor                                                          (conv2d_17, conv2d_19)                                                                                                                                         {}\n",
      "call_function  div_19                                                                   aten.div.Tensor                                                          (add_37, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_20                                                            aten.group_norm.default                                                  (div_19, 32, p_down_blocks_3_resnets_1_norm1_weight, p_down_blocks_3_resnets_1_norm1_bias)                                                                     {}\n",
      "call_function  silu_22                                                                  aten.silu.default                                                        (group_norm_20,)                                                                                                                                               {}\n",
      "call_function  conv2d_20                                                                aten.conv2d.default                                                      (silu_22, p_down_blocks_3_resnets_1_conv1_weight, p_down_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1])                                                        {}\n",
      "call_function  silu_23                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_81                                                                aten.linear.default                                                      (silu_23, p_down_blocks_3_resnets_1_time_emb_proj_weight, p_down_blocks_3_resnets_1_time_emb_proj_bias)                                                        {}\n",
      "call_function  slice_21                                                                 aten.slice.Tensor                                                        (linear_81, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_22                                                                 aten.slice.Tensor                                                        (slice_21, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_16                                                             aten.unsqueeze.default                                                   (slice_22, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_17                                                             aten.unsqueeze.default                                                   (unsqueeze_16, 3)                                                                                                                                              {}\n",
      "call_function  add_38                                                                   aten.add.Tensor                                                          (conv2d_20, unsqueeze_17)                                                                                                                                      {}\n",
      "call_function  group_norm_21                                                            aten.group_norm.default                                                  (add_38, 32, p_down_blocks_3_resnets_1_norm2_weight, p_down_blocks_3_resnets_1_norm2_bias)                                                                     {}\n",
      "call_function  silu_24                                                                  aten.silu.default                                                        (group_norm_21,)                                                                                                                                               {}\n",
      "call_function  dropout_25                                                               aten.dropout.default                                                     (silu_24, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_21                                                                aten.conv2d.default                                                      (dropout_25, p_down_blocks_3_resnets_1_conv2_weight, p_down_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1])                                                     {}\n",
      "call_function  add_39                                                                   aten.add.Tensor                                                          (div_19, conv2d_21)                                                                                                                                            {}\n",
      "call_function  div_20                                                                   aten.div.Tensor                                                          (add_39, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_22                                                            aten.group_norm.default                                                  (div_20, 32, p_mid_block_resnets_0_norm1_weight, p_mid_block_resnets_0_norm1_bias)                                                                             {}\n",
      "call_function  silu_25                                                                  aten.silu.default                                                        (group_norm_22,)                                                                                                                                               {}\n",
      "call_function  conv2d_22                                                                aten.conv2d.default                                                      (silu_25, p_mid_block_resnets_0_conv1_weight, p_mid_block_resnets_0_conv1_bias, [1, 1], [1, 1])                                                                {}\n",
      "call_function  silu_26                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_82                                                                aten.linear.default                                                      (silu_26, p_mid_block_resnets_0_time_emb_proj_weight, p_mid_block_resnets_0_time_emb_proj_bias)                                                                {}\n",
      "call_function  slice_23                                                                 aten.slice.Tensor                                                        (linear_82, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_24                                                                 aten.slice.Tensor                                                        (slice_23, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_18                                                             aten.unsqueeze.default                                                   (slice_24, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_19                                                             aten.unsqueeze.default                                                   (unsqueeze_18, 3)                                                                                                                                              {}\n",
      "call_function  add_40                                                                   aten.add.Tensor                                                          (conv2d_22, unsqueeze_19)                                                                                                                                      {}\n",
      "call_function  group_norm_23                                                            aten.group_norm.default                                                  (add_40, 32, p_mid_block_resnets_0_norm2_weight, p_mid_block_resnets_0_norm2_bias)                                                                             {}\n",
      "call_function  silu_27                                                                  aten.silu.default                                                        (group_norm_23,)                                                                                                                                               {}\n",
      "call_function  dropout_26                                                               aten.dropout.default                                                     (silu_27, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_23                                                                aten.conv2d.default                                                      (dropout_26, p_mid_block_resnets_0_conv2_weight, p_mid_block_resnets_0_conv2_bias, [1, 1], [1, 1])                                                             {}\n",
      "call_function  add_41                                                                   aten.add.Tensor                                                          (div_20, conv2d_23)                                                                                                                                            {}\n",
      "call_function  div_21                                                                   aten.div.Tensor                                                          (add_41, 1)                                                                                                                                                    {}\n",
      "call_function  group_norm_24                                                            aten.group_norm.default                                                  (div_21, 32, p_mid_block_attentions_0_norm_weight, p_mid_block_attentions_0_norm_bias, 1e-06)                                                                  {}\n",
      "call_function  permute_12                                                               aten.permute.default                                                     (group_norm_24, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_60                                                                  aten.view.default                                                        (permute_12, [1, 64, 1280])                                                                                                                                    {}\n",
      "call_function  linear_83                                                                aten.linear.default                                                      (view_60, p_mid_block_attentions_0_proj_in_weight, p_mid_block_attentions_0_proj_in_bias)                                                                      {}\n",
      "call_function  layer_norm_18                                                            aten.layer_norm.default                                                  (linear_83, [1280], p_mid_block_attentions_0_transformer_blocks_0_norm1_weight, p_mid_block_attentions_0_transformer_blocks_0_norm1_bias)                      {}\n",
      "call_function  linear_84                                                                aten.linear.default                                                      (layer_norm_18, p_mid_block_attentions_0_transformer_blocks_0_attn1_to_q_weight)                                                                               {}\n",
      "call_function  linear_85                                                                aten.linear.default                                                      (layer_norm_18, p_mid_block_attentions_0_transformer_blocks_0_attn1_to_k_weight)                                                                               {}\n",
      "call_function  linear_86                                                                aten.linear.default                                                      (layer_norm_18, p_mid_block_attentions_0_transformer_blocks_0_attn1_to_v_weight)                                                                               {}\n",
      "call_function  view_61                                                                  aten.view.default                                                        (linear_84, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_48                                                             aten.transpose.int                                                       (view_61, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_62                                                                  aten.view.default                                                        (linear_85, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_49                                                             aten.transpose.int                                                       (view_62, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_63                                                                  aten.view.default                                                        (linear_86, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_50                                                             aten.transpose.int                                                       (view_63, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_12                                          aten.scaled_dot_product_attention.default                                (transpose_48, transpose_49, transpose_50)                                                                                                                     {}\n",
      "call_function  transpose_51                                                             aten.transpose.int                                                       (scaled_dot_product_attention_12, 1, 2)                                                                                                                        {}\n",
      "call_function  view_64                                                                  aten.view.default                                                        (transpose_51, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_14                                                              aten._to_copy.default                                                    (view_64,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_87                                                                aten.linear.default                                                      (_to_copy_14, p_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias)          {}\n",
      "call_function  dropout_27                                                               aten.dropout.default                                                     (linear_87, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_22                                                                   aten.div.Tensor                                                          (dropout_27, 1.0)                                                                                                                                              {}\n",
      "call_function  add_42                                                                   aten.add.Tensor                                                          (div_22, linear_83)                                                                                                                                            {}\n",
      "call_function  layer_norm_19                                                            aten.layer_norm.default                                                  (add_42, [1280], p_mid_block_attentions_0_transformer_blocks_0_norm2_weight, p_mid_block_attentions_0_transformer_blocks_0_norm2_bias)                         {}\n",
      "call_function  linear_88                                                                aten.linear.default                                                      (layer_norm_19, p_mid_block_attentions_0_transformer_blocks_0_attn2_to_q_weight)                                                                               {}\n",
      "call_function  linear_89                                                                aten.linear.default                                                      (encoder_hidden_states, p_mid_block_attentions_0_transformer_blocks_0_attn2_to_k_weight)                                                                       {}\n",
      "call_function  linear_90                                                                aten.linear.default                                                      (encoder_hidden_states, p_mid_block_attentions_0_transformer_blocks_0_attn2_to_v_weight)                                                                       {}\n",
      "call_function  view_65                                                                  aten.view.default                                                        (linear_88, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_52                                                             aten.transpose.int                                                       (view_65, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_66                                                                  aten.view.default                                                        (linear_89, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_53                                                             aten.transpose.int                                                       (view_66, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_67                                                                  aten.view.default                                                        (linear_90, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_54                                                             aten.transpose.int                                                       (view_67, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_13                                          aten.scaled_dot_product_attention.default                                (transpose_52, transpose_53, transpose_54)                                                                                                                     {}\n",
      "call_function  transpose_55                                                             aten.transpose.int                                                       (scaled_dot_product_attention_13, 1, 2)                                                                                                                        {}\n",
      "call_function  view_68                                                                  aten.view.default                                                        (transpose_55, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_15                                                              aten._to_copy.default                                                    (view_68,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_91                                                                aten.linear.default                                                      (_to_copy_15, p_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_bias)          {}\n",
      "call_function  dropout_28                                                               aten.dropout.default                                                     (linear_91, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_23                                                                   aten.div.Tensor                                                          (dropout_28, 1.0)                                                                                                                                              {}\n",
      "call_function  add_43                                                                   aten.add.Tensor                                                          (div_23, add_42)                                                                                                                                               {}\n",
      "call_function  layer_norm_20                                                            aten.layer_norm.default                                                  (add_43, [1280], p_mid_block_attentions_0_transformer_blocks_0_norm3_weight, p_mid_block_attentions_0_transformer_blocks_0_norm3_bias)                         {}\n",
      "call_function  linear_92                                                                aten.linear.default                                                      (layer_norm_20, p_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_bias)          {}\n",
      "call_function  split_6                                                                  aten.split.Tensor                                                        (linear_92, 5120, -1)                                                                                                                                          {}\n",
      "call_function  getitem_12                                                               <built-in function getitem>                                              (split_6, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_13                                                               <built-in function getitem>                                              (split_6, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_6                                                                   aten.gelu.default                                                        (getitem_13,)                                                                                                                                                  {}\n",
      "call_function  mul_9                                                                    aten.mul.Tensor                                                          (getitem_12, gelu_6)                                                                                                                                           {}\n",
      "call_function  dropout_29                                                               aten.dropout.default                                                     (mul_9, 0.0, False)                                                                                                                                            {}\n",
      "call_function  linear_93                                                                aten.linear.default                                                      (dropout_29, p_mid_block_attentions_0_transformer_blocks_0_ff_net_2_weight, p_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias)                       {}\n",
      "call_function  add_44                                                                   aten.add.Tensor                                                          (linear_93, add_43)                                                                                                                                            {}\n",
      "call_function  linear_94                                                                aten.linear.default                                                      (add_44, p_mid_block_attentions_0_proj_out_weight, p_mid_block_attentions_0_proj_out_bias)                                                                     {}\n",
      "call_function  view_69                                                                  aten.view.default                                                        (linear_94, [1, 8, 8, 1280])                                                                                                                                   {}\n",
      "call_function  permute_13                                                               aten.permute.default                                                     (view_69, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_6                                                                  aten.clone.default                                                       (permute_13,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_45                                                                   aten.add.Tensor                                                          (clone_6, div_21)                                                                                                                                              {}\n",
      "call_function  group_norm_25                                                            aten.group_norm.default                                                  (add_45, 32, p_mid_block_resnets_slice_1__none__none___0_norm1_weight, p_mid_block_resnets_slice_1__none__none___0_norm1_bias)                                 {}\n",
      "call_function  silu_28                                                                  aten.silu.default                                                        (group_norm_25,)                                                                                                                                               {}\n",
      "call_function  conv2d_24                                                                aten.conv2d.default                                                      (silu_28, p_mid_block_resnets_slice_1__none__none___0_conv1_weight, p_mid_block_resnets_slice_1__none__none___0_conv1_bias, [1, 1], [1, 1])                    {}\n",
      "call_function  silu_29                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_95                                                                aten.linear.default                                                      (silu_29, p_mid_block_resnets_slice_1__none__none___0_time_emb_proj_weight, p_mid_block_resnets_slice_1__none__none___0_time_emb_proj_bias)                    {}\n",
      "call_function  slice_25                                                                 aten.slice.Tensor                                                        (linear_95, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_26                                                                 aten.slice.Tensor                                                        (slice_25, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_20                                                             aten.unsqueeze.default                                                   (slice_26, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_21                                                             aten.unsqueeze.default                                                   (unsqueeze_20, 3)                                                                                                                                              {}\n",
      "call_function  add_46                                                                   aten.add.Tensor                                                          (conv2d_24, unsqueeze_21)                                                                                                                                      {}\n",
      "call_function  group_norm_26                                                            aten.group_norm.default                                                  (add_46, 32, p_mid_block_resnets_slice_1__none__none___0_norm2_weight, p_mid_block_resnets_slice_1__none__none___0_norm2_bias)                                 {}\n",
      "call_function  silu_30                                                                  aten.silu.default                                                        (group_norm_26,)                                                                                                                                               {}\n",
      "call_function  dropout_30                                                               aten.dropout.default                                                     (silu_30, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_25                                                                aten.conv2d.default                                                      (dropout_30, p_mid_block_resnets_slice_1__none__none___0_conv2_weight, p_mid_block_resnets_slice_1__none__none___0_conv2_bias, [1, 1], [1, 1])                 {}\n",
      "call_function  add_47                                                                   aten.add.Tensor                                                          (add_45, conv2d_25)                                                                                                                                            {}\n",
      "call_function  div_24                                                                   aten.div.Tensor                                                          (add_47, 1)                                                                                                                                                    {}\n",
      "call_function  cat_2                                                                    aten.cat.default                                                         ([div_24, div_20], 1)                                                                                                                                          {}\n",
      "call_function  group_norm_27                                                            aten.group_norm.default                                                  (cat_2, 32, p_up_blocks_0_resnets_0_norm1_weight, p_up_blocks_0_resnets_0_norm1_bias)                                                                          {}\n",
      "call_function  silu_31                                                                  aten.silu.default                                                        (group_norm_27,)                                                                                                                                               {}\n",
      "call_function  conv2d_26                                                                aten.conv2d.default                                                      (silu_31, p_up_blocks_0_resnets_0_conv1_weight, p_up_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_32                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_96                                                                aten.linear.default                                                      (silu_32, p_up_blocks_0_resnets_0_time_emb_proj_weight, p_up_blocks_0_resnets_0_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_27                                                                 aten.slice.Tensor                                                        (linear_96, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_28                                                                 aten.slice.Tensor                                                        (slice_27, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_22                                                             aten.unsqueeze.default                                                   (slice_28, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_23                                                             aten.unsqueeze.default                                                   (unsqueeze_22, 3)                                                                                                                                              {}\n",
      "call_function  add_48                                                                   aten.add.Tensor                                                          (conv2d_26, unsqueeze_23)                                                                                                                                      {}\n",
      "call_function  group_norm_28                                                            aten.group_norm.default                                                  (add_48, 32, p_up_blocks_0_resnets_0_norm2_weight, p_up_blocks_0_resnets_0_norm2_bias)                                                                         {}\n",
      "call_function  silu_33                                                                  aten.silu.default                                                        (group_norm_28,)                                                                                                                                               {}\n",
      "call_function  dropout_31                                                               aten.dropout.default                                                     (silu_33, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_27                                                                aten.conv2d.default                                                      (dropout_31, p_up_blocks_0_resnets_0_conv2_weight, p_up_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_28                                                                aten.conv2d.default                                                      (cat_2, p_up_blocks_0_resnets_0_conv_shortcut_weight, p_up_blocks_0_resnets_0_conv_shortcut_bias)                                                              {}\n",
      "call_function  add_49                                                                   aten.add.Tensor                                                          (conv2d_28, conv2d_27)                                                                                                                                         {}\n",
      "call_function  div_25                                                                   aten.div.Tensor                                                          (add_49, 1.0)                                                                                                                                                  {}\n",
      "call_function  cat_3                                                                    aten.cat.default                                                         ([div_25, div_19], 1)                                                                                                                                          {}\n",
      "call_function  group_norm_29                                                            aten.group_norm.default                                                  (cat_3, 32, p_up_blocks_0_resnets_1_norm1_weight, p_up_blocks_0_resnets_1_norm1_bias)                                                                          {}\n",
      "call_function  silu_34                                                                  aten.silu.default                                                        (group_norm_29,)                                                                                                                                               {}\n",
      "call_function  conv2d_29                                                                aten.conv2d.default                                                      (silu_34, p_up_blocks_0_resnets_1_conv1_weight, p_up_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_35                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_97                                                                aten.linear.default                                                      (silu_35, p_up_blocks_0_resnets_1_time_emb_proj_weight, p_up_blocks_0_resnets_1_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_29                                                                 aten.slice.Tensor                                                        (linear_97, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_30                                                                 aten.slice.Tensor                                                        (slice_29, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_24                                                             aten.unsqueeze.default                                                   (slice_30, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_25                                                             aten.unsqueeze.default                                                   (unsqueeze_24, 3)                                                                                                                                              {}\n",
      "call_function  add_50                                                                   aten.add.Tensor                                                          (conv2d_29, unsqueeze_25)                                                                                                                                      {}\n",
      "call_function  group_norm_30                                                            aten.group_norm.default                                                  (add_50, 32, p_up_blocks_0_resnets_1_norm2_weight, p_up_blocks_0_resnets_1_norm2_bias)                                                                         {}\n",
      "call_function  silu_36                                                                  aten.silu.default                                                        (group_norm_30,)                                                                                                                                               {}\n",
      "call_function  dropout_32                                                               aten.dropout.default                                                     (silu_36, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_30                                                                aten.conv2d.default                                                      (dropout_32, p_up_blocks_0_resnets_1_conv2_weight, p_up_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_31                                                                aten.conv2d.default                                                      (cat_3, p_up_blocks_0_resnets_1_conv_shortcut_weight, p_up_blocks_0_resnets_1_conv_shortcut_bias)                                                              {}\n",
      "call_function  add_51                                                                   aten.add.Tensor                                                          (conv2d_31, conv2d_30)                                                                                                                                         {}\n",
      "call_function  div_26                                                                   aten.div.Tensor                                                          (add_51, 1.0)                                                                                                                                                  {}\n",
      "call_function  cat_4                                                                    aten.cat.default                                                         ([div_26, conv2d_17], 1)                                                                                                                                       {}\n",
      "call_function  group_norm_31                                                            aten.group_norm.default                                                  (cat_4, 32, p_up_blocks_0_resnets_2_norm1_weight, p_up_blocks_0_resnets_2_norm1_bias)                                                                          {}\n",
      "call_function  silu_37                                                                  aten.silu.default                                                        (group_norm_31,)                                                                                                                                               {}\n",
      "call_function  conv2d_32                                                                aten.conv2d.default                                                      (silu_37, p_up_blocks_0_resnets_2_conv1_weight, p_up_blocks_0_resnets_2_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_38                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_98                                                                aten.linear.default                                                      (silu_38, p_up_blocks_0_resnets_2_time_emb_proj_weight, p_up_blocks_0_resnets_2_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_31                                                                 aten.slice.Tensor                                                        (linear_98, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_32                                                                 aten.slice.Tensor                                                        (slice_31, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_26                                                             aten.unsqueeze.default                                                   (slice_32, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_27                                                             aten.unsqueeze.default                                                   (unsqueeze_26, 3)                                                                                                                                              {}\n",
      "call_function  add_52                                                                   aten.add.Tensor                                                          (conv2d_32, unsqueeze_27)                                                                                                                                      {}\n",
      "call_function  group_norm_32                                                            aten.group_norm.default                                                  (add_52, 32, p_up_blocks_0_resnets_2_norm2_weight, p_up_blocks_0_resnets_2_norm2_bias)                                                                         {}\n",
      "call_function  silu_39                                                                  aten.silu.default                                                        (group_norm_32,)                                                                                                                                               {}\n",
      "call_function  dropout_33                                                               aten.dropout.default                                                     (silu_39, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_33                                                                aten.conv2d.default                                                      (dropout_33, p_up_blocks_0_resnets_2_conv2_weight, p_up_blocks_0_resnets_2_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_34                                                                aten.conv2d.default                                                      (cat_4, p_up_blocks_0_resnets_2_conv_shortcut_weight, p_up_blocks_0_resnets_2_conv_shortcut_bias)                                                              {}\n",
      "call_function  add_53                                                                   aten.add.Tensor                                                          (conv2d_34, conv2d_33)                                                                                                                                         {}\n",
      "call_function  div_27                                                                   aten.div.Tensor                                                          (add_53, 1.0)                                                                                                                                                  {}\n",
      "call_function  upsample_nearest2d                                                       aten.upsample_nearest2d.vec                                              (div_27, None, [2.0, 2.0])                                                                                                                                     {}\n",
      "call_function  conv2d_35                                                                aten.conv2d.default                                                      (upsample_nearest2d, p_up_blocks_0_upsamplers_0_conv_weight, p_up_blocks_0_upsamplers_0_conv_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  cat_5                                                                    aten.cat.default                                                         ([conv2d_35, add_35], 1)                                                                                                                                       {}\n",
      "call_function  group_norm_33                                                            aten.group_norm.default                                                  (cat_5, 32, p_up_blocks_1_resnets_0_norm1_weight, p_up_blocks_1_resnets_0_norm1_bias)                                                                          {}\n",
      "call_function  silu_40                                                                  aten.silu.default                                                        (group_norm_33,)                                                                                                                                               {}\n",
      "call_function  conv2d_36                                                                aten.conv2d.default                                                      (silu_40, p_up_blocks_1_resnets_0_conv1_weight, p_up_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_41                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_99                                                                aten.linear.default                                                      (silu_41, p_up_blocks_1_resnets_0_time_emb_proj_weight, p_up_blocks_1_resnets_0_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_33                                                                 aten.slice.Tensor                                                        (linear_99, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_34                                                                 aten.slice.Tensor                                                        (slice_33, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_28                                                             aten.unsqueeze.default                                                   (slice_34, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_29                                                             aten.unsqueeze.default                                                   (unsqueeze_28, 3)                                                                                                                                              {}\n",
      "call_function  add_54                                                                   aten.add.Tensor                                                          (conv2d_36, unsqueeze_29)                                                                                                                                      {}\n",
      "call_function  group_norm_34                                                            aten.group_norm.default                                                  (add_54, 32, p_up_blocks_1_resnets_0_norm2_weight, p_up_blocks_1_resnets_0_norm2_bias)                                                                         {}\n",
      "call_function  silu_42                                                                  aten.silu.default                                                        (group_norm_34,)                                                                                                                                               {}\n",
      "call_function  dropout_34                                                               aten.dropout.default                                                     (silu_42, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_37                                                                aten.conv2d.default                                                      (dropout_34, p_up_blocks_1_resnets_0_conv2_weight, p_up_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_38                                                                aten.conv2d.default                                                      (cat_5, p_up_blocks_1_resnets_0_conv_shortcut_weight, p_up_blocks_1_resnets_0_conv_shortcut_bias)                                                              {}\n",
      "call_function  add_55                                                                   aten.add.Tensor                                                          (conv2d_38, conv2d_37)                                                                                                                                         {}\n",
      "call_function  div_28                                                                   aten.div.Tensor                                                          (add_55, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_35                                                            aten.group_norm.default                                                  (div_28, 32, p_up_blocks_1_attentions_0_norm_weight, p_up_blocks_1_attentions_0_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_14                                                               aten.permute.default                                                     (group_norm_35, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_70                                                                  aten.view.default                                                        (permute_14, [1, 256, 1280])                                                                                                                                   {}\n",
      "call_function  linear_100                                                               aten.linear.default                                                      (view_70, p_up_blocks_1_attentions_0_proj_in_weight, p_up_blocks_1_attentions_0_proj_in_bias)                                                                  {}\n",
      "call_function  layer_norm_21                                                            aten.layer_norm.default                                                  (linear_100, [1280], p_up_blocks_1_attentions_0_transformer_blocks_0_norm1_weight, p_up_blocks_1_attentions_0_transformer_blocks_0_norm1_bias)                 {}\n",
      "call_function  linear_101                                                               aten.linear.default                                                      (layer_norm_21, p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_102                                                               aten.linear.default                                                      (layer_norm_21, p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_103                                                               aten.linear.default                                                      (layer_norm_21, p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_71                                                                  aten.view.default                                                        (linear_101, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_56                                                             aten.transpose.int                                                       (view_71, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_72                                                                  aten.view.default                                                        (linear_102, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_57                                                             aten.transpose.int                                                       (view_72, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_73                                                                  aten.view.default                                                        (linear_103, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_58                                                             aten.transpose.int                                                       (view_73, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_14                                          aten.scaled_dot_product_attention.default                                (transpose_56, transpose_57, transpose_58)                                                                                                                     {}\n",
      "call_function  transpose_59                                                             aten.transpose.int                                                       (scaled_dot_product_attention_14, 1, 2)                                                                                                                        {}\n",
      "call_function  view_74                                                                  aten.view.default                                                        (transpose_59, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_16                                                              aten._to_copy.default                                                    (view_74,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_104                                                               aten.linear.default                                                      (_to_copy_16, p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_35                                                               aten.dropout.default                                                     (linear_104, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_29                                                                   aten.div.Tensor                                                          (dropout_35, 1.0)                                                                                                                                              {}\n",
      "call_function  add_56                                                                   aten.add.Tensor                                                          (div_29, linear_100)                                                                                                                                           {}\n",
      "call_function  layer_norm_22                                                            aten.layer_norm.default                                                  (add_56, [1280], p_up_blocks_1_attentions_0_transformer_blocks_0_norm2_weight, p_up_blocks_1_attentions_0_transformer_blocks_0_norm2_bias)                     {}\n",
      "call_function  linear_105                                                               aten.linear.default                                                      (layer_norm_22, p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_106                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_107                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_75                                                                  aten.view.default                                                        (linear_105, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_60                                                             aten.transpose.int                                                       (view_75, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_76                                                                  aten.view.default                                                        (linear_106, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_61                                                             aten.transpose.int                                                       (view_76, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_77                                                                  aten.view.default                                                        (linear_107, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_62                                                             aten.transpose.int                                                       (view_77, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_15                                          aten.scaled_dot_product_attention.default                                (transpose_60, transpose_61, transpose_62)                                                                                                                     {}\n",
      "call_function  transpose_63                                                             aten.transpose.int                                                       (scaled_dot_product_attention_15, 1, 2)                                                                                                                        {}\n",
      "call_function  view_78                                                                  aten.view.default                                                        (transpose_63, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_17                                                              aten._to_copy.default                                                    (view_78,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_108                                                               aten.linear.default                                                      (_to_copy_17, p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_36                                                               aten.dropout.default                                                     (linear_108, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_30                                                                   aten.div.Tensor                                                          (dropout_36, 1.0)                                                                                                                                              {}\n",
      "call_function  add_57                                                                   aten.add.Tensor                                                          (div_30, add_56)                                                                                                                                               {}\n",
      "call_function  layer_norm_23                                                            aten.layer_norm.default                                                  (add_57, [1280], p_up_blocks_1_attentions_0_transformer_blocks_0_norm3_weight, p_up_blocks_1_attentions_0_transformer_blocks_0_norm3_bias)                     {}\n",
      "call_function  linear_109                                                               aten.linear.default                                                      (layer_norm_23, p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_7                                                                  aten.split.Tensor                                                        (linear_109, 5120, -1)                                                                                                                                         {}\n",
      "call_function  getitem_14                                                               <built-in function getitem>                                              (split_7, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_15                                                               <built-in function getitem>                                              (split_7, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_7                                                                   aten.gelu.default                                                        (getitem_15,)                                                                                                                                                  {}\n",
      "call_function  mul_10                                                                   aten.mul.Tensor                                                          (getitem_14, gelu_7)                                                                                                                                           {}\n",
      "call_function  dropout_37                                                               aten.dropout.default                                                     (mul_10, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_110                                                               aten.linear.default                                                      (dropout_37, p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight, p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_58                                                                   aten.add.Tensor                                                          (linear_110, add_57)                                                                                                                                           {}\n",
      "call_function  linear_111                                                               aten.linear.default                                                      (add_58, p_up_blocks_1_attentions_0_proj_out_weight, p_up_blocks_1_attentions_0_proj_out_bias)                                                                 {}\n",
      "call_function  view_79                                                                  aten.view.default                                                        (linear_111, [1, 16, 16, 1280])                                                                                                                                {}\n",
      "call_function  permute_15                                                               aten.permute.default                                                     (view_79, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_7                                                                  aten.clone.default                                                       (permute_15,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_59                                                                   aten.add.Tensor                                                          (clone_7, div_28)                                                                                                                                              {}\n",
      "call_function  cat_6                                                                    aten.cat.default                                                         ([add_59, add_29], 1)                                                                                                                                          {}\n",
      "call_function  group_norm_36                                                            aten.group_norm.default                                                  (cat_6, 32, p_up_blocks_1_resnets_1_norm1_weight, p_up_blocks_1_resnets_1_norm1_bias)                                                                          {}\n",
      "call_function  silu_43                                                                  aten.silu.default                                                        (group_norm_36,)                                                                                                                                               {}\n",
      "call_function  conv2d_39                                                                aten.conv2d.default                                                      (silu_43, p_up_blocks_1_resnets_1_conv1_weight, p_up_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_44                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_112                                                               aten.linear.default                                                      (silu_44, p_up_blocks_1_resnets_1_time_emb_proj_weight, p_up_blocks_1_resnets_1_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_35                                                                 aten.slice.Tensor                                                        (linear_112, 0, 0, 9223372036854775807)                                                                                                                        {}\n",
      "call_function  slice_36                                                                 aten.slice.Tensor                                                        (slice_35, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_30                                                             aten.unsqueeze.default                                                   (slice_36, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_31                                                             aten.unsqueeze.default                                                   (unsqueeze_30, 3)                                                                                                                                              {}\n",
      "call_function  add_60                                                                   aten.add.Tensor                                                          (conv2d_39, unsqueeze_31)                                                                                                                                      {}\n",
      "call_function  group_norm_37                                                            aten.group_norm.default                                                  (add_60, 32, p_up_blocks_1_resnets_1_norm2_weight, p_up_blocks_1_resnets_1_norm2_bias)                                                                         {}\n",
      "call_function  silu_45                                                                  aten.silu.default                                                        (group_norm_37,)                                                                                                                                               {}\n",
      "call_function  dropout_38                                                               aten.dropout.default                                                     (silu_45, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_40                                                                aten.conv2d.default                                                      (dropout_38, p_up_blocks_1_resnets_1_conv2_weight, p_up_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_41                                                                aten.conv2d.default                                                      (cat_6, p_up_blocks_1_resnets_1_conv_shortcut_weight, p_up_blocks_1_resnets_1_conv_shortcut_bias)                                                              {}\n",
      "call_function  add_61                                                                   aten.add.Tensor                                                          (conv2d_41, conv2d_40)                                                                                                                                         {}\n",
      "call_function  div_31                                                                   aten.div.Tensor                                                          (add_61, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_38                                                            aten.group_norm.default                                                  (div_31, 32, p_up_blocks_1_attentions_1_norm_weight, p_up_blocks_1_attentions_1_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_16                                                               aten.permute.default                                                     (group_norm_38, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_80                                                                  aten.view.default                                                        (permute_16, [1, 256, 1280])                                                                                                                                   {}\n",
      "call_function  linear_113                                                               aten.linear.default                                                      (view_80, p_up_blocks_1_attentions_1_proj_in_weight, p_up_blocks_1_attentions_1_proj_in_bias)                                                                  {}\n",
      "call_function  layer_norm_24                                                            aten.layer_norm.default                                                  (linear_113, [1280], p_up_blocks_1_attentions_1_transformer_blocks_0_norm1_weight, p_up_blocks_1_attentions_1_transformer_blocks_0_norm1_bias)                 {}\n",
      "call_function  linear_114                                                               aten.linear.default                                                      (layer_norm_24, p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_115                                                               aten.linear.default                                                      (layer_norm_24, p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_116                                                               aten.linear.default                                                      (layer_norm_24, p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_81                                                                  aten.view.default                                                        (linear_114, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_64                                                             aten.transpose.int                                                       (view_81, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_82                                                                  aten.view.default                                                        (linear_115, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_65                                                             aten.transpose.int                                                       (view_82, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_83                                                                  aten.view.default                                                        (linear_116, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_66                                                             aten.transpose.int                                                       (view_83, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_16                                          aten.scaled_dot_product_attention.default                                (transpose_64, transpose_65, transpose_66)                                                                                                                     {}\n",
      "call_function  transpose_67                                                             aten.transpose.int                                                       (scaled_dot_product_attention_16, 1, 2)                                                                                                                        {}\n",
      "call_function  view_84                                                                  aten.view.default                                                        (transpose_67, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_18                                                              aten._to_copy.default                                                    (view_84,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_117                                                               aten.linear.default                                                      (_to_copy_18, p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_39                                                               aten.dropout.default                                                     (linear_117, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_32                                                                   aten.div.Tensor                                                          (dropout_39, 1.0)                                                                                                                                              {}\n",
      "call_function  add_62                                                                   aten.add.Tensor                                                          (div_32, linear_113)                                                                                                                                           {}\n",
      "call_function  layer_norm_25                                                            aten.layer_norm.default                                                  (add_62, [1280], p_up_blocks_1_attentions_1_transformer_blocks_0_norm2_weight, p_up_blocks_1_attentions_1_transformer_blocks_0_norm2_bias)                     {}\n",
      "call_function  linear_118                                                               aten.linear.default                                                      (layer_norm_25, p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_119                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_120                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_85                                                                  aten.view.default                                                        (linear_118, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_68                                                             aten.transpose.int                                                       (view_85, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_86                                                                  aten.view.default                                                        (linear_119, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_69                                                             aten.transpose.int                                                       (view_86, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_87                                                                  aten.view.default                                                        (linear_120, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_70                                                             aten.transpose.int                                                       (view_87, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_17                                          aten.scaled_dot_product_attention.default                                (transpose_68, transpose_69, transpose_70)                                                                                                                     {}\n",
      "call_function  transpose_71                                                             aten.transpose.int                                                       (scaled_dot_product_attention_17, 1, 2)                                                                                                                        {}\n",
      "call_function  view_88                                                                  aten.view.default                                                        (transpose_71, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_19                                                              aten._to_copy.default                                                    (view_88,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_121                                                               aten.linear.default                                                      (_to_copy_19, p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_40                                                               aten.dropout.default                                                     (linear_121, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_33                                                                   aten.div.Tensor                                                          (dropout_40, 1.0)                                                                                                                                              {}\n",
      "call_function  add_63                                                                   aten.add.Tensor                                                          (div_33, add_62)                                                                                                                                               {}\n",
      "call_function  layer_norm_26                                                            aten.layer_norm.default                                                  (add_63, [1280], p_up_blocks_1_attentions_1_transformer_blocks_0_norm3_weight, p_up_blocks_1_attentions_1_transformer_blocks_0_norm3_bias)                     {}\n",
      "call_function  linear_122                                                               aten.linear.default                                                      (layer_norm_26, p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_8                                                                  aten.split.Tensor                                                        (linear_122, 5120, -1)                                                                                                                                         {}\n",
      "call_function  getitem_16                                                               <built-in function getitem>                                              (split_8, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_17                                                               <built-in function getitem>                                              (split_8, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_8                                                                   aten.gelu.default                                                        (getitem_17,)                                                                                                                                                  {}\n",
      "call_function  mul_11                                                                   aten.mul.Tensor                                                          (getitem_16, gelu_8)                                                                                                                                           {}\n",
      "call_function  dropout_41                                                               aten.dropout.default                                                     (mul_11, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_123                                                               aten.linear.default                                                      (dropout_41, p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight, p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_64                                                                   aten.add.Tensor                                                          (linear_123, add_63)                                                                                                                                           {}\n",
      "call_function  linear_124                                                               aten.linear.default                                                      (add_64, p_up_blocks_1_attentions_1_proj_out_weight, p_up_blocks_1_attentions_1_proj_out_bias)                                                                 {}\n",
      "call_function  view_89                                                                  aten.view.default                                                        (linear_124, [1, 16, 16, 1280])                                                                                                                                {}\n",
      "call_function  permute_17                                                               aten.permute.default                                                     (view_89, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_8                                                                  aten.clone.default                                                       (permute_17,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_65                                                                   aten.add.Tensor                                                          (clone_8, div_31)                                                                                                                                              {}\n",
      "call_function  cat_7                                                                    aten.cat.default                                                         ([add_65, conv2d_11], 1)                                                                                                                                       {}\n",
      "call_function  group_norm_39                                                            aten.group_norm.default                                                  (cat_7, 32, p_up_blocks_1_resnets_2_norm1_weight, p_up_blocks_1_resnets_2_norm1_bias)                                                                          {}\n",
      "call_function  silu_46                                                                  aten.silu.default                                                        (group_norm_39,)                                                                                                                                               {}\n",
      "call_function  conv2d_42                                                                aten.conv2d.default                                                      (silu_46, p_up_blocks_1_resnets_2_conv1_weight, p_up_blocks_1_resnets_2_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_47                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_125                                                               aten.linear.default                                                      (silu_47, p_up_blocks_1_resnets_2_time_emb_proj_weight, p_up_blocks_1_resnets_2_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_37                                                                 aten.slice.Tensor                                                        (linear_125, 0, 0, 9223372036854775807)                                                                                                                        {}\n",
      "call_function  slice_38                                                                 aten.slice.Tensor                                                        (slice_37, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_32                                                             aten.unsqueeze.default                                                   (slice_38, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_33                                                             aten.unsqueeze.default                                                   (unsqueeze_32, 3)                                                                                                                                              {}\n",
      "call_function  add_66                                                                   aten.add.Tensor                                                          (conv2d_42, unsqueeze_33)                                                                                                                                      {}\n",
      "call_function  group_norm_40                                                            aten.group_norm.default                                                  (add_66, 32, p_up_blocks_1_resnets_2_norm2_weight, p_up_blocks_1_resnets_2_norm2_bias)                                                                         {}\n",
      "call_function  silu_48                                                                  aten.silu.default                                                        (group_norm_40,)                                                                                                                                               {}\n",
      "call_function  dropout_42                                                               aten.dropout.default                                                     (silu_48, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_43                                                                aten.conv2d.default                                                      (dropout_42, p_up_blocks_1_resnets_2_conv2_weight, p_up_blocks_1_resnets_2_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_44                                                                aten.conv2d.default                                                      (cat_7, p_up_blocks_1_resnets_2_conv_shortcut_weight, p_up_blocks_1_resnets_2_conv_shortcut_bias)                                                              {}\n",
      "call_function  add_67                                                                   aten.add.Tensor                                                          (conv2d_44, conv2d_43)                                                                                                                                         {}\n",
      "call_function  div_34                                                                   aten.div.Tensor                                                          (add_67, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_41                                                            aten.group_norm.default                                                  (div_34, 32, p_up_blocks_1_attentions_2_norm_weight, p_up_blocks_1_attentions_2_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_18                                                               aten.permute.default                                                     (group_norm_41, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_90                                                                  aten.view.default                                                        (permute_18, [1, 256, 1280])                                                                                                                                   {}\n",
      "call_function  linear_126                                                               aten.linear.default                                                      (view_90, p_up_blocks_1_attentions_2_proj_in_weight, p_up_blocks_1_attentions_2_proj_in_bias)                                                                  {}\n",
      "call_function  layer_norm_27                                                            aten.layer_norm.default                                                  (linear_126, [1280], p_up_blocks_1_attentions_2_transformer_blocks_0_norm1_weight, p_up_blocks_1_attentions_2_transformer_blocks_0_norm1_bias)                 {}\n",
      "call_function  linear_127                                                               aten.linear.default                                                      (layer_norm_27, p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_128                                                               aten.linear.default                                                      (layer_norm_27, p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_129                                                               aten.linear.default                                                      (layer_norm_27, p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_91                                                                  aten.view.default                                                        (linear_127, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_72                                                             aten.transpose.int                                                       (view_91, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_92                                                                  aten.view.default                                                        (linear_128, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_73                                                             aten.transpose.int                                                       (view_92, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_93                                                                  aten.view.default                                                        (linear_129, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_74                                                             aten.transpose.int                                                       (view_93, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_18                                          aten.scaled_dot_product_attention.default                                (transpose_72, transpose_73, transpose_74)                                                                                                                     {}\n",
      "call_function  transpose_75                                                             aten.transpose.int                                                       (scaled_dot_product_attention_18, 1, 2)                                                                                                                        {}\n",
      "call_function  view_94                                                                  aten.view.default                                                        (transpose_75, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_20                                                              aten._to_copy.default                                                    (view_94,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_130                                                               aten.linear.default                                                      (_to_copy_20, p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_43                                                               aten.dropout.default                                                     (linear_130, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_35                                                                   aten.div.Tensor                                                          (dropout_43, 1.0)                                                                                                                                              {}\n",
      "call_function  add_68                                                                   aten.add.Tensor                                                          (div_35, linear_126)                                                                                                                                           {}\n",
      "call_function  layer_norm_28                                                            aten.layer_norm.default                                                  (add_68, [1280], p_up_blocks_1_attentions_2_transformer_blocks_0_norm2_weight, p_up_blocks_1_attentions_2_transformer_blocks_0_norm2_bias)                     {}\n",
      "call_function  linear_131                                                               aten.linear.default                                                      (layer_norm_28, p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_132                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_133                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_95                                                                  aten.view.default                                                        (linear_131, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_76                                                             aten.transpose.int                                                       (view_95, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_96                                                                  aten.view.default                                                        (linear_132, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_77                                                             aten.transpose.int                                                       (view_96, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_97                                                                  aten.view.default                                                        (linear_133, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_78                                                             aten.transpose.int                                                       (view_97, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_19                                          aten.scaled_dot_product_attention.default                                (transpose_76, transpose_77, transpose_78)                                                                                                                     {}\n",
      "call_function  transpose_79                                                             aten.transpose.int                                                       (scaled_dot_product_attention_19, 1, 2)                                                                                                                        {}\n",
      "call_function  view_98                                                                  aten.view.default                                                        (transpose_79, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_21                                                              aten._to_copy.default                                                    (view_98,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_134                                                               aten.linear.default                                                      (_to_copy_21, p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_44                                                               aten.dropout.default                                                     (linear_134, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_36                                                                   aten.div.Tensor                                                          (dropout_44, 1.0)                                                                                                                                              {}\n",
      "call_function  add_69                                                                   aten.add.Tensor                                                          (div_36, add_68)                                                                                                                                               {}\n",
      "call_function  layer_norm_29                                                            aten.layer_norm.default                                                  (add_69, [1280], p_up_blocks_1_attentions_2_transformer_blocks_0_norm3_weight, p_up_blocks_1_attentions_2_transformer_blocks_0_norm3_bias)                     {}\n",
      "call_function  linear_135                                                               aten.linear.default                                                      (layer_norm_29, p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_9                                                                  aten.split.Tensor                                                        (linear_135, 5120, -1)                                                                                                                                         {}\n",
      "call_function  getitem_18                                                               <built-in function getitem>                                              (split_9, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_19                                                               <built-in function getitem>                                              (split_9, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_9                                                                   aten.gelu.default                                                        (getitem_19,)                                                                                                                                                  {}\n",
      "call_function  mul_12                                                                   aten.mul.Tensor                                                          (getitem_18, gelu_9)                                                                                                                                           {}\n",
      "call_function  dropout_45                                                               aten.dropout.default                                                     (mul_12, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_136                                                               aten.linear.default                                                      (dropout_45, p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_weight, p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_70                                                                   aten.add.Tensor                                                          (linear_136, add_69)                                                                                                                                           {}\n",
      "call_function  linear_137                                                               aten.linear.default                                                      (add_70, p_up_blocks_1_attentions_2_proj_out_weight, p_up_blocks_1_attentions_2_proj_out_bias)                                                                 {}\n",
      "call_function  view_99                                                                  aten.view.default                                                        (linear_137, [1, 16, 16, 1280])                                                                                                                                {}\n",
      "call_function  permute_19                                                               aten.permute.default                                                     (view_99, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_9                                                                  aten.clone.default                                                       (permute_19,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_71                                                                   aten.add.Tensor                                                          (clone_9, div_34)                                                                                                                                              {}\n",
      "call_function  upsample_nearest2d_1                                                     aten.upsample_nearest2d.vec                                              (add_71, None, [2.0, 2.0])                                                                                                                                     {}\n",
      "call_function  conv2d_45                                                                aten.conv2d.default                                                      (upsample_nearest2d_1, p_up_blocks_1_upsamplers_0_conv_weight, p_up_blocks_1_upsamplers_0_conv_bias, [1, 1], [1, 1])                                           {}\n",
      "call_function  cat_8                                                                    aten.cat.default                                                         ([conv2d_45, add_23], 1)                                                                                                                                       {}\n",
      "call_function  group_norm_42                                                            aten.group_norm.default                                                  (cat_8, 32, p_up_blocks_2_resnets_0_norm1_weight, p_up_blocks_2_resnets_0_norm1_bias)                                                                          {}\n",
      "call_function  silu_49                                                                  aten.silu.default                                                        (group_norm_42,)                                                                                                                                               {}\n",
      "call_function  conv2d_46                                                                aten.conv2d.default                                                      (silu_49, p_up_blocks_2_resnets_0_conv1_weight, p_up_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_50                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_138                                                               aten.linear.default                                                      (silu_50, p_up_blocks_2_resnets_0_time_emb_proj_weight, p_up_blocks_2_resnets_0_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_39                                                                 aten.slice.Tensor                                                        (linear_138, 0, 0, 9223372036854775807)                                                                                                                        {}\n",
      "call_function  slice_40                                                                 aten.slice.Tensor                                                        (slice_39, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_34                                                             aten.unsqueeze.default                                                   (slice_40, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_35                                                             aten.unsqueeze.default                                                   (unsqueeze_34, 3)                                                                                                                                              {}\n",
      "call_function  add_72                                                                   aten.add.Tensor                                                          (conv2d_46, unsqueeze_35)                                                                                                                                      {}\n",
      "call_function  group_norm_43                                                            aten.group_norm.default                                                  (add_72, 32, p_up_blocks_2_resnets_0_norm2_weight, p_up_blocks_2_resnets_0_norm2_bias)                                                                         {}\n",
      "call_function  silu_51                                                                  aten.silu.default                                                        (group_norm_43,)                                                                                                                                               {}\n",
      "call_function  dropout_46                                                               aten.dropout.default                                                     (silu_51, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_47                                                                aten.conv2d.default                                                      (dropout_46, p_up_blocks_2_resnets_0_conv2_weight, p_up_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_48                                                                aten.conv2d.default                                                      (cat_8, p_up_blocks_2_resnets_0_conv_shortcut_weight, p_up_blocks_2_resnets_0_conv_shortcut_bias)                                                              {}\n",
      "call_function  add_73                                                                   aten.add.Tensor                                                          (conv2d_48, conv2d_47)                                                                                                                                         {}\n",
      "call_function  div_37                                                                   aten.div.Tensor                                                          (add_73, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_44                                                            aten.group_norm.default                                                  (div_37, 32, p_up_blocks_2_attentions_0_norm_weight, p_up_blocks_2_attentions_0_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_20                                                               aten.permute.default                                                     (group_norm_44, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_100                                                                 aten.view.default                                                        (permute_20, [1, 1024, 640])                                                                                                                                   {}\n",
      "call_function  linear_139                                                               aten.linear.default                                                      (view_100, p_up_blocks_2_attentions_0_proj_in_weight, p_up_blocks_2_attentions_0_proj_in_bias)                                                                 {}\n",
      "call_function  layer_norm_30                                                            aten.layer_norm.default                                                  (linear_139, [640], p_up_blocks_2_attentions_0_transformer_blocks_0_norm1_weight, p_up_blocks_2_attentions_0_transformer_blocks_0_norm1_bias)                  {}\n",
      "call_function  linear_140                                                               aten.linear.default                                                      (layer_norm_30, p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_141                                                               aten.linear.default                                                      (layer_norm_30, p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_142                                                               aten.linear.default                                                      (layer_norm_30, p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_101                                                                 aten.view.default                                                        (linear_140, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_80                                                             aten.transpose.int                                                       (view_101, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_102                                                                 aten.view.default                                                        (linear_141, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_81                                                             aten.transpose.int                                                       (view_102, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_103                                                                 aten.view.default                                                        (linear_142, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_82                                                             aten.transpose.int                                                       (view_103, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_20                                          aten.scaled_dot_product_attention.default                                (transpose_80, transpose_81, transpose_82)                                                                                                                     {}\n",
      "call_function  transpose_83                                                             aten.transpose.int                                                       (scaled_dot_product_attention_20, 1, 2)                                                                                                                        {}\n",
      "call_function  view_104                                                                 aten.view.default                                                        (transpose_83, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_22                                                              aten._to_copy.default                                                    (view_104,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_143                                                               aten.linear.default                                                      (_to_copy_22, p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_47                                                               aten.dropout.default                                                     (linear_143, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_38                                                                   aten.div.Tensor                                                          (dropout_47, 1.0)                                                                                                                                              {}\n",
      "call_function  add_74                                                                   aten.add.Tensor                                                          (div_38, linear_139)                                                                                                                                           {}\n",
      "call_function  layer_norm_31                                                            aten.layer_norm.default                                                  (add_74, [640], p_up_blocks_2_attentions_0_transformer_blocks_0_norm2_weight, p_up_blocks_2_attentions_0_transformer_blocks_0_norm2_bias)                      {}\n",
      "call_function  linear_144                                                               aten.linear.default                                                      (layer_norm_31, p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_145                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_146                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_105                                                                 aten.view.default                                                        (linear_144, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_84                                                             aten.transpose.int                                                       (view_105, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_106                                                                 aten.view.default                                                        (linear_145, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_85                                                             aten.transpose.int                                                       (view_106, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_107                                                                 aten.view.default                                                        (linear_146, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_86                                                             aten.transpose.int                                                       (view_107, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_21                                          aten.scaled_dot_product_attention.default                                (transpose_84, transpose_85, transpose_86)                                                                                                                     {}\n",
      "call_function  transpose_87                                                             aten.transpose.int                                                       (scaled_dot_product_attention_21, 1, 2)                                                                                                                        {}\n",
      "call_function  view_108                                                                 aten.view.default                                                        (transpose_87, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_23                                                              aten._to_copy.default                                                    (view_108,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_147                                                               aten.linear.default                                                      (_to_copy_23, p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_48                                                               aten.dropout.default                                                     (linear_147, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_39                                                                   aten.div.Tensor                                                          (dropout_48, 1.0)                                                                                                                                              {}\n",
      "call_function  add_75                                                                   aten.add.Tensor                                                          (div_39, add_74)                                                                                                                                               {}\n",
      "call_function  layer_norm_32                                                            aten.layer_norm.default                                                  (add_75, [640], p_up_blocks_2_attentions_0_transformer_blocks_0_norm3_weight, p_up_blocks_2_attentions_0_transformer_blocks_0_norm3_bias)                      {}\n",
      "call_function  linear_148                                                               aten.linear.default                                                      (layer_norm_32, p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_10                                                                 aten.split.Tensor                                                        (linear_148, 2560, -1)                                                                                                                                         {}\n",
      "call_function  getitem_20                                                               <built-in function getitem>                                              (split_10, 0)                                                                                                                                                  {}\n",
      "call_function  getitem_21                                                               <built-in function getitem>                                              (split_10, 1)                                                                                                                                                  {}\n",
      "call_function  gelu_10                                                                  aten.gelu.default                                                        (getitem_21,)                                                                                                                                                  {}\n",
      "call_function  mul_13                                                                   aten.mul.Tensor                                                          (getitem_20, gelu_10)                                                                                                                                          {}\n",
      "call_function  dropout_49                                                               aten.dropout.default                                                     (mul_13, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_149                                                               aten.linear.default                                                      (dropout_49, p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight, p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_76                                                                   aten.add.Tensor                                                          (linear_149, add_75)                                                                                                                                           {}\n",
      "call_function  linear_150                                                               aten.linear.default                                                      (add_76, p_up_blocks_2_attentions_0_proj_out_weight, p_up_blocks_2_attentions_0_proj_out_bias)                                                                 {}\n",
      "call_function  view_109                                                                 aten.view.default                                                        (linear_150, [1, 32, 32, 640])                                                                                                                                 {}\n",
      "call_function  permute_21                                                               aten.permute.default                                                     (view_109, [0, 3, 1, 2])                                                                                                                                       {}\n",
      "call_function  clone_10                                                                 aten.clone.default                                                       (permute_21,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_77                                                                   aten.add.Tensor                                                          (clone_10, div_37)                                                                                                                                             {}\n",
      "call_function  cat_9                                                                    aten.cat.default                                                         ([add_77, add_17], 1)                                                                                                                                          {}\n",
      "call_function  group_norm_45                                                            aten.group_norm.default                                                  (cat_9, 32, p_up_blocks_2_resnets_1_norm1_weight, p_up_blocks_2_resnets_1_norm1_bias)                                                                          {}\n",
      "call_function  silu_52                                                                  aten.silu.default                                                        (group_norm_45,)                                                                                                                                               {}\n",
      "call_function  conv2d_49                                                                aten.conv2d.default                                                      (silu_52, p_up_blocks_2_resnets_1_conv1_weight, p_up_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_53                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_151                                                               aten.linear.default                                                      (silu_53, p_up_blocks_2_resnets_1_time_emb_proj_weight, p_up_blocks_2_resnets_1_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_41                                                                 aten.slice.Tensor                                                        (linear_151, 0, 0, 9223372036854775807)                                                                                                                        {}\n",
      "call_function  slice_42                                                                 aten.slice.Tensor                                                        (slice_41, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_36                                                             aten.unsqueeze.default                                                   (slice_42, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_37                                                             aten.unsqueeze.default                                                   (unsqueeze_36, 3)                                                                                                                                              {}\n",
      "call_function  add_78                                                                   aten.add.Tensor                                                          (conv2d_49, unsqueeze_37)                                                                                                                                      {}\n",
      "call_function  group_norm_46                                                            aten.group_norm.default                                                  (add_78, 32, p_up_blocks_2_resnets_1_norm2_weight, p_up_blocks_2_resnets_1_norm2_bias)                                                                         {}\n",
      "call_function  silu_54                                                                  aten.silu.default                                                        (group_norm_46,)                                                                                                                                               {}\n",
      "call_function  dropout_50                                                               aten.dropout.default                                                     (silu_54, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_50                                                                aten.conv2d.default                                                      (dropout_50, p_up_blocks_2_resnets_1_conv2_weight, p_up_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_51                                                                aten.conv2d.default                                                      (cat_9, p_up_blocks_2_resnets_1_conv_shortcut_weight, p_up_blocks_2_resnets_1_conv_shortcut_bias)                                                              {}\n",
      "call_function  add_79                                                                   aten.add.Tensor                                                          (conv2d_51, conv2d_50)                                                                                                                                         {}\n",
      "call_function  div_40                                                                   aten.div.Tensor                                                          (add_79, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_47                                                            aten.group_norm.default                                                  (div_40, 32, p_up_blocks_2_attentions_1_norm_weight, p_up_blocks_2_attentions_1_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_22                                                               aten.permute.default                                                     (group_norm_47, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_110                                                                 aten.view.default                                                        (permute_22, [1, 1024, 640])                                                                                                                                   {}\n",
      "call_function  linear_152                                                               aten.linear.default                                                      (view_110, p_up_blocks_2_attentions_1_proj_in_weight, p_up_blocks_2_attentions_1_proj_in_bias)                                                                 {}\n",
      "call_function  layer_norm_33                                                            aten.layer_norm.default                                                  (linear_152, [640], p_up_blocks_2_attentions_1_transformer_blocks_0_norm1_weight, p_up_blocks_2_attentions_1_transformer_blocks_0_norm1_bias)                  {}\n",
      "call_function  linear_153                                                               aten.linear.default                                                      (layer_norm_33, p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_154                                                               aten.linear.default                                                      (layer_norm_33, p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_155                                                               aten.linear.default                                                      (layer_norm_33, p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_111                                                                 aten.view.default                                                        (linear_153, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_88                                                             aten.transpose.int                                                       (view_111, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_112                                                                 aten.view.default                                                        (linear_154, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_89                                                             aten.transpose.int                                                       (view_112, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_113                                                                 aten.view.default                                                        (linear_155, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_90                                                             aten.transpose.int                                                       (view_113, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_22                                          aten.scaled_dot_product_attention.default                                (transpose_88, transpose_89, transpose_90)                                                                                                                     {}\n",
      "call_function  transpose_91                                                             aten.transpose.int                                                       (scaled_dot_product_attention_22, 1, 2)                                                                                                                        {}\n",
      "call_function  view_114                                                                 aten.view.default                                                        (transpose_91, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_24                                                              aten._to_copy.default                                                    (view_114,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_156                                                               aten.linear.default                                                      (_to_copy_24, p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_51                                                               aten.dropout.default                                                     (linear_156, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_41                                                                   aten.div.Tensor                                                          (dropout_51, 1.0)                                                                                                                                              {}\n",
      "call_function  add_80                                                                   aten.add.Tensor                                                          (div_41, linear_152)                                                                                                                                           {}\n",
      "call_function  layer_norm_34                                                            aten.layer_norm.default                                                  (add_80, [640], p_up_blocks_2_attentions_1_transformer_blocks_0_norm2_weight, p_up_blocks_2_attentions_1_transformer_blocks_0_norm2_bias)                      {}\n",
      "call_function  linear_157                                                               aten.linear.default                                                      (layer_norm_34, p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_158                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_159                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_115                                                                 aten.view.default                                                        (linear_157, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_92                                                             aten.transpose.int                                                       (view_115, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_116                                                                 aten.view.default                                                        (linear_158, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_93                                                             aten.transpose.int                                                       (view_116, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_117                                                                 aten.view.default                                                        (linear_159, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_94                                                             aten.transpose.int                                                       (view_117, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_23                                          aten.scaled_dot_product_attention.default                                (transpose_92, transpose_93, transpose_94)                                                                                                                     {}\n",
      "call_function  transpose_95                                                             aten.transpose.int                                                       (scaled_dot_product_attention_23, 1, 2)                                                                                                                        {}\n",
      "call_function  view_118                                                                 aten.view.default                                                        (transpose_95, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_25                                                              aten._to_copy.default                                                    (view_118,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_160                                                               aten.linear.default                                                      (_to_copy_25, p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_52                                                               aten.dropout.default                                                     (linear_160, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_42                                                                   aten.div.Tensor                                                          (dropout_52, 1.0)                                                                                                                                              {}\n",
      "call_function  add_81                                                                   aten.add.Tensor                                                          (div_42, add_80)                                                                                                                                               {}\n",
      "call_function  layer_norm_35                                                            aten.layer_norm.default                                                  (add_81, [640], p_up_blocks_2_attentions_1_transformer_blocks_0_norm3_weight, p_up_blocks_2_attentions_1_transformer_blocks_0_norm3_bias)                      {}\n",
      "call_function  linear_161                                                               aten.linear.default                                                      (layer_norm_35, p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_11                                                                 aten.split.Tensor                                                        (linear_161, 2560, -1)                                                                                                                                         {}\n",
      "call_function  getitem_22                                                               <built-in function getitem>                                              (split_11, 0)                                                                                                                                                  {}\n",
      "call_function  getitem_23                                                               <built-in function getitem>                                              (split_11, 1)                                                                                                                                                  {}\n",
      "call_function  gelu_11                                                                  aten.gelu.default                                                        (getitem_23,)                                                                                                                                                  {}\n",
      "call_function  mul_14                                                                   aten.mul.Tensor                                                          (getitem_22, gelu_11)                                                                                                                                          {}\n",
      "call_function  dropout_53                                                               aten.dropout.default                                                     (mul_14, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_162                                                               aten.linear.default                                                      (dropout_53, p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight, p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_82                                                                   aten.add.Tensor                                                          (linear_162, add_81)                                                                                                                                           {}\n",
      "call_function  linear_163                                                               aten.linear.default                                                      (add_82, p_up_blocks_2_attentions_1_proj_out_weight, p_up_blocks_2_attentions_1_proj_out_bias)                                                                 {}\n",
      "call_function  view_119                                                                 aten.view.default                                                        (linear_163, [1, 32, 32, 640])                                                                                                                                 {}\n",
      "call_function  permute_23                                                               aten.permute.default                                                     (view_119, [0, 3, 1, 2])                                                                                                                                       {}\n",
      "call_function  clone_11                                                                 aten.clone.default                                                       (permute_23,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_83                                                                   aten.add.Tensor                                                          (clone_11, div_40)                                                                                                                                             {}\n",
      "call_function  cat_10                                                                   aten.cat.default                                                         ([add_83, conv2d_5], 1)                                                                                                                                        {}\n",
      "call_function  group_norm_48                                                            aten.group_norm.default                                                  (cat_10, 32, p_up_blocks_2_resnets_2_norm1_weight, p_up_blocks_2_resnets_2_norm1_bias)                                                                         {}\n",
      "call_function  silu_55                                                                  aten.silu.default                                                        (group_norm_48,)                                                                                                                                               {}\n",
      "call_function  conv2d_52                                                                aten.conv2d.default                                                      (silu_55, p_up_blocks_2_resnets_2_conv1_weight, p_up_blocks_2_resnets_2_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_56                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_164                                                               aten.linear.default                                                      (silu_56, p_up_blocks_2_resnets_2_time_emb_proj_weight, p_up_blocks_2_resnets_2_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_43                                                                 aten.slice.Tensor                                                        (linear_164, 0, 0, 9223372036854775807)                                                                                                                        {}\n",
      "call_function  slice_44                                                                 aten.slice.Tensor                                                        (slice_43, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_38                                                             aten.unsqueeze.default                                                   (slice_44, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_39                                                             aten.unsqueeze.default                                                   (unsqueeze_38, 3)                                                                                                                                              {}\n",
      "call_function  add_84                                                                   aten.add.Tensor                                                          (conv2d_52, unsqueeze_39)                                                                                                                                      {}\n",
      "call_function  group_norm_49                                                            aten.group_norm.default                                                  (add_84, 32, p_up_blocks_2_resnets_2_norm2_weight, p_up_blocks_2_resnets_2_norm2_bias)                                                                         {}\n",
      "call_function  silu_57                                                                  aten.silu.default                                                        (group_norm_49,)                                                                                                                                               {}\n",
      "call_function  dropout_54                                                               aten.dropout.default                                                     (silu_57, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_53                                                                aten.conv2d.default                                                      (dropout_54, p_up_blocks_2_resnets_2_conv2_weight, p_up_blocks_2_resnets_2_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_54                                                                aten.conv2d.default                                                      (cat_10, p_up_blocks_2_resnets_2_conv_shortcut_weight, p_up_blocks_2_resnets_2_conv_shortcut_bias)                                                             {}\n",
      "call_function  add_85                                                                   aten.add.Tensor                                                          (conv2d_54, conv2d_53)                                                                                                                                         {}\n",
      "call_function  div_43                                                                   aten.div.Tensor                                                          (add_85, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_50                                                            aten.group_norm.default                                                  (div_43, 32, p_up_blocks_2_attentions_2_norm_weight, p_up_blocks_2_attentions_2_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_24                                                               aten.permute.default                                                     (group_norm_50, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_120                                                                 aten.view.default                                                        (permute_24, [1, 1024, 640])                                                                                                                                   {}\n",
      "call_function  linear_165                                                               aten.linear.default                                                      (view_120, p_up_blocks_2_attentions_2_proj_in_weight, p_up_blocks_2_attentions_2_proj_in_bias)                                                                 {}\n",
      "call_function  layer_norm_36                                                            aten.layer_norm.default                                                  (linear_165, [640], p_up_blocks_2_attentions_2_transformer_blocks_0_norm1_weight, p_up_blocks_2_attentions_2_transformer_blocks_0_norm1_bias)                  {}\n",
      "call_function  linear_166                                                               aten.linear.default                                                      (layer_norm_36, p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_167                                                               aten.linear.default                                                      (layer_norm_36, p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_168                                                               aten.linear.default                                                      (layer_norm_36, p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_121                                                                 aten.view.default                                                        (linear_166, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_96                                                             aten.transpose.int                                                       (view_121, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_122                                                                 aten.view.default                                                        (linear_167, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_97                                                             aten.transpose.int                                                       (view_122, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_123                                                                 aten.view.default                                                        (linear_168, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_98                                                             aten.transpose.int                                                       (view_123, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_24                                          aten.scaled_dot_product_attention.default                                (transpose_96, transpose_97, transpose_98)                                                                                                                     {}\n",
      "call_function  transpose_99                                                             aten.transpose.int                                                       (scaled_dot_product_attention_24, 1, 2)                                                                                                                        {}\n",
      "call_function  view_124                                                                 aten.view.default                                                        (transpose_99, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_26                                                              aten._to_copy.default                                                    (view_124,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_169                                                               aten.linear.default                                                      (_to_copy_26, p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_55                                                               aten.dropout.default                                                     (linear_169, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_44                                                                   aten.div.Tensor                                                          (dropout_55, 1.0)                                                                                                                                              {}\n",
      "call_function  add_86                                                                   aten.add.Tensor                                                          (div_44, linear_165)                                                                                                                                           {}\n",
      "call_function  layer_norm_37                                                            aten.layer_norm.default                                                  (add_86, [640], p_up_blocks_2_attentions_2_transformer_blocks_0_norm2_weight, p_up_blocks_2_attentions_2_transformer_blocks_0_norm2_bias)                      {}\n",
      "call_function  linear_170                                                               aten.linear.default                                                      (layer_norm_37, p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_171                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_172                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_125                                                                 aten.view.default                                                        (linear_170, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_100                                                            aten.transpose.int                                                       (view_125, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_126                                                                 aten.view.default                                                        (linear_171, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_101                                                            aten.transpose.int                                                       (view_126, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_127                                                                 aten.view.default                                                        (linear_172, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_102                                                            aten.transpose.int                                                       (view_127, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_25                                          aten.scaled_dot_product_attention.default                                (transpose_100, transpose_101, transpose_102)                                                                                                                  {}\n",
      "call_function  transpose_103                                                            aten.transpose.int                                                       (scaled_dot_product_attention_25, 1, 2)                                                                                                                        {}\n",
      "call_function  view_128                                                                 aten.view.default                                                        (transpose_103, [1, -1, 640])                                                                                                                                  {}\n",
      "call_function  _to_copy_27                                                              aten._to_copy.default                                                    (view_128,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_173                                                               aten.linear.default                                                      (_to_copy_27, p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_56                                                               aten.dropout.default                                                     (linear_173, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_45                                                                   aten.div.Tensor                                                          (dropout_56, 1.0)                                                                                                                                              {}\n",
      "call_function  add_87                                                                   aten.add.Tensor                                                          (div_45, add_86)                                                                                                                                               {}\n",
      "call_function  layer_norm_38                                                            aten.layer_norm.default                                                  (add_87, [640], p_up_blocks_2_attentions_2_transformer_blocks_0_norm3_weight, p_up_blocks_2_attentions_2_transformer_blocks_0_norm3_bias)                      {}\n",
      "call_function  linear_174                                                               aten.linear.default                                                      (layer_norm_38, p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_12                                                                 aten.split.Tensor                                                        (linear_174, 2560, -1)                                                                                                                                         {}\n",
      "call_function  getitem_24                                                               <built-in function getitem>                                              (split_12, 0)                                                                                                                                                  {}\n",
      "call_function  getitem_25                                                               <built-in function getitem>                                              (split_12, 1)                                                                                                                                                  {}\n",
      "call_function  gelu_12                                                                  aten.gelu.default                                                        (getitem_25,)                                                                                                                                                  {}\n",
      "call_function  mul_15                                                                   aten.mul.Tensor                                                          (getitem_24, gelu_12)                                                                                                                                          {}\n",
      "call_function  dropout_57                                                               aten.dropout.default                                                     (mul_15, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_175                                                               aten.linear.default                                                      (dropout_57, p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_weight, p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_88                                                                   aten.add.Tensor                                                          (linear_175, add_87)                                                                                                                                           {}\n",
      "call_function  linear_176                                                               aten.linear.default                                                      (add_88, p_up_blocks_2_attentions_2_proj_out_weight, p_up_blocks_2_attentions_2_proj_out_bias)                                                                 {}\n",
      "call_function  view_129                                                                 aten.view.default                                                        (linear_176, [1, 32, 32, 640])                                                                                                                                 {}\n",
      "call_function  permute_25                                                               aten.permute.default                                                     (view_129, [0, 3, 1, 2])                                                                                                                                       {}\n",
      "call_function  clone_12                                                                 aten.clone.default                                                       (permute_25,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_89                                                                   aten.add.Tensor                                                          (clone_12, div_43)                                                                                                                                             {}\n",
      "call_function  upsample_nearest2d_2                                                     aten.upsample_nearest2d.vec                                              (add_89, None, [2.0, 2.0])                                                                                                                                     {}\n",
      "call_function  conv2d_55                                                                aten.conv2d.default                                                      (upsample_nearest2d_2, p_up_blocks_2_upsamplers_0_conv_weight, p_up_blocks_2_upsamplers_0_conv_bias, [1, 1], [1, 1])                                           {}\n",
      "call_function  cat_11                                                                   aten.cat.default                                                         ([conv2d_55, add_11], 1)                                                                                                                                       {}\n",
      "call_function  group_norm_51                                                            aten.group_norm.default                                                  (cat_11, 32, p_up_blocks_3_resnets_0_norm1_weight, p_up_blocks_3_resnets_0_norm1_bias)                                                                         {}\n",
      "call_function  silu_58                                                                  aten.silu.default                                                        (group_norm_51,)                                                                                                                                               {}\n",
      "call_function  conv2d_56                                                                aten.conv2d.default                                                      (silu_58, p_up_blocks_3_resnets_0_conv1_weight, p_up_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_59                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_177                                                               aten.linear.default                                                      (silu_59, p_up_blocks_3_resnets_0_time_emb_proj_weight, p_up_blocks_3_resnets_0_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_45                                                                 aten.slice.Tensor                                                        (linear_177, 0, 0, 9223372036854775807)                                                                                                                        {}\n",
      "call_function  slice_46                                                                 aten.slice.Tensor                                                        (slice_45, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_40                                                             aten.unsqueeze.default                                                   (slice_46, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_41                                                             aten.unsqueeze.default                                                   (unsqueeze_40, 3)                                                                                                                                              {}\n",
      "call_function  add_90                                                                   aten.add.Tensor                                                          (conv2d_56, unsqueeze_41)                                                                                                                                      {}\n",
      "call_function  group_norm_52                                                            aten.group_norm.default                                                  (add_90, 32, p_up_blocks_3_resnets_0_norm2_weight, p_up_blocks_3_resnets_0_norm2_bias)                                                                         {}\n",
      "call_function  silu_60                                                                  aten.silu.default                                                        (group_norm_52,)                                                                                                                                               {}\n",
      "call_function  dropout_58                                                               aten.dropout.default                                                     (silu_60, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_57                                                                aten.conv2d.default                                                      (dropout_58, p_up_blocks_3_resnets_0_conv2_weight, p_up_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_58                                                                aten.conv2d.default                                                      (cat_11, p_up_blocks_3_resnets_0_conv_shortcut_weight, p_up_blocks_3_resnets_0_conv_shortcut_bias)                                                             {}\n",
      "call_function  add_91                                                                   aten.add.Tensor                                                          (conv2d_58, conv2d_57)                                                                                                                                         {}\n",
      "call_function  div_46                                                                   aten.div.Tensor                                                          (add_91, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_53                                                            aten.group_norm.default                                                  (div_46, 32, p_up_blocks_3_attentions_0_norm_weight, p_up_blocks_3_attentions_0_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_26                                                               aten.permute.default                                                     (group_norm_53, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_130                                                                 aten.view.default                                                        (permute_26, [1, 4096, 320])                                                                                                                                   {}\n",
      "call_function  linear_178                                                               aten.linear.default                                                      (view_130, p_up_blocks_3_attentions_0_proj_in_weight, p_up_blocks_3_attentions_0_proj_in_bias)                                                                 {}\n",
      "call_function  layer_norm_39                                                            aten.layer_norm.default                                                  (linear_178, [320], p_up_blocks_3_attentions_0_transformer_blocks_0_norm1_weight, p_up_blocks_3_attentions_0_transformer_blocks_0_norm1_bias)                  {}\n",
      "call_function  linear_179                                                               aten.linear.default                                                      (layer_norm_39, p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_180                                                               aten.linear.default                                                      (layer_norm_39, p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_181                                                               aten.linear.default                                                      (layer_norm_39, p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_131                                                                 aten.view.default                                                        (linear_179, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_104                                                            aten.transpose.int                                                       (view_131, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_132                                                                 aten.view.default                                                        (linear_180, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_105                                                            aten.transpose.int                                                       (view_132, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_133                                                                 aten.view.default                                                        (linear_181, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_106                                                            aten.transpose.int                                                       (view_133, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_26                                          aten.scaled_dot_product_attention.default                                (transpose_104, transpose_105, transpose_106)                                                                                                                  {}\n",
      "call_function  transpose_107                                                            aten.transpose.int                                                       (scaled_dot_product_attention_26, 1, 2)                                                                                                                        {}\n",
      "call_function  view_134                                                                 aten.view.default                                                        (transpose_107, [1, -1, 320])                                                                                                                                  {}\n",
      "call_function  _to_copy_28                                                              aten._to_copy.default                                                    (view_134,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_182                                                               aten.linear.default                                                      (_to_copy_28, p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_59                                                               aten.dropout.default                                                     (linear_182, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_47                                                                   aten.div.Tensor                                                          (dropout_59, 1.0)                                                                                                                                              {}\n",
      "call_function  add_92                                                                   aten.add.Tensor                                                          (div_47, linear_178)                                                                                                                                           {}\n",
      "call_function  layer_norm_40                                                            aten.layer_norm.default                                                  (add_92, [320], p_up_blocks_3_attentions_0_transformer_blocks_0_norm2_weight, p_up_blocks_3_attentions_0_transformer_blocks_0_norm2_bias)                      {}\n",
      "call_function  linear_183                                                               aten.linear.default                                                      (layer_norm_40, p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_184                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_185                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_135                                                                 aten.view.default                                                        (linear_183, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_108                                                            aten.transpose.int                                                       (view_135, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_136                                                                 aten.view.default                                                        (linear_184, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_109                                                            aten.transpose.int                                                       (view_136, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_137                                                                 aten.view.default                                                        (linear_185, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_110                                                            aten.transpose.int                                                       (view_137, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_27                                          aten.scaled_dot_product_attention.default                                (transpose_108, transpose_109, transpose_110)                                                                                                                  {}\n",
      "call_function  transpose_111                                                            aten.transpose.int                                                       (scaled_dot_product_attention_27, 1, 2)                                                                                                                        {}\n",
      "call_function  view_138                                                                 aten.view.default                                                        (transpose_111, [1, -1, 320])                                                                                                                                  {}\n",
      "call_function  _to_copy_29                                                              aten._to_copy.default                                                    (view_138,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_186                                                               aten.linear.default                                                      (_to_copy_29, p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_60                                                               aten.dropout.default                                                     (linear_186, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_48                                                                   aten.div.Tensor                                                          (dropout_60, 1.0)                                                                                                                                              {}\n",
      "call_function  add_93                                                                   aten.add.Tensor                                                          (div_48, add_92)                                                                                                                                               {}\n",
      "call_function  layer_norm_41                                                            aten.layer_norm.default                                                  (add_93, [320], p_up_blocks_3_attentions_0_transformer_blocks_0_norm3_weight, p_up_blocks_3_attentions_0_transformer_blocks_0_norm3_bias)                      {}\n",
      "call_function  linear_187                                                               aten.linear.default                                                      (layer_norm_41, p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_13                                                                 aten.split.Tensor                                                        (linear_187, 1280, -1)                                                                                                                                         {}\n",
      "call_function  getitem_26                                                               <built-in function getitem>                                              (split_13, 0)                                                                                                                                                  {}\n",
      "call_function  getitem_27                                                               <built-in function getitem>                                              (split_13, 1)                                                                                                                                                  {}\n",
      "call_function  gelu_13                                                                  aten.gelu.default                                                        (getitem_27,)                                                                                                                                                  {}\n",
      "call_function  mul_16                                                                   aten.mul.Tensor                                                          (getitem_26, gelu_13)                                                                                                                                          {}\n",
      "call_function  dropout_61                                                               aten.dropout.default                                                     (mul_16, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_188                                                               aten.linear.default                                                      (dropout_61, p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_weight, p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_94                                                                   aten.add.Tensor                                                          (linear_188, add_93)                                                                                                                                           {}\n",
      "call_function  linear_189                                                               aten.linear.default                                                      (add_94, p_up_blocks_3_attentions_0_proj_out_weight, p_up_blocks_3_attentions_0_proj_out_bias)                                                                 {}\n",
      "call_function  view_139                                                                 aten.view.default                                                        (linear_189, [1, 64, 64, 320])                                                                                                                                 {}\n",
      "call_function  permute_27                                                               aten.permute.default                                                     (view_139, [0, 3, 1, 2])                                                                                                                                       {}\n",
      "call_function  clone_13                                                                 aten.clone.default                                                       (permute_27,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_95                                                                   aten.add.Tensor                                                          (clone_13, div_46)                                                                                                                                             {}\n",
      "call_function  cat_12                                                                   aten.cat.default                                                         ([add_95, add_5], 1)                                                                                                                                           {}\n",
      "call_function  group_norm_54                                                            aten.group_norm.default                                                  (cat_12, 32, p_up_blocks_3_resnets_1_norm1_weight, p_up_blocks_3_resnets_1_norm1_bias)                                                                         {}\n",
      "call_function  silu_61                                                                  aten.silu.default                                                        (group_norm_54,)                                                                                                                                               {}\n",
      "call_function  conv2d_59                                                                aten.conv2d.default                                                      (silu_61, p_up_blocks_3_resnets_1_conv1_weight, p_up_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_62                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_190                                                               aten.linear.default                                                      (silu_62, p_up_blocks_3_resnets_1_time_emb_proj_weight, p_up_blocks_3_resnets_1_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_47                                                                 aten.slice.Tensor                                                        (linear_190, 0, 0, 9223372036854775807)                                                                                                                        {}\n",
      "call_function  slice_48                                                                 aten.slice.Tensor                                                        (slice_47, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_42                                                             aten.unsqueeze.default                                                   (slice_48, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_43                                                             aten.unsqueeze.default                                                   (unsqueeze_42, 3)                                                                                                                                              {}\n",
      "call_function  add_96                                                                   aten.add.Tensor                                                          (conv2d_59, unsqueeze_43)                                                                                                                                      {}\n",
      "call_function  group_norm_55                                                            aten.group_norm.default                                                  (add_96, 32, p_up_blocks_3_resnets_1_norm2_weight, p_up_blocks_3_resnets_1_norm2_bias)                                                                         {}\n",
      "call_function  silu_63                                                                  aten.silu.default                                                        (group_norm_55,)                                                                                                                                               {}\n",
      "call_function  dropout_62                                                               aten.dropout.default                                                     (silu_63, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_60                                                                aten.conv2d.default                                                      (dropout_62, p_up_blocks_3_resnets_1_conv2_weight, p_up_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_61                                                                aten.conv2d.default                                                      (cat_12, p_up_blocks_3_resnets_1_conv_shortcut_weight, p_up_blocks_3_resnets_1_conv_shortcut_bias)                                                             {}\n",
      "call_function  add_97                                                                   aten.add.Tensor                                                          (conv2d_61, conv2d_60)                                                                                                                                         {}\n",
      "call_function  div_49                                                                   aten.div.Tensor                                                          (add_97, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_56                                                            aten.group_norm.default                                                  (div_49, 32, p_up_blocks_3_attentions_1_norm_weight, p_up_blocks_3_attentions_1_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_28                                                               aten.permute.default                                                     (group_norm_56, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_140                                                                 aten.view.default                                                        (permute_28, [1, 4096, 320])                                                                                                                                   {}\n",
      "call_function  linear_191                                                               aten.linear.default                                                      (view_140, p_up_blocks_3_attentions_1_proj_in_weight, p_up_blocks_3_attentions_1_proj_in_bias)                                                                 {}\n",
      "call_function  layer_norm_42                                                            aten.layer_norm.default                                                  (linear_191, [320], p_up_blocks_3_attentions_1_transformer_blocks_0_norm1_weight, p_up_blocks_3_attentions_1_transformer_blocks_0_norm1_bias)                  {}\n",
      "call_function  linear_192                                                               aten.linear.default                                                      (layer_norm_42, p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_193                                                               aten.linear.default                                                      (layer_norm_42, p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_194                                                               aten.linear.default                                                      (layer_norm_42, p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_141                                                                 aten.view.default                                                        (linear_192, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_112                                                            aten.transpose.int                                                       (view_141, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_142                                                                 aten.view.default                                                        (linear_193, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_113                                                            aten.transpose.int                                                       (view_142, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_143                                                                 aten.view.default                                                        (linear_194, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_114                                                            aten.transpose.int                                                       (view_143, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_28                                          aten.scaled_dot_product_attention.default                                (transpose_112, transpose_113, transpose_114)                                                                                                                  {}\n",
      "call_function  transpose_115                                                            aten.transpose.int                                                       (scaled_dot_product_attention_28, 1, 2)                                                                                                                        {}\n",
      "call_function  view_144                                                                 aten.view.default                                                        (transpose_115, [1, -1, 320])                                                                                                                                  {}\n",
      "call_function  _to_copy_30                                                              aten._to_copy.default                                                    (view_144,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_195                                                               aten.linear.default                                                      (_to_copy_30, p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_63                                                               aten.dropout.default                                                     (linear_195, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_50                                                                   aten.div.Tensor                                                          (dropout_63, 1.0)                                                                                                                                              {}\n",
      "call_function  add_98                                                                   aten.add.Tensor                                                          (div_50, linear_191)                                                                                                                                           {}\n",
      "call_function  layer_norm_43                                                            aten.layer_norm.default                                                  (add_98, [320], p_up_blocks_3_attentions_1_transformer_blocks_0_norm2_weight, p_up_blocks_3_attentions_1_transformer_blocks_0_norm2_bias)                      {}\n",
      "call_function  linear_196                                                               aten.linear.default                                                      (layer_norm_43, p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_197                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_198                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_145                                                                 aten.view.default                                                        (linear_196, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_116                                                            aten.transpose.int                                                       (view_145, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_146                                                                 aten.view.default                                                        (linear_197, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_117                                                            aten.transpose.int                                                       (view_146, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_147                                                                 aten.view.default                                                        (linear_198, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_118                                                            aten.transpose.int                                                       (view_147, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_29                                          aten.scaled_dot_product_attention.default                                (transpose_116, transpose_117, transpose_118)                                                                                                                  {}\n",
      "call_function  transpose_119                                                            aten.transpose.int                                                       (scaled_dot_product_attention_29, 1, 2)                                                                                                                        {}\n",
      "call_function  view_148                                                                 aten.view.default                                                        (transpose_119, [1, -1, 320])                                                                                                                                  {}\n",
      "call_function  _to_copy_31                                                              aten._to_copy.default                                                    (view_148,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_199                                                               aten.linear.default                                                      (_to_copy_31, p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_64                                                               aten.dropout.default                                                     (linear_199, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_51                                                                   aten.div.Tensor                                                          (dropout_64, 1.0)                                                                                                                                              {}\n",
      "call_function  add_99                                                                   aten.add.Tensor                                                          (div_51, add_98)                                                                                                                                               {}\n",
      "call_function  layer_norm_44                                                            aten.layer_norm.default                                                  (add_99, [320], p_up_blocks_3_attentions_1_transformer_blocks_0_norm3_weight, p_up_blocks_3_attentions_1_transformer_blocks_0_norm3_bias)                      {}\n",
      "call_function  linear_200                                                               aten.linear.default                                                      (layer_norm_44, p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_14                                                                 aten.split.Tensor                                                        (linear_200, 1280, -1)                                                                                                                                         {}\n",
      "call_function  getitem_28                                                               <built-in function getitem>                                              (split_14, 0)                                                                                                                                                  {}\n",
      "call_function  getitem_29                                                               <built-in function getitem>                                              (split_14, 1)                                                                                                                                                  {}\n",
      "call_function  gelu_14                                                                  aten.gelu.default                                                        (getitem_29,)                                                                                                                                                  {}\n",
      "call_function  mul_17                                                                   aten.mul.Tensor                                                          (getitem_28, gelu_14)                                                                                                                                          {}\n",
      "call_function  dropout_65                                                               aten.dropout.default                                                     (mul_17, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_201                                                               aten.linear.default                                                      (dropout_65, p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_weight, p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_100                                                                  aten.add.Tensor                                                          (linear_201, add_99)                                                                                                                                           {}\n",
      "call_function  linear_202                                                               aten.linear.default                                                      (add_100, p_up_blocks_3_attentions_1_proj_out_weight, p_up_blocks_3_attentions_1_proj_out_bias)                                                                {}\n",
      "call_function  view_149                                                                 aten.view.default                                                        (linear_202, [1, 64, 64, 320])                                                                                                                                 {}\n",
      "call_function  permute_29                                                               aten.permute.default                                                     (view_149, [0, 3, 1, 2])                                                                                                                                       {}\n",
      "call_function  clone_14                                                                 aten.clone.default                                                       (permute_29,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_101                                                                  aten.add.Tensor                                                          (clone_14, div_49)                                                                                                                                             {}\n",
      "call_function  cat_13                                                                   aten.cat.default                                                         ([add_101, conv2d], 1)                                                                                                                                         {}\n",
      "call_function  group_norm_57                                                            aten.group_norm.default                                                  (cat_13, 32, p_up_blocks_3_resnets_2_norm1_weight, p_up_blocks_3_resnets_2_norm1_bias)                                                                         {}\n",
      "call_function  silu_64                                                                  aten.silu.default                                                        (group_norm_57,)                                                                                                                                               {}\n",
      "call_function  conv2d_62                                                                aten.conv2d.default                                                      (silu_64, p_up_blocks_3_resnets_2_conv1_weight, p_up_blocks_3_resnets_2_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_65                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_203                                                               aten.linear.default                                                      (silu_65, p_up_blocks_3_resnets_2_time_emb_proj_weight, p_up_blocks_3_resnets_2_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_49                                                                 aten.slice.Tensor                                                        (linear_203, 0, 0, 9223372036854775807)                                                                                                                        {}\n",
      "call_function  slice_50                                                                 aten.slice.Tensor                                                        (slice_49, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_44                                                             aten.unsqueeze.default                                                   (slice_50, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_45                                                             aten.unsqueeze.default                                                   (unsqueeze_44, 3)                                                                                                                                              {}\n",
      "call_function  add_102                                                                  aten.add.Tensor                                                          (conv2d_62, unsqueeze_45)                                                                                                                                      {}\n",
      "call_function  group_norm_58                                                            aten.group_norm.default                                                  (add_102, 32, p_up_blocks_3_resnets_2_norm2_weight, p_up_blocks_3_resnets_2_norm2_bias)                                                                        {}\n",
      "call_function  silu_66                                                                  aten.silu.default                                                        (group_norm_58,)                                                                                                                                               {}\n",
      "call_function  dropout_66                                                               aten.dropout.default                                                     (silu_66, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_63                                                                aten.conv2d.default                                                      (dropout_66, p_up_blocks_3_resnets_2_conv2_weight, p_up_blocks_3_resnets_2_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_64                                                                aten.conv2d.default                                                      (cat_13, p_up_blocks_3_resnets_2_conv_shortcut_weight, p_up_blocks_3_resnets_2_conv_shortcut_bias)                                                             {}\n",
      "call_function  add_103                                                                  aten.add.Tensor                                                          (conv2d_64, conv2d_63)                                                                                                                                         {}\n",
      "call_function  div_52                                                                   aten.div.Tensor                                                          (add_103, 1.0)                                                                                                                                                 {}\n",
      "call_function  group_norm_59                                                            aten.group_norm.default                                                  (div_52, 32, p_up_blocks_3_attentions_2_norm_weight, p_up_blocks_3_attentions_2_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_30                                                               aten.permute.default                                                     (group_norm_59, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_150                                                                 aten.view.default                                                        (permute_30, [1, 4096, 320])                                                                                                                                   {}\n",
      "call_function  linear_204                                                               aten.linear.default                                                      (view_150, p_up_blocks_3_attentions_2_proj_in_weight, p_up_blocks_3_attentions_2_proj_in_bias)                                                                 {}\n",
      "call_function  layer_norm_45                                                            aten.layer_norm.default                                                  (linear_204, [320], p_up_blocks_3_attentions_2_transformer_blocks_0_norm1_weight, p_up_blocks_3_attentions_2_transformer_blocks_0_norm1_bias)                  {}\n",
      "call_function  linear_205                                                               aten.linear.default                                                      (layer_norm_45, p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_206                                                               aten.linear.default                                                      (layer_norm_45, p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_207                                                               aten.linear.default                                                      (layer_norm_45, p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_151                                                                 aten.view.default                                                        (linear_205, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_120                                                            aten.transpose.int                                                       (view_151, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_152                                                                 aten.view.default                                                        (linear_206, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_121                                                            aten.transpose.int                                                       (view_152, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_153                                                                 aten.view.default                                                        (linear_207, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_122                                                            aten.transpose.int                                                       (view_153, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_30                                          aten.scaled_dot_product_attention.default                                (transpose_120, transpose_121, transpose_122)                                                                                                                  {}\n",
      "call_function  transpose_123                                                            aten.transpose.int                                                       (scaled_dot_product_attention_30, 1, 2)                                                                                                                        {}\n",
      "call_function  view_154                                                                 aten.view.default                                                        (transpose_123, [1, -1, 320])                                                                                                                                  {}\n",
      "call_function  _to_copy_32                                                              aten._to_copy.default                                                    (view_154,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_208                                                               aten.linear.default                                                      (_to_copy_32, p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_67                                                               aten.dropout.default                                                     (linear_208, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_53                                                                   aten.div.Tensor                                                          (dropout_67, 1.0)                                                                                                                                              {}\n",
      "call_function  add_104                                                                  aten.add.Tensor                                                          (div_53, linear_204)                                                                                                                                           {}\n",
      "call_function  layer_norm_46                                                            aten.layer_norm.default                                                  (add_104, [320], p_up_blocks_3_attentions_2_transformer_blocks_0_norm2_weight, p_up_blocks_3_attentions_2_transformer_blocks_0_norm2_bias)                     {}\n",
      "call_function  linear_209                                                               aten.linear.default                                                      (layer_norm_46, p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_210                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_211                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_155                                                                 aten.view.default                                                        (linear_209, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_124                                                            aten.transpose.int                                                       (view_155, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_156                                                                 aten.view.default                                                        (linear_210, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_125                                                            aten.transpose.int                                                       (view_156, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_157                                                                 aten.view.default                                                        (linear_211, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_126                                                            aten.transpose.int                                                       (view_157, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_31                                          aten.scaled_dot_product_attention.default                                (transpose_124, transpose_125, transpose_126)                                                                                                                  {}\n",
      "call_function  transpose_127                                                            aten.transpose.int                                                       (scaled_dot_product_attention_31, 1, 2)                                                                                                                        {}\n",
      "call_function  view_158                                                                 aten.view.default                                                        (transpose_127, [1, -1, 320])                                                                                                                                  {}\n",
      "call_function  _to_copy_33                                                              aten._to_copy.default                                                    (view_158,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_212                                                               aten.linear.default                                                      (_to_copy_33, p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_68                                                               aten.dropout.default                                                     (linear_212, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_54                                                                   aten.div.Tensor                                                          (dropout_68, 1.0)                                                                                                                                              {}\n",
      "call_function  add_105                                                                  aten.add.Tensor                                                          (div_54, add_104)                                                                                                                                              {}\n",
      "call_function  layer_norm_47                                                            aten.layer_norm.default                                                  (add_105, [320], p_up_blocks_3_attentions_2_transformer_blocks_0_norm3_weight, p_up_blocks_3_attentions_2_transformer_blocks_0_norm3_bias)                     {}\n",
      "call_function  linear_213                                                               aten.linear.default                                                      (layer_norm_47, p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_15                                                                 aten.split.Tensor                                                        (linear_213, 1280, -1)                                                                                                                                         {}\n",
      "call_function  getitem_30                                                               <built-in function getitem>                                              (split_15, 0)                                                                                                                                                  {}\n",
      "call_function  getitem_31                                                               <built-in function getitem>                                              (split_15, 1)                                                                                                                                                  {}\n",
      "call_function  gelu_15                                                                  aten.gelu.default                                                        (getitem_31,)                                                                                                                                                  {}\n",
      "call_function  mul_18                                                                   aten.mul.Tensor                                                          (getitem_30, gelu_15)                                                                                                                                          {}\n",
      "call_function  dropout_69                                                               aten.dropout.default                                                     (mul_18, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_214                                                               aten.linear.default                                                      (dropout_69, p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_weight, p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_106                                                                  aten.add.Tensor                                                          (linear_214, add_105)                                                                                                                                          {}\n",
      "call_function  linear_215                                                               aten.linear.default                                                      (add_106, p_up_blocks_3_attentions_2_proj_out_weight, p_up_blocks_3_attentions_2_proj_out_bias)                                                                {}\n",
      "call_function  view_159                                                                 aten.view.default                                                        (linear_215, [1, 64, 64, 320])                                                                                                                                 {}\n",
      "call_function  permute_31                                                               aten.permute.default                                                     (view_159, [0, 3, 1, 2])                                                                                                                                       {}\n",
      "call_function  clone_15                                                                 aten.clone.default                                                       (permute_31,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_107                                                                  aten.add.Tensor                                                          (clone_15, div_52)                                                                                                                                             {}\n",
      "call_function  group_norm_60                                                            aten.group_norm.default                                                  (add_107, 32, p_conv_norm_out_weight, p_conv_norm_out_bias)                                                                                                    {}\n",
      "call_function  silu_67                                                                  aten.silu.default                                                        (group_norm_60,)                                                                                                                                               {}\n",
      "call_function  conv2d_65                                                                aten.conv2d.default                                                      (silu_67, p_conv_out_weight, p_conv_out_bias, [1, 1], [1, 1])                                                                                                  {}\n",
      "output         output                                                                   output                                                                   ((conv2d_65,),)                                                                                                                                                {}\n",
      "graph():\n",
      "    %p_time_embedding_linear_1_weight : [num_users=1] = placeholder[target=p_time_embedding_linear_1_weight]\n",
      "    %p_time_embedding_linear_1_bias : [num_users=1] = placeholder[target=p_time_embedding_linear_1_bias]\n",
      "    %p_time_embedding_linear_2_weight : [num_users=1] = placeholder[target=p_time_embedding_linear_2_weight]\n",
      "    %p_time_embedding_linear_2_bias : [num_users=1] = placeholder[target=p_time_embedding_linear_2_bias]\n",
      "    %p_conv_in_weight : [num_users=1] = placeholder[target=p_conv_in_weight]\n",
      "    %p_conv_in_bias : [num_users=1] = placeholder[target=p_conv_in_bias]\n",
      "    %p_down_blocks_0_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_down_blocks_0_resnets_0_norm1_weight]\n",
      "    %p_down_blocks_0_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_down_blocks_0_resnets_0_norm1_bias]\n",
      "    %p_down_blocks_0_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_down_blocks_0_resnets_0_conv1_weight]\n",
      "    %p_down_blocks_0_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_down_blocks_0_resnets_0_conv1_bias]\n",
      "    %p_down_blocks_0_resnets_0_time_emb_proj_weight : [num_users=1] = placeholder[target=p_down_blocks_0_resnets_0_time_emb_proj_weight]\n",
      "    %p_down_blocks_0_resnets_0_time_emb_proj_bias : [num_users=1] = placeholder[target=p_down_blocks_0_resnets_0_time_emb_proj_bias]\n",
      "    %p_down_blocks_0_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_down_blocks_0_resnets_0_norm2_weight]\n",
      "    %p_down_blocks_0_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_down_blocks_0_resnets_0_norm2_bias]\n",
      "    %p_down_blocks_0_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_down_blocks_0_resnets_0_conv2_weight]\n",
      "    %p_down_blocks_0_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_down_blocks_0_resnets_0_conv2_bias]\n",
      "    %p_down_blocks_0_attentions_0_norm_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_norm_weight]\n",
      "    %p_down_blocks_0_attentions_0_norm_bias : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_norm_bias]\n",
      "    %p_down_blocks_0_attentions_0_proj_in_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_proj_in_weight]\n",
      "    %p_down_blocks_0_attentions_0_proj_in_bias : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_proj_in_bias]\n",
      "    %p_down_blocks_0_attentions_0_transformer_blocks_0_norm1_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_transformer_blocks_0_norm1_weight]\n",
      "    %p_down_blocks_0_attentions_0_transformer_blocks_0_norm1_bias : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_transformer_blocks_0_norm1_bias]\n",
      "    %p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q_weight]\n",
      "    %p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k_weight]\n",
      "    %p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v_weight]\n",
      "    %p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_weight]\n",
      "    %p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias]\n",
      "    %p_down_blocks_0_attentions_0_transformer_blocks_0_norm2_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_transformer_blocks_0_norm2_weight]\n",
      "    %p_down_blocks_0_attentions_0_transformer_blocks_0_norm2_bias : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_transformer_blocks_0_norm2_bias]\n",
      "    %p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q_weight]\n",
      "    %p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k_weight]\n",
      "    %p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v_weight]\n",
      "    %p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_weight]\n",
      "    %p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_bias : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_bias]\n",
      "    %p_down_blocks_0_attentions_0_transformer_blocks_0_norm3_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_transformer_blocks_0_norm3_weight]\n",
      "    %p_down_blocks_0_attentions_0_transformer_blocks_0_norm3_bias : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_transformer_blocks_0_norm3_bias]\n",
      "    %p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_weight]\n",
      "    %p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_bias : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_bias]\n",
      "    %p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_weight]\n",
      "    %p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias]\n",
      "    %p_down_blocks_0_attentions_0_proj_out_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_proj_out_weight]\n",
      "    %p_down_blocks_0_attentions_0_proj_out_bias : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_0_proj_out_bias]\n",
      "    %p_down_blocks_0_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_down_blocks_0_resnets_1_norm1_weight]\n",
      "    %p_down_blocks_0_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_down_blocks_0_resnets_1_norm1_bias]\n",
      "    %p_down_blocks_0_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_down_blocks_0_resnets_1_conv1_weight]\n",
      "    %p_down_blocks_0_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_down_blocks_0_resnets_1_conv1_bias]\n",
      "    %p_down_blocks_0_resnets_1_time_emb_proj_weight : [num_users=1] = placeholder[target=p_down_blocks_0_resnets_1_time_emb_proj_weight]\n",
      "    %p_down_blocks_0_resnets_1_time_emb_proj_bias : [num_users=1] = placeholder[target=p_down_blocks_0_resnets_1_time_emb_proj_bias]\n",
      "    %p_down_blocks_0_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_down_blocks_0_resnets_1_norm2_weight]\n",
      "    %p_down_blocks_0_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_down_blocks_0_resnets_1_norm2_bias]\n",
      "    %p_down_blocks_0_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_down_blocks_0_resnets_1_conv2_weight]\n",
      "    %p_down_blocks_0_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_down_blocks_0_resnets_1_conv2_bias]\n",
      "    %p_down_blocks_0_attentions_1_norm_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_norm_weight]\n",
      "    %p_down_blocks_0_attentions_1_norm_bias : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_norm_bias]\n",
      "    %p_down_blocks_0_attentions_1_proj_in_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_proj_in_weight]\n",
      "    %p_down_blocks_0_attentions_1_proj_in_bias : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_proj_in_bias]\n",
      "    %p_down_blocks_0_attentions_1_transformer_blocks_0_norm1_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_transformer_blocks_0_norm1_weight]\n",
      "    %p_down_blocks_0_attentions_1_transformer_blocks_0_norm1_bias : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_transformer_blocks_0_norm1_bias]\n",
      "    %p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q_weight]\n",
      "    %p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k_weight]\n",
      "    %p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v_weight]\n",
      "    %p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_weight]\n",
      "    %p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_bias : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_bias]\n",
      "    %p_down_blocks_0_attentions_1_transformer_blocks_0_norm2_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_transformer_blocks_0_norm2_weight]\n",
      "    %p_down_blocks_0_attentions_1_transformer_blocks_0_norm2_bias : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_transformer_blocks_0_norm2_bias]\n",
      "    %p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q_weight]\n",
      "    %p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k_weight]\n",
      "    %p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v_weight]\n",
      "    %p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_weight]\n",
      "    %p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_bias : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_bias]\n",
      "    %p_down_blocks_0_attentions_1_transformer_blocks_0_norm3_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_transformer_blocks_0_norm3_weight]\n",
      "    %p_down_blocks_0_attentions_1_transformer_blocks_0_norm3_bias : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_transformer_blocks_0_norm3_bias]\n",
      "    %p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_weight]\n",
      "    %p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_bias : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_bias]\n",
      "    %p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_weight]\n",
      "    %p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_bias : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_bias]\n",
      "    %p_down_blocks_0_attentions_1_proj_out_weight : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_proj_out_weight]\n",
      "    %p_down_blocks_0_attentions_1_proj_out_bias : [num_users=1] = placeholder[target=p_down_blocks_0_attentions_1_proj_out_bias]\n",
      "    %p_down_blocks_0_downsamplers_0_conv_weight : [num_users=1] = placeholder[target=p_down_blocks_0_downsamplers_0_conv_weight]\n",
      "    %p_down_blocks_0_downsamplers_0_conv_bias : [num_users=1] = placeholder[target=p_down_blocks_0_downsamplers_0_conv_bias]\n",
      "    %p_down_blocks_1_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_down_blocks_1_resnets_0_norm1_weight]\n",
      "    %p_down_blocks_1_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_down_blocks_1_resnets_0_norm1_bias]\n",
      "    %p_down_blocks_1_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_down_blocks_1_resnets_0_conv1_weight]\n",
      "    %p_down_blocks_1_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_down_blocks_1_resnets_0_conv1_bias]\n",
      "    %p_down_blocks_1_resnets_0_time_emb_proj_weight : [num_users=1] = placeholder[target=p_down_blocks_1_resnets_0_time_emb_proj_weight]\n",
      "    %p_down_blocks_1_resnets_0_time_emb_proj_bias : [num_users=1] = placeholder[target=p_down_blocks_1_resnets_0_time_emb_proj_bias]\n",
      "    %p_down_blocks_1_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_down_blocks_1_resnets_0_norm2_weight]\n",
      "    %p_down_blocks_1_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_down_blocks_1_resnets_0_norm2_bias]\n",
      "    %p_down_blocks_1_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_down_blocks_1_resnets_0_conv2_weight]\n",
      "    %p_down_blocks_1_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_down_blocks_1_resnets_0_conv2_bias]\n",
      "    %p_down_blocks_1_resnets_0_conv_shortcut_weight : [num_users=1] = placeholder[target=p_down_blocks_1_resnets_0_conv_shortcut_weight]\n",
      "    %p_down_blocks_1_resnets_0_conv_shortcut_bias : [num_users=1] = placeholder[target=p_down_blocks_1_resnets_0_conv_shortcut_bias]\n",
      "    %p_down_blocks_1_attentions_0_norm_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_norm_weight]\n",
      "    %p_down_blocks_1_attentions_0_norm_bias : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_norm_bias]\n",
      "    %p_down_blocks_1_attentions_0_proj_in_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_proj_in_weight]\n",
      "    %p_down_blocks_1_attentions_0_proj_in_bias : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_proj_in_bias]\n",
      "    %p_down_blocks_1_attentions_0_transformer_blocks_0_norm1_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_transformer_blocks_0_norm1_weight]\n",
      "    %p_down_blocks_1_attentions_0_transformer_blocks_0_norm1_bias : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_transformer_blocks_0_norm1_bias]\n",
      "    %p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight]\n",
      "    %p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight]\n",
      "    %p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight]\n",
      "    %p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight]\n",
      "    %p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias]\n",
      "    %p_down_blocks_1_attentions_0_transformer_blocks_0_norm2_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_transformer_blocks_0_norm2_weight]\n",
      "    %p_down_blocks_1_attentions_0_transformer_blocks_0_norm2_bias : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_transformer_blocks_0_norm2_bias]\n",
      "    %p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight]\n",
      "    %p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight]\n",
      "    %p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight]\n",
      "    %p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight]\n",
      "    %p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias]\n",
      "    %p_down_blocks_1_attentions_0_transformer_blocks_0_norm3_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_transformer_blocks_0_norm3_weight]\n",
      "    %p_down_blocks_1_attentions_0_transformer_blocks_0_norm3_bias : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_transformer_blocks_0_norm3_bias]\n",
      "    %p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight]\n",
      "    %p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias]\n",
      "    %p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight]\n",
      "    %p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias]\n",
      "    %p_down_blocks_1_attentions_0_proj_out_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_proj_out_weight]\n",
      "    %p_down_blocks_1_attentions_0_proj_out_bias : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_0_proj_out_bias]\n",
      "    %p_down_blocks_1_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_down_blocks_1_resnets_1_norm1_weight]\n",
      "    %p_down_blocks_1_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_down_blocks_1_resnets_1_norm1_bias]\n",
      "    %p_down_blocks_1_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_down_blocks_1_resnets_1_conv1_weight]\n",
      "    %p_down_blocks_1_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_down_blocks_1_resnets_1_conv1_bias]\n",
      "    %p_down_blocks_1_resnets_1_time_emb_proj_weight : [num_users=1] = placeholder[target=p_down_blocks_1_resnets_1_time_emb_proj_weight]\n",
      "    %p_down_blocks_1_resnets_1_time_emb_proj_bias : [num_users=1] = placeholder[target=p_down_blocks_1_resnets_1_time_emb_proj_bias]\n",
      "    %p_down_blocks_1_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_down_blocks_1_resnets_1_norm2_weight]\n",
      "    %p_down_blocks_1_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_down_blocks_1_resnets_1_norm2_bias]\n",
      "    %p_down_blocks_1_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_down_blocks_1_resnets_1_conv2_weight]\n",
      "    %p_down_blocks_1_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_down_blocks_1_resnets_1_conv2_bias]\n",
      "    %p_down_blocks_1_attentions_1_norm_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_norm_weight]\n",
      "    %p_down_blocks_1_attentions_1_norm_bias : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_norm_bias]\n",
      "    %p_down_blocks_1_attentions_1_proj_in_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_proj_in_weight]\n",
      "    %p_down_blocks_1_attentions_1_proj_in_bias : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_proj_in_bias]\n",
      "    %p_down_blocks_1_attentions_1_transformer_blocks_0_norm1_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_transformer_blocks_0_norm1_weight]\n",
      "    %p_down_blocks_1_attentions_1_transformer_blocks_0_norm1_bias : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_transformer_blocks_0_norm1_bias]\n",
      "    %p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight]\n",
      "    %p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight]\n",
      "    %p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight]\n",
      "    %p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight]\n",
      "    %p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias]\n",
      "    %p_down_blocks_1_attentions_1_transformer_blocks_0_norm2_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_transformer_blocks_0_norm2_weight]\n",
      "    %p_down_blocks_1_attentions_1_transformer_blocks_0_norm2_bias : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_transformer_blocks_0_norm2_bias]\n",
      "    %p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight]\n",
      "    %p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight]\n",
      "    %p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight]\n",
      "    %p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight]\n",
      "    %p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias]\n",
      "    %p_down_blocks_1_attentions_1_transformer_blocks_0_norm3_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_transformer_blocks_0_norm3_weight]\n",
      "    %p_down_blocks_1_attentions_1_transformer_blocks_0_norm3_bias : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_transformer_blocks_0_norm3_bias]\n",
      "    %p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight]\n",
      "    %p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias]\n",
      "    %p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight]\n",
      "    %p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias]\n",
      "    %p_down_blocks_1_attentions_1_proj_out_weight : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_proj_out_weight]\n",
      "    %p_down_blocks_1_attentions_1_proj_out_bias : [num_users=1] = placeholder[target=p_down_blocks_1_attentions_1_proj_out_bias]\n",
      "    %p_down_blocks_1_downsamplers_0_conv_weight : [num_users=1] = placeholder[target=p_down_blocks_1_downsamplers_0_conv_weight]\n",
      "    %p_down_blocks_1_downsamplers_0_conv_bias : [num_users=1] = placeholder[target=p_down_blocks_1_downsamplers_0_conv_bias]\n",
      "    %p_down_blocks_2_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_down_blocks_2_resnets_0_norm1_weight]\n",
      "    %p_down_blocks_2_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_down_blocks_2_resnets_0_norm1_bias]\n",
      "    %p_down_blocks_2_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_down_blocks_2_resnets_0_conv1_weight]\n",
      "    %p_down_blocks_2_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_down_blocks_2_resnets_0_conv1_bias]\n",
      "    %p_down_blocks_2_resnets_0_time_emb_proj_weight : [num_users=1] = placeholder[target=p_down_blocks_2_resnets_0_time_emb_proj_weight]\n",
      "    %p_down_blocks_2_resnets_0_time_emb_proj_bias : [num_users=1] = placeholder[target=p_down_blocks_2_resnets_0_time_emb_proj_bias]\n",
      "    %p_down_blocks_2_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_down_blocks_2_resnets_0_norm2_weight]\n",
      "    %p_down_blocks_2_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_down_blocks_2_resnets_0_norm2_bias]\n",
      "    %p_down_blocks_2_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_down_blocks_2_resnets_0_conv2_weight]\n",
      "    %p_down_blocks_2_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_down_blocks_2_resnets_0_conv2_bias]\n",
      "    %p_down_blocks_2_resnets_0_conv_shortcut_weight : [num_users=1] = placeholder[target=p_down_blocks_2_resnets_0_conv_shortcut_weight]\n",
      "    %p_down_blocks_2_resnets_0_conv_shortcut_bias : [num_users=1] = placeholder[target=p_down_blocks_2_resnets_0_conv_shortcut_bias]\n",
      "    %p_down_blocks_2_attentions_0_norm_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_norm_weight]\n",
      "    %p_down_blocks_2_attentions_0_norm_bias : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_norm_bias]\n",
      "    %p_down_blocks_2_attentions_0_proj_in_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_proj_in_weight]\n",
      "    %p_down_blocks_2_attentions_0_proj_in_bias : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_proj_in_bias]\n",
      "    %p_down_blocks_2_attentions_0_transformer_blocks_0_norm1_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_transformer_blocks_0_norm1_weight]\n",
      "    %p_down_blocks_2_attentions_0_transformer_blocks_0_norm1_bias : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_transformer_blocks_0_norm1_bias]\n",
      "    %p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight]\n",
      "    %p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight]\n",
      "    %p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight]\n",
      "    %p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight]\n",
      "    %p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias]\n",
      "    %p_down_blocks_2_attentions_0_transformer_blocks_0_norm2_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_transformer_blocks_0_norm2_weight]\n",
      "    %p_down_blocks_2_attentions_0_transformer_blocks_0_norm2_bias : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_transformer_blocks_0_norm2_bias]\n",
      "    %p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight]\n",
      "    %p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight]\n",
      "    %p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight]\n",
      "    %p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight]\n",
      "    %p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias]\n",
      "    %p_down_blocks_2_attentions_0_transformer_blocks_0_norm3_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_transformer_blocks_0_norm3_weight]\n",
      "    %p_down_blocks_2_attentions_0_transformer_blocks_0_norm3_bias : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_transformer_blocks_0_norm3_bias]\n",
      "    %p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight]\n",
      "    %p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias]\n",
      "    %p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight]\n",
      "    %p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias]\n",
      "    %p_down_blocks_2_attentions_0_proj_out_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_proj_out_weight]\n",
      "    %p_down_blocks_2_attentions_0_proj_out_bias : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_0_proj_out_bias]\n",
      "    %p_down_blocks_2_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_down_blocks_2_resnets_1_norm1_weight]\n",
      "    %p_down_blocks_2_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_down_blocks_2_resnets_1_norm1_bias]\n",
      "    %p_down_blocks_2_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_down_blocks_2_resnets_1_conv1_weight]\n",
      "    %p_down_blocks_2_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_down_blocks_2_resnets_1_conv1_bias]\n",
      "    %p_down_blocks_2_resnets_1_time_emb_proj_weight : [num_users=1] = placeholder[target=p_down_blocks_2_resnets_1_time_emb_proj_weight]\n",
      "    %p_down_blocks_2_resnets_1_time_emb_proj_bias : [num_users=1] = placeholder[target=p_down_blocks_2_resnets_1_time_emb_proj_bias]\n",
      "    %p_down_blocks_2_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_down_blocks_2_resnets_1_norm2_weight]\n",
      "    %p_down_blocks_2_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_down_blocks_2_resnets_1_norm2_bias]\n",
      "    %p_down_blocks_2_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_down_blocks_2_resnets_1_conv2_weight]\n",
      "    %p_down_blocks_2_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_down_blocks_2_resnets_1_conv2_bias]\n",
      "    %p_down_blocks_2_attentions_1_norm_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_norm_weight]\n",
      "    %p_down_blocks_2_attentions_1_norm_bias : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_norm_bias]\n",
      "    %p_down_blocks_2_attentions_1_proj_in_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_proj_in_weight]\n",
      "    %p_down_blocks_2_attentions_1_proj_in_bias : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_proj_in_bias]\n",
      "    %p_down_blocks_2_attentions_1_transformer_blocks_0_norm1_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_transformer_blocks_0_norm1_weight]\n",
      "    %p_down_blocks_2_attentions_1_transformer_blocks_0_norm1_bias : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_transformer_blocks_0_norm1_bias]\n",
      "    %p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight]\n",
      "    %p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight]\n",
      "    %p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight]\n",
      "    %p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight]\n",
      "    %p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias]\n",
      "    %p_down_blocks_2_attentions_1_transformer_blocks_0_norm2_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_transformer_blocks_0_norm2_weight]\n",
      "    %p_down_blocks_2_attentions_1_transformer_blocks_0_norm2_bias : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_transformer_blocks_0_norm2_bias]\n",
      "    %p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight]\n",
      "    %p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight]\n",
      "    %p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight]\n",
      "    %p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight]\n",
      "    %p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias]\n",
      "    %p_down_blocks_2_attentions_1_transformer_blocks_0_norm3_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_transformer_blocks_0_norm3_weight]\n",
      "    %p_down_blocks_2_attentions_1_transformer_blocks_0_norm3_bias : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_transformer_blocks_0_norm3_bias]\n",
      "    %p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight]\n",
      "    %p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias]\n",
      "    %p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight]\n",
      "    %p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias]\n",
      "    %p_down_blocks_2_attentions_1_proj_out_weight : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_proj_out_weight]\n",
      "    %p_down_blocks_2_attentions_1_proj_out_bias : [num_users=1] = placeholder[target=p_down_blocks_2_attentions_1_proj_out_bias]\n",
      "    %p_down_blocks_2_downsamplers_0_conv_weight : [num_users=1] = placeholder[target=p_down_blocks_2_downsamplers_0_conv_weight]\n",
      "    %p_down_blocks_2_downsamplers_0_conv_bias : [num_users=1] = placeholder[target=p_down_blocks_2_downsamplers_0_conv_bias]\n",
      "    %p_down_blocks_3_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_down_blocks_3_resnets_0_norm1_weight]\n",
      "    %p_down_blocks_3_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_down_blocks_3_resnets_0_norm1_bias]\n",
      "    %p_down_blocks_3_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_down_blocks_3_resnets_0_conv1_weight]\n",
      "    %p_down_blocks_3_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_down_blocks_3_resnets_0_conv1_bias]\n",
      "    %p_down_blocks_3_resnets_0_time_emb_proj_weight : [num_users=1] = placeholder[target=p_down_blocks_3_resnets_0_time_emb_proj_weight]\n",
      "    %p_down_blocks_3_resnets_0_time_emb_proj_bias : [num_users=1] = placeholder[target=p_down_blocks_3_resnets_0_time_emb_proj_bias]\n",
      "    %p_down_blocks_3_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_down_blocks_3_resnets_0_norm2_weight]\n",
      "    %p_down_blocks_3_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_down_blocks_3_resnets_0_norm2_bias]\n",
      "    %p_down_blocks_3_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_down_blocks_3_resnets_0_conv2_weight]\n",
      "    %p_down_blocks_3_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_down_blocks_3_resnets_0_conv2_bias]\n",
      "    %p_down_blocks_3_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_down_blocks_3_resnets_1_norm1_weight]\n",
      "    %p_down_blocks_3_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_down_blocks_3_resnets_1_norm1_bias]\n",
      "    %p_down_blocks_3_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_down_blocks_3_resnets_1_conv1_weight]\n",
      "    %p_down_blocks_3_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_down_blocks_3_resnets_1_conv1_bias]\n",
      "    %p_down_blocks_3_resnets_1_time_emb_proj_weight : [num_users=1] = placeholder[target=p_down_blocks_3_resnets_1_time_emb_proj_weight]\n",
      "    %p_down_blocks_3_resnets_1_time_emb_proj_bias : [num_users=1] = placeholder[target=p_down_blocks_3_resnets_1_time_emb_proj_bias]\n",
      "    %p_down_blocks_3_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_down_blocks_3_resnets_1_norm2_weight]\n",
      "    %p_down_blocks_3_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_down_blocks_3_resnets_1_norm2_bias]\n",
      "    %p_down_blocks_3_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_down_blocks_3_resnets_1_conv2_weight]\n",
      "    %p_down_blocks_3_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_down_blocks_3_resnets_1_conv2_bias]\n",
      "    %p_mid_block_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_mid_block_resnets_0_norm1_weight]\n",
      "    %p_mid_block_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_mid_block_resnets_0_norm1_bias]\n",
      "    %p_mid_block_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_mid_block_resnets_0_conv1_weight]\n",
      "    %p_mid_block_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_mid_block_resnets_0_conv1_bias]\n",
      "    %p_mid_block_resnets_0_time_emb_proj_weight : [num_users=1] = placeholder[target=p_mid_block_resnets_0_time_emb_proj_weight]\n",
      "    %p_mid_block_resnets_0_time_emb_proj_bias : [num_users=1] = placeholder[target=p_mid_block_resnets_0_time_emb_proj_bias]\n",
      "    %p_mid_block_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_mid_block_resnets_0_norm2_weight]\n",
      "    %p_mid_block_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_mid_block_resnets_0_norm2_bias]\n",
      "    %p_mid_block_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_mid_block_resnets_0_conv2_weight]\n",
      "    %p_mid_block_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_mid_block_resnets_0_conv2_bias]\n",
      "    %p_mid_block_attentions_0_norm_weight : [num_users=1] = placeholder[target=p_mid_block_attentions_0_norm_weight]\n",
      "    %p_mid_block_attentions_0_norm_bias : [num_users=1] = placeholder[target=p_mid_block_attentions_0_norm_bias]\n",
      "    %p_mid_block_attentions_0_proj_in_weight : [num_users=1] = placeholder[target=p_mid_block_attentions_0_proj_in_weight]\n",
      "    %p_mid_block_attentions_0_proj_in_bias : [num_users=1] = placeholder[target=p_mid_block_attentions_0_proj_in_bias]\n",
      "    %p_mid_block_attentions_0_transformer_blocks_0_norm1_weight : [num_users=1] = placeholder[target=p_mid_block_attentions_0_transformer_blocks_0_norm1_weight]\n",
      "    %p_mid_block_attentions_0_transformer_blocks_0_norm1_bias : [num_users=1] = placeholder[target=p_mid_block_attentions_0_transformer_blocks_0_norm1_bias]\n",
      "    %p_mid_block_attentions_0_transformer_blocks_0_attn1_to_q_weight : [num_users=1] = placeholder[target=p_mid_block_attentions_0_transformer_blocks_0_attn1_to_q_weight]\n",
      "    %p_mid_block_attentions_0_transformer_blocks_0_attn1_to_k_weight : [num_users=1] = placeholder[target=p_mid_block_attentions_0_transformer_blocks_0_attn1_to_k_weight]\n",
      "    %p_mid_block_attentions_0_transformer_blocks_0_attn1_to_v_weight : [num_users=1] = placeholder[target=p_mid_block_attentions_0_transformer_blocks_0_attn1_to_v_weight]\n",
      "    %p_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_weight : [num_users=1] = placeholder[target=p_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_weight]\n",
      "    %p_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias : [num_users=1] = placeholder[target=p_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias]\n",
      "    %p_mid_block_attentions_0_transformer_blocks_0_norm2_weight : [num_users=1] = placeholder[target=p_mid_block_attentions_0_transformer_blocks_0_norm2_weight]\n",
      "    %p_mid_block_attentions_0_transformer_blocks_0_norm2_bias : [num_users=1] = placeholder[target=p_mid_block_attentions_0_transformer_blocks_0_norm2_bias]\n",
      "    %p_mid_block_attentions_0_transformer_blocks_0_attn2_to_q_weight : [num_users=1] = placeholder[target=p_mid_block_attentions_0_transformer_blocks_0_attn2_to_q_weight]\n",
      "    %p_mid_block_attentions_0_transformer_blocks_0_attn2_to_k_weight : [num_users=1] = placeholder[target=p_mid_block_attentions_0_transformer_blocks_0_attn2_to_k_weight]\n",
      "    %p_mid_block_attentions_0_transformer_blocks_0_attn2_to_v_weight : [num_users=1] = placeholder[target=p_mid_block_attentions_0_transformer_blocks_0_attn2_to_v_weight]\n",
      "    %p_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_weight : [num_users=1] = placeholder[target=p_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_weight]\n",
      "    %p_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_bias : [num_users=1] = placeholder[target=p_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_bias]\n",
      "    %p_mid_block_attentions_0_transformer_blocks_0_norm3_weight : [num_users=1] = placeholder[target=p_mid_block_attentions_0_transformer_blocks_0_norm3_weight]\n",
      "    %p_mid_block_attentions_0_transformer_blocks_0_norm3_bias : [num_users=1] = placeholder[target=p_mid_block_attentions_0_transformer_blocks_0_norm3_bias]\n",
      "    %p_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_weight : [num_users=1] = placeholder[target=p_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_weight]\n",
      "    %p_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_bias : [num_users=1] = placeholder[target=p_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_bias]\n",
      "    %p_mid_block_attentions_0_transformer_blocks_0_ff_net_2_weight : [num_users=1] = placeholder[target=p_mid_block_attentions_0_transformer_blocks_0_ff_net_2_weight]\n",
      "    %p_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias : [num_users=1] = placeholder[target=p_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias]\n",
      "    %p_mid_block_attentions_0_proj_out_weight : [num_users=1] = placeholder[target=p_mid_block_attentions_0_proj_out_weight]\n",
      "    %p_mid_block_attentions_0_proj_out_bias : [num_users=1] = placeholder[target=p_mid_block_attentions_0_proj_out_bias]\n",
      "    %p_mid_block_resnets_slice_1__none__none___0_norm1_weight : [num_users=1] = placeholder[target=p_mid_block_resnets_slice_1__none__none___0_norm1_weight]\n",
      "    %p_mid_block_resnets_slice_1__none__none___0_norm1_bias : [num_users=1] = placeholder[target=p_mid_block_resnets_slice_1__none__none___0_norm1_bias]\n",
      "    %p_mid_block_resnets_slice_1__none__none___0_conv1_weight : [num_users=1] = placeholder[target=p_mid_block_resnets_slice_1__none__none___0_conv1_weight]\n",
      "    %p_mid_block_resnets_slice_1__none__none___0_conv1_bias : [num_users=1] = placeholder[target=p_mid_block_resnets_slice_1__none__none___0_conv1_bias]\n",
      "    %p_mid_block_resnets_slice_1__none__none___0_time_emb_proj_weight : [num_users=1] = placeholder[target=p_mid_block_resnets_slice_1__none__none___0_time_emb_proj_weight]\n",
      "    %p_mid_block_resnets_slice_1__none__none___0_time_emb_proj_bias : [num_users=1] = placeholder[target=p_mid_block_resnets_slice_1__none__none___0_time_emb_proj_bias]\n",
      "    %p_mid_block_resnets_slice_1__none__none___0_norm2_weight : [num_users=1] = placeholder[target=p_mid_block_resnets_slice_1__none__none___0_norm2_weight]\n",
      "    %p_mid_block_resnets_slice_1__none__none___0_norm2_bias : [num_users=1] = placeholder[target=p_mid_block_resnets_slice_1__none__none___0_norm2_bias]\n",
      "    %p_mid_block_resnets_slice_1__none__none___0_conv2_weight : [num_users=1] = placeholder[target=p_mid_block_resnets_slice_1__none__none___0_conv2_weight]\n",
      "    %p_mid_block_resnets_slice_1__none__none___0_conv2_bias : [num_users=1] = placeholder[target=p_mid_block_resnets_slice_1__none__none___0_conv2_bias]\n",
      "    %p_up_blocks_0_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_0_norm1_weight]\n",
      "    %p_up_blocks_0_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_0_norm1_bias]\n",
      "    %p_up_blocks_0_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_0_conv1_weight]\n",
      "    %p_up_blocks_0_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_0_conv1_bias]\n",
      "    %p_up_blocks_0_resnets_0_time_emb_proj_weight : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_0_time_emb_proj_weight]\n",
      "    %p_up_blocks_0_resnets_0_time_emb_proj_bias : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_0_time_emb_proj_bias]\n",
      "    %p_up_blocks_0_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_0_norm2_weight]\n",
      "    %p_up_blocks_0_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_0_norm2_bias]\n",
      "    %p_up_blocks_0_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_0_conv2_weight]\n",
      "    %p_up_blocks_0_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_0_conv2_bias]\n",
      "    %p_up_blocks_0_resnets_0_conv_shortcut_weight : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_0_conv_shortcut_weight]\n",
      "    %p_up_blocks_0_resnets_0_conv_shortcut_bias : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_0_conv_shortcut_bias]\n",
      "    %p_up_blocks_0_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_1_norm1_weight]\n",
      "    %p_up_blocks_0_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_1_norm1_bias]\n",
      "    %p_up_blocks_0_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_1_conv1_weight]\n",
      "    %p_up_blocks_0_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_1_conv1_bias]\n",
      "    %p_up_blocks_0_resnets_1_time_emb_proj_weight : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_1_time_emb_proj_weight]\n",
      "    %p_up_blocks_0_resnets_1_time_emb_proj_bias : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_1_time_emb_proj_bias]\n",
      "    %p_up_blocks_0_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_1_norm2_weight]\n",
      "    %p_up_blocks_0_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_1_norm2_bias]\n",
      "    %p_up_blocks_0_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_1_conv2_weight]\n",
      "    %p_up_blocks_0_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_1_conv2_bias]\n",
      "    %p_up_blocks_0_resnets_1_conv_shortcut_weight : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_1_conv_shortcut_weight]\n",
      "    %p_up_blocks_0_resnets_1_conv_shortcut_bias : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_1_conv_shortcut_bias]\n",
      "    %p_up_blocks_0_resnets_2_norm1_weight : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_2_norm1_weight]\n",
      "    %p_up_blocks_0_resnets_2_norm1_bias : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_2_norm1_bias]\n",
      "    %p_up_blocks_0_resnets_2_conv1_weight : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_2_conv1_weight]\n",
      "    %p_up_blocks_0_resnets_2_conv1_bias : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_2_conv1_bias]\n",
      "    %p_up_blocks_0_resnets_2_time_emb_proj_weight : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_2_time_emb_proj_weight]\n",
      "    %p_up_blocks_0_resnets_2_time_emb_proj_bias : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_2_time_emb_proj_bias]\n",
      "    %p_up_blocks_0_resnets_2_norm2_weight : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_2_norm2_weight]\n",
      "    %p_up_blocks_0_resnets_2_norm2_bias : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_2_norm2_bias]\n",
      "    %p_up_blocks_0_resnets_2_conv2_weight : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_2_conv2_weight]\n",
      "    %p_up_blocks_0_resnets_2_conv2_bias : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_2_conv2_bias]\n",
      "    %p_up_blocks_0_resnets_2_conv_shortcut_weight : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_2_conv_shortcut_weight]\n",
      "    %p_up_blocks_0_resnets_2_conv_shortcut_bias : [num_users=1] = placeholder[target=p_up_blocks_0_resnets_2_conv_shortcut_bias]\n",
      "    %p_up_blocks_0_upsamplers_0_conv_weight : [num_users=1] = placeholder[target=p_up_blocks_0_upsamplers_0_conv_weight]\n",
      "    %p_up_blocks_0_upsamplers_0_conv_bias : [num_users=1] = placeholder[target=p_up_blocks_0_upsamplers_0_conv_bias]\n",
      "    %p_up_blocks_1_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_0_norm1_weight]\n",
      "    %p_up_blocks_1_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_0_norm1_bias]\n",
      "    %p_up_blocks_1_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_0_conv1_weight]\n",
      "    %p_up_blocks_1_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_0_conv1_bias]\n",
      "    %p_up_blocks_1_resnets_0_time_emb_proj_weight : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_0_time_emb_proj_weight]\n",
      "    %p_up_blocks_1_resnets_0_time_emb_proj_bias : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_0_time_emb_proj_bias]\n",
      "    %p_up_blocks_1_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_0_norm2_weight]\n",
      "    %p_up_blocks_1_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_0_norm2_bias]\n",
      "    %p_up_blocks_1_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_0_conv2_weight]\n",
      "    %p_up_blocks_1_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_0_conv2_bias]\n",
      "    %p_up_blocks_1_resnets_0_conv_shortcut_weight : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_0_conv_shortcut_weight]\n",
      "    %p_up_blocks_1_resnets_0_conv_shortcut_bias : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_0_conv_shortcut_bias]\n",
      "    %p_up_blocks_1_attentions_0_norm_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_norm_weight]\n",
      "    %p_up_blocks_1_attentions_0_norm_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_norm_bias]\n",
      "    %p_up_blocks_1_attentions_0_proj_in_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_proj_in_weight]\n",
      "    %p_up_blocks_1_attentions_0_proj_in_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_proj_in_bias]\n",
      "    %p_up_blocks_1_attentions_0_transformer_blocks_0_norm1_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_transformer_blocks_0_norm1_weight]\n",
      "    %p_up_blocks_1_attentions_0_transformer_blocks_0_norm1_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_transformer_blocks_0_norm1_bias]\n",
      "    %p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight]\n",
      "    %p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight]\n",
      "    %p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight]\n",
      "    %p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight]\n",
      "    %p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias]\n",
      "    %p_up_blocks_1_attentions_0_transformer_blocks_0_norm2_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_transformer_blocks_0_norm2_weight]\n",
      "    %p_up_blocks_1_attentions_0_transformer_blocks_0_norm2_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_transformer_blocks_0_norm2_bias]\n",
      "    %p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight]\n",
      "    %p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight]\n",
      "    %p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight]\n",
      "    %p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight]\n",
      "    %p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias]\n",
      "    %p_up_blocks_1_attentions_0_transformer_blocks_0_norm3_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_transformer_blocks_0_norm3_weight]\n",
      "    %p_up_blocks_1_attentions_0_transformer_blocks_0_norm3_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_transformer_blocks_0_norm3_bias]\n",
      "    %p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight]\n",
      "    %p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias]\n",
      "    %p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight]\n",
      "    %p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias]\n",
      "    %p_up_blocks_1_attentions_0_proj_out_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_proj_out_weight]\n",
      "    %p_up_blocks_1_attentions_0_proj_out_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_0_proj_out_bias]\n",
      "    %p_up_blocks_1_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_1_norm1_weight]\n",
      "    %p_up_blocks_1_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_1_norm1_bias]\n",
      "    %p_up_blocks_1_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_1_conv1_weight]\n",
      "    %p_up_blocks_1_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_1_conv1_bias]\n",
      "    %p_up_blocks_1_resnets_1_time_emb_proj_weight : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_1_time_emb_proj_weight]\n",
      "    %p_up_blocks_1_resnets_1_time_emb_proj_bias : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_1_time_emb_proj_bias]\n",
      "    %p_up_blocks_1_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_1_norm2_weight]\n",
      "    %p_up_blocks_1_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_1_norm2_bias]\n",
      "    %p_up_blocks_1_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_1_conv2_weight]\n",
      "    %p_up_blocks_1_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_1_conv2_bias]\n",
      "    %p_up_blocks_1_resnets_1_conv_shortcut_weight : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_1_conv_shortcut_weight]\n",
      "    %p_up_blocks_1_resnets_1_conv_shortcut_bias : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_1_conv_shortcut_bias]\n",
      "    %p_up_blocks_1_attentions_1_norm_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_norm_weight]\n",
      "    %p_up_blocks_1_attentions_1_norm_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_norm_bias]\n",
      "    %p_up_blocks_1_attentions_1_proj_in_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_proj_in_weight]\n",
      "    %p_up_blocks_1_attentions_1_proj_in_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_proj_in_bias]\n",
      "    %p_up_blocks_1_attentions_1_transformer_blocks_0_norm1_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_transformer_blocks_0_norm1_weight]\n",
      "    %p_up_blocks_1_attentions_1_transformer_blocks_0_norm1_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_transformer_blocks_0_norm1_bias]\n",
      "    %p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight]\n",
      "    %p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight]\n",
      "    %p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight]\n",
      "    %p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight]\n",
      "    %p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias]\n",
      "    %p_up_blocks_1_attentions_1_transformer_blocks_0_norm2_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_transformer_blocks_0_norm2_weight]\n",
      "    %p_up_blocks_1_attentions_1_transformer_blocks_0_norm2_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_transformer_blocks_0_norm2_bias]\n",
      "    %p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight]\n",
      "    %p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight]\n",
      "    %p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight]\n",
      "    %p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight]\n",
      "    %p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias]\n",
      "    %p_up_blocks_1_attentions_1_transformer_blocks_0_norm3_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_transformer_blocks_0_norm3_weight]\n",
      "    %p_up_blocks_1_attentions_1_transformer_blocks_0_norm3_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_transformer_blocks_0_norm3_bias]\n",
      "    %p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight]\n",
      "    %p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias]\n",
      "    %p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight]\n",
      "    %p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias]\n",
      "    %p_up_blocks_1_attentions_1_proj_out_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_proj_out_weight]\n",
      "    %p_up_blocks_1_attentions_1_proj_out_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_1_proj_out_bias]\n",
      "    %p_up_blocks_1_resnets_2_norm1_weight : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_2_norm1_weight]\n",
      "    %p_up_blocks_1_resnets_2_norm1_bias : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_2_norm1_bias]\n",
      "    %p_up_blocks_1_resnets_2_conv1_weight : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_2_conv1_weight]\n",
      "    %p_up_blocks_1_resnets_2_conv1_bias : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_2_conv1_bias]\n",
      "    %p_up_blocks_1_resnets_2_time_emb_proj_weight : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_2_time_emb_proj_weight]\n",
      "    %p_up_blocks_1_resnets_2_time_emb_proj_bias : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_2_time_emb_proj_bias]\n",
      "    %p_up_blocks_1_resnets_2_norm2_weight : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_2_norm2_weight]\n",
      "    %p_up_blocks_1_resnets_2_norm2_bias : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_2_norm2_bias]\n",
      "    %p_up_blocks_1_resnets_2_conv2_weight : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_2_conv2_weight]\n",
      "    %p_up_blocks_1_resnets_2_conv2_bias : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_2_conv2_bias]\n",
      "    %p_up_blocks_1_resnets_2_conv_shortcut_weight : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_2_conv_shortcut_weight]\n",
      "    %p_up_blocks_1_resnets_2_conv_shortcut_bias : [num_users=1] = placeholder[target=p_up_blocks_1_resnets_2_conv_shortcut_bias]\n",
      "    %p_up_blocks_1_attentions_2_norm_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_norm_weight]\n",
      "    %p_up_blocks_1_attentions_2_norm_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_norm_bias]\n",
      "    %p_up_blocks_1_attentions_2_proj_in_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_proj_in_weight]\n",
      "    %p_up_blocks_1_attentions_2_proj_in_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_proj_in_bias]\n",
      "    %p_up_blocks_1_attentions_2_transformer_blocks_0_norm1_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_transformer_blocks_0_norm1_weight]\n",
      "    %p_up_blocks_1_attentions_2_transformer_blocks_0_norm1_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_transformer_blocks_0_norm1_bias]\n",
      "    %p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q_weight]\n",
      "    %p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k_weight]\n",
      "    %p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v_weight]\n",
      "    %p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_weight]\n",
      "    %p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_bias]\n",
      "    %p_up_blocks_1_attentions_2_transformer_blocks_0_norm2_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_transformer_blocks_0_norm2_weight]\n",
      "    %p_up_blocks_1_attentions_2_transformer_blocks_0_norm2_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_transformer_blocks_0_norm2_bias]\n",
      "    %p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q_weight]\n",
      "    %p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k_weight]\n",
      "    %p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v_weight]\n",
      "    %p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_weight]\n",
      "    %p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_bias]\n",
      "    %p_up_blocks_1_attentions_2_transformer_blocks_0_norm3_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_transformer_blocks_0_norm3_weight]\n",
      "    %p_up_blocks_1_attentions_2_transformer_blocks_0_norm3_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_transformer_blocks_0_norm3_bias]\n",
      "    %p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_weight]\n",
      "    %p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_bias]\n",
      "    %p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_weight]\n",
      "    %p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_bias]\n",
      "    %p_up_blocks_1_attentions_2_proj_out_weight : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_proj_out_weight]\n",
      "    %p_up_blocks_1_attentions_2_proj_out_bias : [num_users=1] = placeholder[target=p_up_blocks_1_attentions_2_proj_out_bias]\n",
      "    %p_up_blocks_1_upsamplers_0_conv_weight : [num_users=1] = placeholder[target=p_up_blocks_1_upsamplers_0_conv_weight]\n",
      "    %p_up_blocks_1_upsamplers_0_conv_bias : [num_users=1] = placeholder[target=p_up_blocks_1_upsamplers_0_conv_bias]\n",
      "    %p_up_blocks_2_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_0_norm1_weight]\n",
      "    %p_up_blocks_2_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_0_norm1_bias]\n",
      "    %p_up_blocks_2_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_0_conv1_weight]\n",
      "    %p_up_blocks_2_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_0_conv1_bias]\n",
      "    %p_up_blocks_2_resnets_0_time_emb_proj_weight : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_0_time_emb_proj_weight]\n",
      "    %p_up_blocks_2_resnets_0_time_emb_proj_bias : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_0_time_emb_proj_bias]\n",
      "    %p_up_blocks_2_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_0_norm2_weight]\n",
      "    %p_up_blocks_2_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_0_norm2_bias]\n",
      "    %p_up_blocks_2_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_0_conv2_weight]\n",
      "    %p_up_blocks_2_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_0_conv2_bias]\n",
      "    %p_up_blocks_2_resnets_0_conv_shortcut_weight : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_0_conv_shortcut_weight]\n",
      "    %p_up_blocks_2_resnets_0_conv_shortcut_bias : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_0_conv_shortcut_bias]\n",
      "    %p_up_blocks_2_attentions_0_norm_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_norm_weight]\n",
      "    %p_up_blocks_2_attentions_0_norm_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_norm_bias]\n",
      "    %p_up_blocks_2_attentions_0_proj_in_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_proj_in_weight]\n",
      "    %p_up_blocks_2_attentions_0_proj_in_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_proj_in_bias]\n",
      "    %p_up_blocks_2_attentions_0_transformer_blocks_0_norm1_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_transformer_blocks_0_norm1_weight]\n",
      "    %p_up_blocks_2_attentions_0_transformer_blocks_0_norm1_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_transformer_blocks_0_norm1_bias]\n",
      "    %p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight]\n",
      "    %p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight]\n",
      "    %p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight]\n",
      "    %p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight]\n",
      "    %p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias]\n",
      "    %p_up_blocks_2_attentions_0_transformer_blocks_0_norm2_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_transformer_blocks_0_norm2_weight]\n",
      "    %p_up_blocks_2_attentions_0_transformer_blocks_0_norm2_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_transformer_blocks_0_norm2_bias]\n",
      "    %p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight]\n",
      "    %p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight]\n",
      "    %p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight]\n",
      "    %p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight]\n",
      "    %p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias]\n",
      "    %p_up_blocks_2_attentions_0_transformer_blocks_0_norm3_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_transformer_blocks_0_norm3_weight]\n",
      "    %p_up_blocks_2_attentions_0_transformer_blocks_0_norm3_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_transformer_blocks_0_norm3_bias]\n",
      "    %p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight]\n",
      "    %p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias]\n",
      "    %p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight]\n",
      "    %p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias]\n",
      "    %p_up_blocks_2_attentions_0_proj_out_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_proj_out_weight]\n",
      "    %p_up_blocks_2_attentions_0_proj_out_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_0_proj_out_bias]\n",
      "    %p_up_blocks_2_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_1_norm1_weight]\n",
      "    %p_up_blocks_2_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_1_norm1_bias]\n",
      "    %p_up_blocks_2_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_1_conv1_weight]\n",
      "    %p_up_blocks_2_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_1_conv1_bias]\n",
      "    %p_up_blocks_2_resnets_1_time_emb_proj_weight : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_1_time_emb_proj_weight]\n",
      "    %p_up_blocks_2_resnets_1_time_emb_proj_bias : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_1_time_emb_proj_bias]\n",
      "    %p_up_blocks_2_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_1_norm2_weight]\n",
      "    %p_up_blocks_2_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_1_norm2_bias]\n",
      "    %p_up_blocks_2_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_1_conv2_weight]\n",
      "    %p_up_blocks_2_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_1_conv2_bias]\n",
      "    %p_up_blocks_2_resnets_1_conv_shortcut_weight : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_1_conv_shortcut_weight]\n",
      "    %p_up_blocks_2_resnets_1_conv_shortcut_bias : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_1_conv_shortcut_bias]\n",
      "    %p_up_blocks_2_attentions_1_norm_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_norm_weight]\n",
      "    %p_up_blocks_2_attentions_1_norm_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_norm_bias]\n",
      "    %p_up_blocks_2_attentions_1_proj_in_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_proj_in_weight]\n",
      "    %p_up_blocks_2_attentions_1_proj_in_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_proj_in_bias]\n",
      "    %p_up_blocks_2_attentions_1_transformer_blocks_0_norm1_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_transformer_blocks_0_norm1_weight]\n",
      "    %p_up_blocks_2_attentions_1_transformer_blocks_0_norm1_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_transformer_blocks_0_norm1_bias]\n",
      "    %p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight]\n",
      "    %p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight]\n",
      "    %p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight]\n",
      "    %p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight]\n",
      "    %p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias]\n",
      "    %p_up_blocks_2_attentions_1_transformer_blocks_0_norm2_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_transformer_blocks_0_norm2_weight]\n",
      "    %p_up_blocks_2_attentions_1_transformer_blocks_0_norm2_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_transformer_blocks_0_norm2_bias]\n",
      "    %p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight]\n",
      "    %p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight]\n",
      "    %p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight]\n",
      "    %p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight]\n",
      "    %p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias]\n",
      "    %p_up_blocks_2_attentions_1_transformer_blocks_0_norm3_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_transformer_blocks_0_norm3_weight]\n",
      "    %p_up_blocks_2_attentions_1_transformer_blocks_0_norm3_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_transformer_blocks_0_norm3_bias]\n",
      "    %p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight]\n",
      "    %p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias]\n",
      "    %p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight]\n",
      "    %p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias]\n",
      "    %p_up_blocks_2_attentions_1_proj_out_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_proj_out_weight]\n",
      "    %p_up_blocks_2_attentions_1_proj_out_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_1_proj_out_bias]\n",
      "    %p_up_blocks_2_resnets_2_norm1_weight : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_2_norm1_weight]\n",
      "    %p_up_blocks_2_resnets_2_norm1_bias : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_2_norm1_bias]\n",
      "    %p_up_blocks_2_resnets_2_conv1_weight : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_2_conv1_weight]\n",
      "    %p_up_blocks_2_resnets_2_conv1_bias : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_2_conv1_bias]\n",
      "    %p_up_blocks_2_resnets_2_time_emb_proj_weight : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_2_time_emb_proj_weight]\n",
      "    %p_up_blocks_2_resnets_2_time_emb_proj_bias : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_2_time_emb_proj_bias]\n",
      "    %p_up_blocks_2_resnets_2_norm2_weight : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_2_norm2_weight]\n",
      "    %p_up_blocks_2_resnets_2_norm2_bias : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_2_norm2_bias]\n",
      "    %p_up_blocks_2_resnets_2_conv2_weight : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_2_conv2_weight]\n",
      "    %p_up_blocks_2_resnets_2_conv2_bias : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_2_conv2_bias]\n",
      "    %p_up_blocks_2_resnets_2_conv_shortcut_weight : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_2_conv_shortcut_weight]\n",
      "    %p_up_blocks_2_resnets_2_conv_shortcut_bias : [num_users=1] = placeholder[target=p_up_blocks_2_resnets_2_conv_shortcut_bias]\n",
      "    %p_up_blocks_2_attentions_2_norm_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_norm_weight]\n",
      "    %p_up_blocks_2_attentions_2_norm_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_norm_bias]\n",
      "    %p_up_blocks_2_attentions_2_proj_in_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_proj_in_weight]\n",
      "    %p_up_blocks_2_attentions_2_proj_in_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_proj_in_bias]\n",
      "    %p_up_blocks_2_attentions_2_transformer_blocks_0_norm1_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_transformer_blocks_0_norm1_weight]\n",
      "    %p_up_blocks_2_attentions_2_transformer_blocks_0_norm1_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_transformer_blocks_0_norm1_bias]\n",
      "    %p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q_weight]\n",
      "    %p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k_weight]\n",
      "    %p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v_weight]\n",
      "    %p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_weight]\n",
      "    %p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_bias]\n",
      "    %p_up_blocks_2_attentions_2_transformer_blocks_0_norm2_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_transformer_blocks_0_norm2_weight]\n",
      "    %p_up_blocks_2_attentions_2_transformer_blocks_0_norm2_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_transformer_blocks_0_norm2_bias]\n",
      "    %p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q_weight]\n",
      "    %p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k_weight]\n",
      "    %p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v_weight]\n",
      "    %p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_weight]\n",
      "    %p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_bias]\n",
      "    %p_up_blocks_2_attentions_2_transformer_blocks_0_norm3_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_transformer_blocks_0_norm3_weight]\n",
      "    %p_up_blocks_2_attentions_2_transformer_blocks_0_norm3_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_transformer_blocks_0_norm3_bias]\n",
      "    %p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_weight]\n",
      "    %p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_bias]\n",
      "    %p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_weight]\n",
      "    %p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_bias]\n",
      "    %p_up_blocks_2_attentions_2_proj_out_weight : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_proj_out_weight]\n",
      "    %p_up_blocks_2_attentions_2_proj_out_bias : [num_users=1] = placeholder[target=p_up_blocks_2_attentions_2_proj_out_bias]\n",
      "    %p_up_blocks_2_upsamplers_0_conv_weight : [num_users=1] = placeholder[target=p_up_blocks_2_upsamplers_0_conv_weight]\n",
      "    %p_up_blocks_2_upsamplers_0_conv_bias : [num_users=1] = placeholder[target=p_up_blocks_2_upsamplers_0_conv_bias]\n",
      "    %p_up_blocks_3_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_0_norm1_weight]\n",
      "    %p_up_blocks_3_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_0_norm1_bias]\n",
      "    %p_up_blocks_3_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_0_conv1_weight]\n",
      "    %p_up_blocks_3_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_0_conv1_bias]\n",
      "    %p_up_blocks_3_resnets_0_time_emb_proj_weight : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_0_time_emb_proj_weight]\n",
      "    %p_up_blocks_3_resnets_0_time_emb_proj_bias : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_0_time_emb_proj_bias]\n",
      "    %p_up_blocks_3_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_0_norm2_weight]\n",
      "    %p_up_blocks_3_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_0_norm2_bias]\n",
      "    %p_up_blocks_3_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_0_conv2_weight]\n",
      "    %p_up_blocks_3_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_0_conv2_bias]\n",
      "    %p_up_blocks_3_resnets_0_conv_shortcut_weight : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_0_conv_shortcut_weight]\n",
      "    %p_up_blocks_3_resnets_0_conv_shortcut_bias : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_0_conv_shortcut_bias]\n",
      "    %p_up_blocks_3_attentions_0_norm_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_norm_weight]\n",
      "    %p_up_blocks_3_attentions_0_norm_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_norm_bias]\n",
      "    %p_up_blocks_3_attentions_0_proj_in_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_proj_in_weight]\n",
      "    %p_up_blocks_3_attentions_0_proj_in_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_proj_in_bias]\n",
      "    %p_up_blocks_3_attentions_0_transformer_blocks_0_norm1_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_transformer_blocks_0_norm1_weight]\n",
      "    %p_up_blocks_3_attentions_0_transformer_blocks_0_norm1_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_transformer_blocks_0_norm1_bias]\n",
      "    %p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q_weight]\n",
      "    %p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k_weight]\n",
      "    %p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v_weight]\n",
      "    %p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_weight]\n",
      "    %p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_bias]\n",
      "    %p_up_blocks_3_attentions_0_transformer_blocks_0_norm2_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_transformer_blocks_0_norm2_weight]\n",
      "    %p_up_blocks_3_attentions_0_transformer_blocks_0_norm2_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_transformer_blocks_0_norm2_bias]\n",
      "    %p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q_weight]\n",
      "    %p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k_weight]\n",
      "    %p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v_weight]\n",
      "    %p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_weight]\n",
      "    %p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_bias]\n",
      "    %p_up_blocks_3_attentions_0_transformer_blocks_0_norm3_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_transformer_blocks_0_norm3_weight]\n",
      "    %p_up_blocks_3_attentions_0_transformer_blocks_0_norm3_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_transformer_blocks_0_norm3_bias]\n",
      "    %p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_weight]\n",
      "    %p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_bias]\n",
      "    %p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_weight]\n",
      "    %p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_bias]\n",
      "    %p_up_blocks_3_attentions_0_proj_out_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_proj_out_weight]\n",
      "    %p_up_blocks_3_attentions_0_proj_out_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_0_proj_out_bias]\n",
      "    %p_up_blocks_3_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_1_norm1_weight]\n",
      "    %p_up_blocks_3_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_1_norm1_bias]\n",
      "    %p_up_blocks_3_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_1_conv1_weight]\n",
      "    %p_up_blocks_3_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_1_conv1_bias]\n",
      "    %p_up_blocks_3_resnets_1_time_emb_proj_weight : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_1_time_emb_proj_weight]\n",
      "    %p_up_blocks_3_resnets_1_time_emb_proj_bias : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_1_time_emb_proj_bias]\n",
      "    %p_up_blocks_3_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_1_norm2_weight]\n",
      "    %p_up_blocks_3_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_1_norm2_bias]\n",
      "    %p_up_blocks_3_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_1_conv2_weight]\n",
      "    %p_up_blocks_3_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_1_conv2_bias]\n",
      "    %p_up_blocks_3_resnets_1_conv_shortcut_weight : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_1_conv_shortcut_weight]\n",
      "    %p_up_blocks_3_resnets_1_conv_shortcut_bias : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_1_conv_shortcut_bias]\n",
      "    %p_up_blocks_3_attentions_1_norm_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_norm_weight]\n",
      "    %p_up_blocks_3_attentions_1_norm_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_norm_bias]\n",
      "    %p_up_blocks_3_attentions_1_proj_in_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_proj_in_weight]\n",
      "    %p_up_blocks_3_attentions_1_proj_in_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_proj_in_bias]\n",
      "    %p_up_blocks_3_attentions_1_transformer_blocks_0_norm1_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_transformer_blocks_0_norm1_weight]\n",
      "    %p_up_blocks_3_attentions_1_transformer_blocks_0_norm1_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_transformer_blocks_0_norm1_bias]\n",
      "    %p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q_weight]\n",
      "    %p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k_weight]\n",
      "    %p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v_weight]\n",
      "    %p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_weight]\n",
      "    %p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_bias]\n",
      "    %p_up_blocks_3_attentions_1_transformer_blocks_0_norm2_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_transformer_blocks_0_norm2_weight]\n",
      "    %p_up_blocks_3_attentions_1_transformer_blocks_0_norm2_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_transformer_blocks_0_norm2_bias]\n",
      "    %p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q_weight]\n",
      "    %p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k_weight]\n",
      "    %p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v_weight]\n",
      "    %p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_weight]\n",
      "    %p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_bias]\n",
      "    %p_up_blocks_3_attentions_1_transformer_blocks_0_norm3_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_transformer_blocks_0_norm3_weight]\n",
      "    %p_up_blocks_3_attentions_1_transformer_blocks_0_norm3_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_transformer_blocks_0_norm3_bias]\n",
      "    %p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_weight]\n",
      "    %p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_bias]\n",
      "    %p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_weight]\n",
      "    %p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_bias]\n",
      "    %p_up_blocks_3_attentions_1_proj_out_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_proj_out_weight]\n",
      "    %p_up_blocks_3_attentions_1_proj_out_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_1_proj_out_bias]\n",
      "    %p_up_blocks_3_resnets_2_norm1_weight : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_2_norm1_weight]\n",
      "    %p_up_blocks_3_resnets_2_norm1_bias : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_2_norm1_bias]\n",
      "    %p_up_blocks_3_resnets_2_conv1_weight : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_2_conv1_weight]\n",
      "    %p_up_blocks_3_resnets_2_conv1_bias : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_2_conv1_bias]\n",
      "    %p_up_blocks_3_resnets_2_time_emb_proj_weight : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_2_time_emb_proj_weight]\n",
      "    %p_up_blocks_3_resnets_2_time_emb_proj_bias : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_2_time_emb_proj_bias]\n",
      "    %p_up_blocks_3_resnets_2_norm2_weight : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_2_norm2_weight]\n",
      "    %p_up_blocks_3_resnets_2_norm2_bias : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_2_norm2_bias]\n",
      "    %p_up_blocks_3_resnets_2_conv2_weight : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_2_conv2_weight]\n",
      "    %p_up_blocks_3_resnets_2_conv2_bias : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_2_conv2_bias]\n",
      "    %p_up_blocks_3_resnets_2_conv_shortcut_weight : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_2_conv_shortcut_weight]\n",
      "    %p_up_blocks_3_resnets_2_conv_shortcut_bias : [num_users=1] = placeholder[target=p_up_blocks_3_resnets_2_conv_shortcut_bias]\n",
      "    %p_up_blocks_3_attentions_2_norm_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_norm_weight]\n",
      "    %p_up_blocks_3_attentions_2_norm_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_norm_bias]\n",
      "    %p_up_blocks_3_attentions_2_proj_in_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_proj_in_weight]\n",
      "    %p_up_blocks_3_attentions_2_proj_in_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_proj_in_bias]\n",
      "    %p_up_blocks_3_attentions_2_transformer_blocks_0_norm1_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_transformer_blocks_0_norm1_weight]\n",
      "    %p_up_blocks_3_attentions_2_transformer_blocks_0_norm1_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_transformer_blocks_0_norm1_bias]\n",
      "    %p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q_weight]\n",
      "    %p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k_weight]\n",
      "    %p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v_weight]\n",
      "    %p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_weight]\n",
      "    %p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_bias]\n",
      "    %p_up_blocks_3_attentions_2_transformer_blocks_0_norm2_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_transformer_blocks_0_norm2_weight]\n",
      "    %p_up_blocks_3_attentions_2_transformer_blocks_0_norm2_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_transformer_blocks_0_norm2_bias]\n",
      "    %p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q_weight]\n",
      "    %p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k_weight]\n",
      "    %p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v_weight]\n",
      "    %p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_weight]\n",
      "    %p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_bias]\n",
      "    %p_up_blocks_3_attentions_2_transformer_blocks_0_norm3_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_transformer_blocks_0_norm3_weight]\n",
      "    %p_up_blocks_3_attentions_2_transformer_blocks_0_norm3_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_transformer_blocks_0_norm3_bias]\n",
      "    %p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_weight]\n",
      "    %p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_bias]\n",
      "    %p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_weight]\n",
      "    %p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_bias]\n",
      "    %p_up_blocks_3_attentions_2_proj_out_weight : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_proj_out_weight]\n",
      "    %p_up_blocks_3_attentions_2_proj_out_bias : [num_users=1] = placeholder[target=p_up_blocks_3_attentions_2_proj_out_bias]\n",
      "    %p_conv_norm_out_weight : [num_users=1] = placeholder[target=p_conv_norm_out_weight]\n",
      "    %p_conv_norm_out_bias : [num_users=1] = placeholder[target=p_conv_norm_out_bias]\n",
      "    %p_conv_out_weight : [num_users=1] = placeholder[target=p_conv_out_weight]\n",
      "    %p_conv_out_bias : [num_users=1] = placeholder[target=p_conv_out_bias]\n",
      "    %sample : [num_users=1] = placeholder[target=sample]\n",
      "    %timestep : [num_users=1] = placeholder[target=timestep]\n",
      "    %encoder_hidden_states : [num_users=32] = placeholder[target=encoder_hidden_states]\n",
      "    %expand : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%timestep, [1]), kwargs = {})\n",
      "    %arange : [num_users=1] = call_function[target=torch.ops.aten.arange.start](args = (0, 160), kwargs = {dtype: torch.float32, device: cpu, pin_memory: False})\n",
      "    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arange, -9.210340371976184), kwargs = {})\n",
      "    %div : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%mul, 160), kwargs = {})\n",
      "    %exp : [num_users=1] = call_function[target=torch.ops.aten.exp.default](args = (%div,), kwargs = {})\n",
      "    %slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%expand, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_1, 1), kwargs = {})\n",
      "    %_to_copy : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%unsqueeze,), kwargs = {dtype: torch.float32})\n",
      "    %unsqueeze_1 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%exp, 0), kwargs = {})\n",
      "    %slice_2 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%unsqueeze_1, 1, 0, 9223372036854775807), kwargs = {})\n",
      "    %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_to_copy, %slice_2), kwargs = {})\n",
      "    %mul_2 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1, 1), kwargs = {})\n",
      "    %sin : [num_users=1] = call_function[target=torch.ops.aten.sin.default](args = (%mul_2,), kwargs = {})\n",
      "    %cos : [num_users=1] = call_function[target=torch.ops.aten.cos.default](args = (%mul_2,), kwargs = {})\n",
      "    %cat : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%sin, %cos], -1), kwargs = {})\n",
      "    %slice_3 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%cat, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %slice_4 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_3, 1, 160, 9223372036854775807), kwargs = {})\n",
      "    %slice_5 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%cat, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %slice_6 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_5, 1, 0, 160), kwargs = {})\n",
      "    %cat_1 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%slice_4, %slice_6], -1), kwargs = {})\n",
      "    %_to_copy_1 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%cat_1,), kwargs = {dtype: torch.float32})\n",
      "    %linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_1, %p_time_embedding_linear_1_weight, %p_time_embedding_linear_1_bias), kwargs = {})\n",
      "    %silu : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear,), kwargs = {})\n",
      "    %linear_1 : [num_users=22] = call_function[target=torch.ops.aten.linear.default](args = (%silu, %p_time_embedding_linear_2_weight, %p_time_embedding_linear_2_bias), kwargs = {})\n",
      "    %conv2d : [num_users=3] = call_function[target=torch.ops.aten.conv2d.default](args = (%sample, %p_conv_in_weight, %p_conv_in_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d, 32, %p_down_blocks_0_resnets_0_norm1_weight, %p_down_blocks_0_resnets_0_norm1_bias), kwargs = {})\n",
      "    %silu_1 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm,), kwargs = {})\n",
      "    %conv2d_1 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_1, %p_down_blocks_0_resnets_0_conv1_weight, %p_down_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %silu_2 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_1,), kwargs = {})\n",
      "    %linear_2 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%silu_2, %p_down_blocks_0_resnets_0_time_emb_proj_weight, %p_down_blocks_0_resnets_0_time_emb_proj_bias), kwargs = {})\n",
      "    %slice_7 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%linear_2, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %slice_8 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_7, 1, 0, 9223372036854775807), kwargs = {})\n",
      "    %unsqueeze_2 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_8, 2), kwargs = {})\n",
      "    %unsqueeze_3 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_2, 3), kwargs = {})\n",
      "    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_1, %unsqueeze_3), kwargs = {})\n",
      "    %group_norm_1 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add, 32, %p_down_blocks_0_resnets_0_norm2_weight, %p_down_blocks_0_resnets_0_norm2_bias), kwargs = {})\n",
      "    %silu_3 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_1,), kwargs = {})\n",
      "    %dropout : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_3, 0.0, False), kwargs = {})\n",
      "    %conv2d_2 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout, %p_down_blocks_0_resnets_0_conv2_weight, %p_down_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d, %conv2d_2), kwargs = {})\n",
      "    %div_1 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_1, 1.0), kwargs = {})\n",
      "    %group_norm_2 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_1, 32, %p_down_blocks_0_attentions_0_norm_weight, %p_down_blocks_0_attentions_0_norm_bias, 1e-06), kwargs = {})\n",
      "    %permute : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%group_norm_2, [0, 2, 3, 1]), kwargs = {})\n",
      "    %view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%permute, [1, 4096, 320]), kwargs = {})\n",
      "    %linear_3 : [num_users=2] = call_function[target=torch.ops.aten.linear.default](args = (%view, %p_down_blocks_0_attentions_0_proj_in_weight, %p_down_blocks_0_attentions_0_proj_in_bias), kwargs = {})\n",
      "    %layer_norm : [num_users=3] = call_function[target=torch.ops.aten.layer_norm.default](args = (%linear_3, [320], %p_down_blocks_0_attentions_0_transformer_blocks_0_norm1_weight, %p_down_blocks_0_attentions_0_transformer_blocks_0_norm1_bias), kwargs = {})\n",
      "    %linear_4 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm, %p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q_weight), kwargs = {})\n",
      "    %linear_5 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm, %p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k_weight), kwargs = {})\n",
      "    %linear_6 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm, %p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v_weight), kwargs = {})\n",
      "    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_4, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_1, 1, 2), kwargs = {})\n",
      "    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_5, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_1 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_2, 1, 2), kwargs = {})\n",
      "    %view_3 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_6, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_2 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_3, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose, %transpose_1, %transpose_2), kwargs = {})\n",
      "    %transpose_3 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention, 1, 2), kwargs = {})\n",
      "    %view_4 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_3, [1, -1, 320]), kwargs = {})\n",
      "    %_to_copy_2 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_4,), kwargs = {dtype: torch.float32})\n",
      "    %linear_7 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_2, %p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, %p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias), kwargs = {})\n",
      "    %dropout_1 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_7, 0.0, False), kwargs = {})\n",
      "    %div_2 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_1, 1.0), kwargs = {})\n",
      "    %add_2 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_2, %linear_3), kwargs = {})\n",
      "    %layer_norm_1 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_2, [320], %p_down_blocks_0_attentions_0_transformer_blocks_0_norm2_weight, %p_down_blocks_0_attentions_0_transformer_blocks_0_norm2_bias), kwargs = {})\n",
      "    %linear_8 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_1, %p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q_weight), kwargs = {})\n",
      "    %linear_9 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k_weight), kwargs = {})\n",
      "    %linear_10 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v_weight), kwargs = {})\n",
      "    %view_5 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_8, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_4 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_5, 1, 2), kwargs = {})\n",
      "    %view_6 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_9, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_5 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_6, 1, 2), kwargs = {})\n",
      "    %view_7 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_10, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_6 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_7, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_4, %transpose_5, %transpose_6), kwargs = {})\n",
      "    %transpose_7 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_1, 1, 2), kwargs = {})\n",
      "    %view_8 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_7, [1, -1, 320]), kwargs = {})\n",
      "    %_to_copy_3 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_8,), kwargs = {dtype: torch.float32})\n",
      "    %linear_11 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_3, %p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, %p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_bias), kwargs = {})\n",
      "    %dropout_2 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_11, 0.0, False), kwargs = {})\n",
      "    %div_3 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_2, 1.0), kwargs = {})\n",
      "    %add_3 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_3, %add_2), kwargs = {})\n",
      "    %layer_norm_2 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_3, [320], %p_down_blocks_0_attentions_0_transformer_blocks_0_norm3_weight, %p_down_blocks_0_attentions_0_transformer_blocks_0_norm3_bias), kwargs = {})\n",
      "    %linear_12 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_2, %p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, %p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_bias), kwargs = {})\n",
      "    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%linear_12, 1280, -1), kwargs = {})\n",
      "    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n",
      "    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n",
      "    %gelu : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%getitem_1,), kwargs = {})\n",
      "    %mul_3 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem, %gelu), kwargs = {})\n",
      "    %dropout_3 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%mul_3, 0.0, False), kwargs = {})\n",
      "    %linear_13 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%dropout_3, %p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_weight, %p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias), kwargs = {})\n",
      "    %add_4 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%linear_13, %add_3), kwargs = {})\n",
      "    %linear_14 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%add_4, %p_down_blocks_0_attentions_0_proj_out_weight, %p_down_blocks_0_attentions_0_proj_out_bias), kwargs = {})\n",
      "    %view_9 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_14, [1, 64, 64, 320]), kwargs = {})\n",
      "    %permute_1 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_9, [0, 3, 1, 2]), kwargs = {})\n",
      "    %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_1,), kwargs = {memory_format: torch.contiguous_format})\n",
      "    %add_5 : [num_users=3] = call_function[target=torch.ops.aten.add.Tensor](args = (%clone, %div_1), kwargs = {})\n",
      "    %group_norm_3 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_5, 32, %p_down_blocks_0_resnets_1_norm1_weight, %p_down_blocks_0_resnets_1_norm1_bias), kwargs = {})\n",
      "    %silu_4 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_3,), kwargs = {})\n",
      "    %conv2d_3 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_4, %p_down_blocks_0_resnets_1_conv1_weight, %p_down_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %silu_5 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_1,), kwargs = {})\n",
      "    %linear_15 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%silu_5, %p_down_blocks_0_resnets_1_time_emb_proj_weight, %p_down_blocks_0_resnets_1_time_emb_proj_bias), kwargs = {})\n",
      "    %slice_9 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%linear_15, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %slice_10 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_9, 1, 0, 9223372036854775807), kwargs = {})\n",
      "    %unsqueeze_4 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_10, 2), kwargs = {})\n",
      "    %unsqueeze_5 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_4, 3), kwargs = {})\n",
      "    %add_6 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_3, %unsqueeze_5), kwargs = {})\n",
      "    %group_norm_4 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_6, 32, %p_down_blocks_0_resnets_1_norm2_weight, %p_down_blocks_0_resnets_1_norm2_bias), kwargs = {})\n",
      "    %silu_6 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_4,), kwargs = {})\n",
      "    %dropout_4 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_6, 0.0, False), kwargs = {})\n",
      "    %conv2d_4 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_4, %p_down_blocks_0_resnets_1_conv2_weight, %p_down_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_7 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_5, %conv2d_4), kwargs = {})\n",
      "    %div_4 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_7, 1.0), kwargs = {})\n",
      "    %group_norm_5 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_4, 32, %p_down_blocks_0_attentions_1_norm_weight, %p_down_blocks_0_attentions_1_norm_bias, 1e-06), kwargs = {})\n",
      "    %permute_2 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%group_norm_5, [0, 2, 3, 1]), kwargs = {})\n",
      "    %view_10 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%permute_2, [1, 4096, 320]), kwargs = {})\n",
      "    %linear_16 : [num_users=2] = call_function[target=torch.ops.aten.linear.default](args = (%view_10, %p_down_blocks_0_attentions_1_proj_in_weight, %p_down_blocks_0_attentions_1_proj_in_bias), kwargs = {})\n",
      "    %layer_norm_3 : [num_users=3] = call_function[target=torch.ops.aten.layer_norm.default](args = (%linear_16, [320], %p_down_blocks_0_attentions_1_transformer_blocks_0_norm1_weight, %p_down_blocks_0_attentions_1_transformer_blocks_0_norm1_bias), kwargs = {})\n",
      "    %linear_17 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_3, %p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q_weight), kwargs = {})\n",
      "    %linear_18 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_3, %p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k_weight), kwargs = {})\n",
      "    %linear_19 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_3, %p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v_weight), kwargs = {})\n",
      "    %view_11 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_17, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_8 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_11, 1, 2), kwargs = {})\n",
      "    %view_12 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_18, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_9 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_12, 1, 2), kwargs = {})\n",
      "    %view_13 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_19, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_10 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_13, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_2 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_8, %transpose_9, %transpose_10), kwargs = {})\n",
      "    %transpose_11 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_2, 1, 2), kwargs = {})\n",
      "    %view_14 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_11, [1, -1, 320]), kwargs = {})\n",
      "    %_to_copy_4 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_14,), kwargs = {dtype: torch.float32})\n",
      "    %linear_20 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_4, %p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, %p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_bias), kwargs = {})\n",
      "    %dropout_5 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_20, 0.0, False), kwargs = {})\n",
      "    %div_5 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_5, 1.0), kwargs = {})\n",
      "    %add_8 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_5, %linear_16), kwargs = {})\n",
      "    %layer_norm_4 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_8, [320], %p_down_blocks_0_attentions_1_transformer_blocks_0_norm2_weight, %p_down_blocks_0_attentions_1_transformer_blocks_0_norm2_bias), kwargs = {})\n",
      "    %linear_21 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_4, %p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q_weight), kwargs = {})\n",
      "    %linear_22 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k_weight), kwargs = {})\n",
      "    %linear_23 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v_weight), kwargs = {})\n",
      "    %view_15 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_21, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_12 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_15, 1, 2), kwargs = {})\n",
      "    %view_16 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_22, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_13 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_16, 1, 2), kwargs = {})\n",
      "    %view_17 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_23, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_14 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_17, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_3 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_12, %transpose_13, %transpose_14), kwargs = {})\n",
      "    %transpose_15 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_3, 1, 2), kwargs = {})\n",
      "    %view_18 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_15, [1, -1, 320]), kwargs = {})\n",
      "    %_to_copy_5 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_18,), kwargs = {dtype: torch.float32})\n",
      "    %linear_24 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_5, %p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, %p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_bias), kwargs = {})\n",
      "    %dropout_6 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_24, 0.0, False), kwargs = {})\n",
      "    %div_6 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_6, 1.0), kwargs = {})\n",
      "    %add_9 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_6, %add_8), kwargs = {})\n",
      "    %layer_norm_5 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_9, [320], %p_down_blocks_0_attentions_1_transformer_blocks_0_norm3_weight, %p_down_blocks_0_attentions_1_transformer_blocks_0_norm3_bias), kwargs = {})\n",
      "    %linear_25 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_5, %p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, %p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_bias), kwargs = {})\n",
      "    %split_1 : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%linear_25, 1280, -1), kwargs = {})\n",
      "    %getitem_2 : [num_users=1] = call_function[target=operator.getitem](args = (%split_1, 0), kwargs = {})\n",
      "    %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%split_1, 1), kwargs = {})\n",
      "    %gelu_1 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%getitem_3,), kwargs = {})\n",
      "    %mul_4 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_2, %gelu_1), kwargs = {})\n",
      "    %dropout_7 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%mul_4, 0.0, False), kwargs = {})\n",
      "    %linear_26 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%dropout_7, %p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_weight, %p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_bias), kwargs = {})\n",
      "    %add_10 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%linear_26, %add_9), kwargs = {})\n",
      "    %linear_27 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%add_10, %p_down_blocks_0_attentions_1_proj_out_weight, %p_down_blocks_0_attentions_1_proj_out_bias), kwargs = {})\n",
      "    %view_19 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_27, [1, 64, 64, 320]), kwargs = {})\n",
      "    %permute_3 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_19, [0, 3, 1, 2]), kwargs = {})\n",
      "    %clone_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_3,), kwargs = {memory_format: torch.contiguous_format})\n",
      "    %add_11 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%clone_1, %div_4), kwargs = {})\n",
      "    %conv2d_5 : [num_users=3] = call_function[target=torch.ops.aten.conv2d.default](args = (%add_11, %p_down_blocks_0_downsamplers_0_conv_weight, %p_down_blocks_0_downsamplers_0_conv_bias, [2, 2], [1, 1]), kwargs = {})\n",
      "    %group_norm_6 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_5, 32, %p_down_blocks_1_resnets_0_norm1_weight, %p_down_blocks_1_resnets_0_norm1_bias), kwargs = {})\n",
      "    %silu_7 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_6,), kwargs = {})\n",
      "    %conv2d_6 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_7, %p_down_blocks_1_resnets_0_conv1_weight, %p_down_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %silu_8 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_1,), kwargs = {})\n",
      "    %linear_28 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%silu_8, %p_down_blocks_1_resnets_0_time_emb_proj_weight, %p_down_blocks_1_resnets_0_time_emb_proj_bias), kwargs = {})\n",
      "    %slice_11 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%linear_28, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %slice_12 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_11, 1, 0, 9223372036854775807), kwargs = {})\n",
      "    %unsqueeze_6 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_12, 2), kwargs = {})\n",
      "    %unsqueeze_7 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_6, 3), kwargs = {})\n",
      "    %add_12 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_6, %unsqueeze_7), kwargs = {})\n",
      "    %group_norm_7 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_12, 32, %p_down_blocks_1_resnets_0_norm2_weight, %p_down_blocks_1_resnets_0_norm2_bias), kwargs = {})\n",
      "    %silu_9 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_7,), kwargs = {})\n",
      "    %dropout_8 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_9, 0.0, False), kwargs = {})\n",
      "    %conv2d_7 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_8, %p_down_blocks_1_resnets_0_conv2_weight, %p_down_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_8 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%conv2d_5, %p_down_blocks_1_resnets_0_conv_shortcut_weight, %p_down_blocks_1_resnets_0_conv_shortcut_bias), kwargs = {})\n",
      "    %add_13 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_8, %conv2d_7), kwargs = {})\n",
      "    %div_7 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_13, 1.0), kwargs = {})\n",
      "    %group_norm_8 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_7, 32, %p_down_blocks_1_attentions_0_norm_weight, %p_down_blocks_1_attentions_0_norm_bias, 1e-06), kwargs = {})\n",
      "    %permute_4 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%group_norm_8, [0, 2, 3, 1]), kwargs = {})\n",
      "    %view_20 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%permute_4, [1, 1024, 640]), kwargs = {})\n",
      "    %linear_29 : [num_users=2] = call_function[target=torch.ops.aten.linear.default](args = (%view_20, %p_down_blocks_1_attentions_0_proj_in_weight, %p_down_blocks_1_attentions_0_proj_in_bias), kwargs = {})\n",
      "    %layer_norm_6 : [num_users=3] = call_function[target=torch.ops.aten.layer_norm.default](args = (%linear_29, [640], %p_down_blocks_1_attentions_0_transformer_blocks_0_norm1_weight, %p_down_blocks_1_attentions_0_transformer_blocks_0_norm1_bias), kwargs = {})\n",
      "    %linear_30 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_6, %p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight), kwargs = {})\n",
      "    %linear_31 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_6, %p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight), kwargs = {})\n",
      "    %linear_32 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_6, %p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight), kwargs = {})\n",
      "    %view_21 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_30, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_16 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_21, 1, 2), kwargs = {})\n",
      "    %view_22 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_31, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_17 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_22, 1, 2), kwargs = {})\n",
      "    %view_23 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_32, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_18 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_23, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_4 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_16, %transpose_17, %transpose_18), kwargs = {})\n",
      "    %transpose_19 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_4, 1, 2), kwargs = {})\n",
      "    %view_24 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_19, [1, -1, 640]), kwargs = {})\n",
      "    %_to_copy_6 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_24,), kwargs = {dtype: torch.float32})\n",
      "    %linear_33 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_6, %p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, %p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias), kwargs = {})\n",
      "    %dropout_9 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_33, 0.0, False), kwargs = {})\n",
      "    %div_8 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_9, 1.0), kwargs = {})\n",
      "    %add_14 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_8, %linear_29), kwargs = {})\n",
      "    %layer_norm_7 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_14, [640], %p_down_blocks_1_attentions_0_transformer_blocks_0_norm2_weight, %p_down_blocks_1_attentions_0_transformer_blocks_0_norm2_bias), kwargs = {})\n",
      "    %linear_34 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_7, %p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight), kwargs = {})\n",
      "    %linear_35 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight), kwargs = {})\n",
      "    %linear_36 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight), kwargs = {})\n",
      "    %view_25 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_34, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_20 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_25, 1, 2), kwargs = {})\n",
      "    %view_26 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_35, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_21 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_26, 1, 2), kwargs = {})\n",
      "    %view_27 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_36, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_22 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_27, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_5 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_20, %transpose_21, %transpose_22), kwargs = {})\n",
      "    %transpose_23 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_5, 1, 2), kwargs = {})\n",
      "    %view_28 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_23, [1, -1, 640]), kwargs = {})\n",
      "    %_to_copy_7 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_28,), kwargs = {dtype: torch.float32})\n",
      "    %linear_37 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_7, %p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, %p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias), kwargs = {})\n",
      "    %dropout_10 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_37, 0.0, False), kwargs = {})\n",
      "    %div_9 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_10, 1.0), kwargs = {})\n",
      "    %add_15 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_9, %add_14), kwargs = {})\n",
      "    %layer_norm_8 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_15, [640], %p_down_blocks_1_attentions_0_transformer_blocks_0_norm3_weight, %p_down_blocks_1_attentions_0_transformer_blocks_0_norm3_bias), kwargs = {})\n",
      "    %linear_38 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_8, %p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, %p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias), kwargs = {})\n",
      "    %split_2 : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%linear_38, 2560, -1), kwargs = {})\n",
      "    %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%split_2, 0), kwargs = {})\n",
      "    %getitem_5 : [num_users=1] = call_function[target=operator.getitem](args = (%split_2, 1), kwargs = {})\n",
      "    %gelu_2 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%getitem_5,), kwargs = {})\n",
      "    %mul_5 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_4, %gelu_2), kwargs = {})\n",
      "    %dropout_11 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%mul_5, 0.0, False), kwargs = {})\n",
      "    %linear_39 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%dropout_11, %p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight, %p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias), kwargs = {})\n",
      "    %add_16 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%linear_39, %add_15), kwargs = {})\n",
      "    %linear_40 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%add_16, %p_down_blocks_1_attentions_0_proj_out_weight, %p_down_blocks_1_attentions_0_proj_out_bias), kwargs = {})\n",
      "    %view_29 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_40, [1, 32, 32, 640]), kwargs = {})\n",
      "    %permute_5 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_29, [0, 3, 1, 2]), kwargs = {})\n",
      "    %clone_2 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_5,), kwargs = {memory_format: torch.contiguous_format})\n",
      "    %add_17 : [num_users=3] = call_function[target=torch.ops.aten.add.Tensor](args = (%clone_2, %div_7), kwargs = {})\n",
      "    %group_norm_9 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_17, 32, %p_down_blocks_1_resnets_1_norm1_weight, %p_down_blocks_1_resnets_1_norm1_bias), kwargs = {})\n",
      "    %silu_10 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_9,), kwargs = {})\n",
      "    %conv2d_9 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_10, %p_down_blocks_1_resnets_1_conv1_weight, %p_down_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %silu_11 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_1,), kwargs = {})\n",
      "    %linear_41 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%silu_11, %p_down_blocks_1_resnets_1_time_emb_proj_weight, %p_down_blocks_1_resnets_1_time_emb_proj_bias), kwargs = {})\n",
      "    %slice_13 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%linear_41, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %slice_14 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_13, 1, 0, 9223372036854775807), kwargs = {})\n",
      "    %unsqueeze_8 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_14, 2), kwargs = {})\n",
      "    %unsqueeze_9 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_8, 3), kwargs = {})\n",
      "    %add_18 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_9, %unsqueeze_9), kwargs = {})\n",
      "    %group_norm_10 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_18, 32, %p_down_blocks_1_resnets_1_norm2_weight, %p_down_blocks_1_resnets_1_norm2_bias), kwargs = {})\n",
      "    %silu_12 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_10,), kwargs = {})\n",
      "    %dropout_12 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_12, 0.0, False), kwargs = {})\n",
      "    %conv2d_10 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_12, %p_down_blocks_1_resnets_1_conv2_weight, %p_down_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_19 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_17, %conv2d_10), kwargs = {})\n",
      "    %div_10 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_19, 1.0), kwargs = {})\n",
      "    %group_norm_11 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_10, 32, %p_down_blocks_1_attentions_1_norm_weight, %p_down_blocks_1_attentions_1_norm_bias, 1e-06), kwargs = {})\n",
      "    %permute_6 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%group_norm_11, [0, 2, 3, 1]), kwargs = {})\n",
      "    %view_30 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%permute_6, [1, 1024, 640]), kwargs = {})\n",
      "    %linear_42 : [num_users=2] = call_function[target=torch.ops.aten.linear.default](args = (%view_30, %p_down_blocks_1_attentions_1_proj_in_weight, %p_down_blocks_1_attentions_1_proj_in_bias), kwargs = {})\n",
      "    %layer_norm_9 : [num_users=3] = call_function[target=torch.ops.aten.layer_norm.default](args = (%linear_42, [640], %p_down_blocks_1_attentions_1_transformer_blocks_0_norm1_weight, %p_down_blocks_1_attentions_1_transformer_blocks_0_norm1_bias), kwargs = {})\n",
      "    %linear_43 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_9, %p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight), kwargs = {})\n",
      "    %linear_44 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_9, %p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight), kwargs = {})\n",
      "    %linear_45 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_9, %p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight), kwargs = {})\n",
      "    %view_31 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_43, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_24 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_31, 1, 2), kwargs = {})\n",
      "    %view_32 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_44, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_25 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_32, 1, 2), kwargs = {})\n",
      "    %view_33 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_45, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_26 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_33, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_6 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_24, %transpose_25, %transpose_26), kwargs = {})\n",
      "    %transpose_27 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_6, 1, 2), kwargs = {})\n",
      "    %view_34 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_27, [1, -1, 640]), kwargs = {})\n",
      "    %_to_copy_8 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_34,), kwargs = {dtype: torch.float32})\n",
      "    %linear_46 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_8, %p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, %p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias), kwargs = {})\n",
      "    %dropout_13 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_46, 0.0, False), kwargs = {})\n",
      "    %div_11 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_13, 1.0), kwargs = {})\n",
      "    %add_20 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_11, %linear_42), kwargs = {})\n",
      "    %layer_norm_10 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_20, [640], %p_down_blocks_1_attentions_1_transformer_blocks_0_norm2_weight, %p_down_blocks_1_attentions_1_transformer_blocks_0_norm2_bias), kwargs = {})\n",
      "    %linear_47 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_10, %p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight), kwargs = {})\n",
      "    %linear_48 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight), kwargs = {})\n",
      "    %linear_49 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight), kwargs = {})\n",
      "    %view_35 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_47, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_28 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_35, 1, 2), kwargs = {})\n",
      "    %view_36 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_48, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_29 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_36, 1, 2), kwargs = {})\n",
      "    %view_37 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_49, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_30 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_37, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_7 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_28, %transpose_29, %transpose_30), kwargs = {})\n",
      "    %transpose_31 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_7, 1, 2), kwargs = {})\n",
      "    %view_38 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_31, [1, -1, 640]), kwargs = {})\n",
      "    %_to_copy_9 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_38,), kwargs = {dtype: torch.float32})\n",
      "    %linear_50 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_9, %p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, %p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias), kwargs = {})\n",
      "    %dropout_14 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_50, 0.0, False), kwargs = {})\n",
      "    %div_12 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_14, 1.0), kwargs = {})\n",
      "    %add_21 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_12, %add_20), kwargs = {})\n",
      "    %layer_norm_11 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_21, [640], %p_down_blocks_1_attentions_1_transformer_blocks_0_norm3_weight, %p_down_blocks_1_attentions_1_transformer_blocks_0_norm3_bias), kwargs = {})\n",
      "    %linear_51 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_11, %p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, %p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias), kwargs = {})\n",
      "    %split_3 : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%linear_51, 2560, -1), kwargs = {})\n",
      "    %getitem_6 : [num_users=1] = call_function[target=operator.getitem](args = (%split_3, 0), kwargs = {})\n",
      "    %getitem_7 : [num_users=1] = call_function[target=operator.getitem](args = (%split_3, 1), kwargs = {})\n",
      "    %gelu_3 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%getitem_7,), kwargs = {})\n",
      "    %mul_6 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_6, %gelu_3), kwargs = {})\n",
      "    %dropout_15 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%mul_6, 0.0, False), kwargs = {})\n",
      "    %linear_52 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%dropout_15, %p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight, %p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias), kwargs = {})\n",
      "    %add_22 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%linear_52, %add_21), kwargs = {})\n",
      "    %linear_53 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%add_22, %p_down_blocks_1_attentions_1_proj_out_weight, %p_down_blocks_1_attentions_1_proj_out_bias), kwargs = {})\n",
      "    %view_39 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_53, [1, 32, 32, 640]), kwargs = {})\n",
      "    %permute_7 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_39, [0, 3, 1, 2]), kwargs = {})\n",
      "    %clone_3 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_7,), kwargs = {memory_format: torch.contiguous_format})\n",
      "    %add_23 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%clone_3, %div_10), kwargs = {})\n",
      "    %conv2d_11 : [num_users=3] = call_function[target=torch.ops.aten.conv2d.default](args = (%add_23, %p_down_blocks_1_downsamplers_0_conv_weight, %p_down_blocks_1_downsamplers_0_conv_bias, [2, 2], [1, 1]), kwargs = {})\n",
      "    %group_norm_12 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_11, 32, %p_down_blocks_2_resnets_0_norm1_weight, %p_down_blocks_2_resnets_0_norm1_bias), kwargs = {})\n",
      "    %silu_13 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_12,), kwargs = {})\n",
      "    %conv2d_12 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_13, %p_down_blocks_2_resnets_0_conv1_weight, %p_down_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %silu_14 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_1,), kwargs = {})\n",
      "    %linear_54 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%silu_14, %p_down_blocks_2_resnets_0_time_emb_proj_weight, %p_down_blocks_2_resnets_0_time_emb_proj_bias), kwargs = {})\n",
      "    %slice_15 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%linear_54, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %slice_16 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_15, 1, 0, 9223372036854775807), kwargs = {})\n",
      "    %unsqueeze_10 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_16, 2), kwargs = {})\n",
      "    %unsqueeze_11 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_10, 3), kwargs = {})\n",
      "    %add_24 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_12, %unsqueeze_11), kwargs = {})\n",
      "    %group_norm_13 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_24, 32, %p_down_blocks_2_resnets_0_norm2_weight, %p_down_blocks_2_resnets_0_norm2_bias), kwargs = {})\n",
      "    %silu_15 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_13,), kwargs = {})\n",
      "    %dropout_16 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_15, 0.0, False), kwargs = {})\n",
      "    %conv2d_13 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_16, %p_down_blocks_2_resnets_0_conv2_weight, %p_down_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_14 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%conv2d_11, %p_down_blocks_2_resnets_0_conv_shortcut_weight, %p_down_blocks_2_resnets_0_conv_shortcut_bias), kwargs = {})\n",
      "    %add_25 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_14, %conv2d_13), kwargs = {})\n",
      "    %div_13 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_25, 1.0), kwargs = {})\n",
      "    %group_norm_14 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_13, 32, %p_down_blocks_2_attentions_0_norm_weight, %p_down_blocks_2_attentions_0_norm_bias, 1e-06), kwargs = {})\n",
      "    %permute_8 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%group_norm_14, [0, 2, 3, 1]), kwargs = {})\n",
      "    %view_40 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%permute_8, [1, 256, 1280]), kwargs = {})\n",
      "    %linear_55 : [num_users=2] = call_function[target=torch.ops.aten.linear.default](args = (%view_40, %p_down_blocks_2_attentions_0_proj_in_weight, %p_down_blocks_2_attentions_0_proj_in_bias), kwargs = {})\n",
      "    %layer_norm_12 : [num_users=3] = call_function[target=torch.ops.aten.layer_norm.default](args = (%linear_55, [1280], %p_down_blocks_2_attentions_0_transformer_blocks_0_norm1_weight, %p_down_blocks_2_attentions_0_transformer_blocks_0_norm1_bias), kwargs = {})\n",
      "    %linear_56 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_12, %p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight), kwargs = {})\n",
      "    %linear_57 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_12, %p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight), kwargs = {})\n",
      "    %linear_58 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_12, %p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight), kwargs = {})\n",
      "    %view_41 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_56, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_32 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_41, 1, 2), kwargs = {})\n",
      "    %view_42 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_57, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_33 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_42, 1, 2), kwargs = {})\n",
      "    %view_43 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_58, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_34 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_43, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_8 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_32, %transpose_33, %transpose_34), kwargs = {})\n",
      "    %transpose_35 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_8, 1, 2), kwargs = {})\n",
      "    %view_44 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_35, [1, -1, 1280]), kwargs = {})\n",
      "    %_to_copy_10 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_44,), kwargs = {dtype: torch.float32})\n",
      "    %linear_59 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_10, %p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, %p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias), kwargs = {})\n",
      "    %dropout_17 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_59, 0.0, False), kwargs = {})\n",
      "    %div_14 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_17, 1.0), kwargs = {})\n",
      "    %add_26 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_14, %linear_55), kwargs = {})\n",
      "    %layer_norm_13 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_26, [1280], %p_down_blocks_2_attentions_0_transformer_blocks_0_norm2_weight, %p_down_blocks_2_attentions_0_transformer_blocks_0_norm2_bias), kwargs = {})\n",
      "    %linear_60 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_13, %p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight), kwargs = {})\n",
      "    %linear_61 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight), kwargs = {})\n",
      "    %linear_62 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight), kwargs = {})\n",
      "    %view_45 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_60, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_36 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_45, 1, 2), kwargs = {})\n",
      "    %view_46 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_61, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_37 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_46, 1, 2), kwargs = {})\n",
      "    %view_47 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_62, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_38 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_47, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_9 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_36, %transpose_37, %transpose_38), kwargs = {})\n",
      "    %transpose_39 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_9, 1, 2), kwargs = {})\n",
      "    %view_48 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_39, [1, -1, 1280]), kwargs = {})\n",
      "    %_to_copy_11 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_48,), kwargs = {dtype: torch.float32})\n",
      "    %linear_63 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_11, %p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, %p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias), kwargs = {})\n",
      "    %dropout_18 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_63, 0.0, False), kwargs = {})\n",
      "    %div_15 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_18, 1.0), kwargs = {})\n",
      "    %add_27 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_15, %add_26), kwargs = {})\n",
      "    %layer_norm_14 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_27, [1280], %p_down_blocks_2_attentions_0_transformer_blocks_0_norm3_weight, %p_down_blocks_2_attentions_0_transformer_blocks_0_norm3_bias), kwargs = {})\n",
      "    %linear_64 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_14, %p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, %p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias), kwargs = {})\n",
      "    %split_4 : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%linear_64, 5120, -1), kwargs = {})\n",
      "    %getitem_8 : [num_users=1] = call_function[target=operator.getitem](args = (%split_4, 0), kwargs = {})\n",
      "    %getitem_9 : [num_users=1] = call_function[target=operator.getitem](args = (%split_4, 1), kwargs = {})\n",
      "    %gelu_4 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%getitem_9,), kwargs = {})\n",
      "    %mul_7 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_8, %gelu_4), kwargs = {})\n",
      "    %dropout_19 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%mul_7, 0.0, False), kwargs = {})\n",
      "    %linear_65 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%dropout_19, %p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight, %p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias), kwargs = {})\n",
      "    %add_28 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%linear_65, %add_27), kwargs = {})\n",
      "    %linear_66 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%add_28, %p_down_blocks_2_attentions_0_proj_out_weight, %p_down_blocks_2_attentions_0_proj_out_bias), kwargs = {})\n",
      "    %view_49 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_66, [1, 16, 16, 1280]), kwargs = {})\n",
      "    %permute_9 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_49, [0, 3, 1, 2]), kwargs = {})\n",
      "    %clone_4 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_9,), kwargs = {memory_format: torch.contiguous_format})\n",
      "    %add_29 : [num_users=3] = call_function[target=torch.ops.aten.add.Tensor](args = (%clone_4, %div_13), kwargs = {})\n",
      "    %group_norm_15 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_29, 32, %p_down_blocks_2_resnets_1_norm1_weight, %p_down_blocks_2_resnets_1_norm1_bias), kwargs = {})\n",
      "    %silu_16 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_15,), kwargs = {})\n",
      "    %conv2d_15 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_16, %p_down_blocks_2_resnets_1_conv1_weight, %p_down_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %silu_17 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_1,), kwargs = {})\n",
      "    %linear_67 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%silu_17, %p_down_blocks_2_resnets_1_time_emb_proj_weight, %p_down_blocks_2_resnets_1_time_emb_proj_bias), kwargs = {})\n",
      "    %slice_17 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%linear_67, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %slice_18 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_17, 1, 0, 9223372036854775807), kwargs = {})\n",
      "    %unsqueeze_12 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_18, 2), kwargs = {})\n",
      "    %unsqueeze_13 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_12, 3), kwargs = {})\n",
      "    %add_30 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_15, %unsqueeze_13), kwargs = {})\n",
      "    %group_norm_16 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_30, 32, %p_down_blocks_2_resnets_1_norm2_weight, %p_down_blocks_2_resnets_1_norm2_bias), kwargs = {})\n",
      "    %silu_18 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_16,), kwargs = {})\n",
      "    %dropout_20 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_18, 0.0, False), kwargs = {})\n",
      "    %conv2d_16 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_20, %p_down_blocks_2_resnets_1_conv2_weight, %p_down_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_31 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_29, %conv2d_16), kwargs = {})\n",
      "    %div_16 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_31, 1.0), kwargs = {})\n",
      "    %group_norm_17 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_16, 32, %p_down_blocks_2_attentions_1_norm_weight, %p_down_blocks_2_attentions_1_norm_bias, 1e-06), kwargs = {})\n",
      "    %permute_10 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%group_norm_17, [0, 2, 3, 1]), kwargs = {})\n",
      "    %view_50 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%permute_10, [1, 256, 1280]), kwargs = {})\n",
      "    %linear_68 : [num_users=2] = call_function[target=torch.ops.aten.linear.default](args = (%view_50, %p_down_blocks_2_attentions_1_proj_in_weight, %p_down_blocks_2_attentions_1_proj_in_bias), kwargs = {})\n",
      "    %layer_norm_15 : [num_users=3] = call_function[target=torch.ops.aten.layer_norm.default](args = (%linear_68, [1280], %p_down_blocks_2_attentions_1_transformer_blocks_0_norm1_weight, %p_down_blocks_2_attentions_1_transformer_blocks_0_norm1_bias), kwargs = {})\n",
      "    %linear_69 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_15, %p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight), kwargs = {})\n",
      "    %linear_70 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_15, %p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight), kwargs = {})\n",
      "    %linear_71 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_15, %p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight), kwargs = {})\n",
      "    %view_51 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_69, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_40 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_51, 1, 2), kwargs = {})\n",
      "    %view_52 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_70, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_41 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_52, 1, 2), kwargs = {})\n",
      "    %view_53 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_71, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_42 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_53, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_10 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_40, %transpose_41, %transpose_42), kwargs = {})\n",
      "    %transpose_43 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_10, 1, 2), kwargs = {})\n",
      "    %view_54 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_43, [1, -1, 1280]), kwargs = {})\n",
      "    %_to_copy_12 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_54,), kwargs = {dtype: torch.float32})\n",
      "    %linear_72 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_12, %p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, %p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias), kwargs = {})\n",
      "    %dropout_21 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_72, 0.0, False), kwargs = {})\n",
      "    %div_17 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_21, 1.0), kwargs = {})\n",
      "    %add_32 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_17, %linear_68), kwargs = {})\n",
      "    %layer_norm_16 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_32, [1280], %p_down_blocks_2_attentions_1_transformer_blocks_0_norm2_weight, %p_down_blocks_2_attentions_1_transformer_blocks_0_norm2_bias), kwargs = {})\n",
      "    %linear_73 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_16, %p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight), kwargs = {})\n",
      "    %linear_74 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight), kwargs = {})\n",
      "    %linear_75 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight), kwargs = {})\n",
      "    %view_55 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_73, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_44 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_55, 1, 2), kwargs = {})\n",
      "    %view_56 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_74, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_45 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_56, 1, 2), kwargs = {})\n",
      "    %view_57 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_75, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_46 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_57, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_11 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_44, %transpose_45, %transpose_46), kwargs = {})\n",
      "    %transpose_47 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_11, 1, 2), kwargs = {})\n",
      "    %view_58 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_47, [1, -1, 1280]), kwargs = {})\n",
      "    %_to_copy_13 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_58,), kwargs = {dtype: torch.float32})\n",
      "    %linear_76 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_13, %p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, %p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias), kwargs = {})\n",
      "    %dropout_22 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_76, 0.0, False), kwargs = {})\n",
      "    %div_18 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_22, 1.0), kwargs = {})\n",
      "    %add_33 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_18, %add_32), kwargs = {})\n",
      "    %layer_norm_17 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_33, [1280], %p_down_blocks_2_attentions_1_transformer_blocks_0_norm3_weight, %p_down_blocks_2_attentions_1_transformer_blocks_0_norm3_bias), kwargs = {})\n",
      "    %linear_77 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_17, %p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, %p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias), kwargs = {})\n",
      "    %split_5 : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%linear_77, 5120, -1), kwargs = {})\n",
      "    %getitem_10 : [num_users=1] = call_function[target=operator.getitem](args = (%split_5, 0), kwargs = {})\n",
      "    %getitem_11 : [num_users=1] = call_function[target=operator.getitem](args = (%split_5, 1), kwargs = {})\n",
      "    %gelu_5 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%getitem_11,), kwargs = {})\n",
      "    %mul_8 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_10, %gelu_5), kwargs = {})\n",
      "    %dropout_23 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%mul_8, 0.0, False), kwargs = {})\n",
      "    %linear_78 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%dropout_23, %p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight, %p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias), kwargs = {})\n",
      "    %add_34 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%linear_78, %add_33), kwargs = {})\n",
      "    %linear_79 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%add_34, %p_down_blocks_2_attentions_1_proj_out_weight, %p_down_blocks_2_attentions_1_proj_out_bias), kwargs = {})\n",
      "    %view_59 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_79, [1, 16, 16, 1280]), kwargs = {})\n",
      "    %permute_11 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_59, [0, 3, 1, 2]), kwargs = {})\n",
      "    %clone_5 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_11,), kwargs = {memory_format: torch.contiguous_format})\n",
      "    %add_35 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%clone_5, %div_16), kwargs = {})\n",
      "    %conv2d_17 : [num_users=3] = call_function[target=torch.ops.aten.conv2d.default](args = (%add_35, %p_down_blocks_2_downsamplers_0_conv_weight, %p_down_blocks_2_downsamplers_0_conv_bias, [2, 2], [1, 1]), kwargs = {})\n",
      "    %group_norm_18 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_17, 32, %p_down_blocks_3_resnets_0_norm1_weight, %p_down_blocks_3_resnets_0_norm1_bias), kwargs = {})\n",
      "    %silu_19 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_18,), kwargs = {})\n",
      "    %conv2d_18 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_19, %p_down_blocks_3_resnets_0_conv1_weight, %p_down_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %silu_20 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_1,), kwargs = {})\n",
      "    %linear_80 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%silu_20, %p_down_blocks_3_resnets_0_time_emb_proj_weight, %p_down_blocks_3_resnets_0_time_emb_proj_bias), kwargs = {})\n",
      "    %slice_19 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%linear_80, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %slice_20 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_19, 1, 0, 9223372036854775807), kwargs = {})\n",
      "    %unsqueeze_14 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_20, 2), kwargs = {})\n",
      "    %unsqueeze_15 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_14, 3), kwargs = {})\n",
      "    %add_36 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_18, %unsqueeze_15), kwargs = {})\n",
      "    %group_norm_19 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_36, 32, %p_down_blocks_3_resnets_0_norm2_weight, %p_down_blocks_3_resnets_0_norm2_bias), kwargs = {})\n",
      "    %silu_21 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_19,), kwargs = {})\n",
      "    %dropout_24 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_21, 0.0, False), kwargs = {})\n",
      "    %conv2d_19 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_24, %p_down_blocks_3_resnets_0_conv2_weight, %p_down_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_37 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_17, %conv2d_19), kwargs = {})\n",
      "    %div_19 : [num_users=3] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_37, 1.0), kwargs = {})\n",
      "    %group_norm_20 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_19, 32, %p_down_blocks_3_resnets_1_norm1_weight, %p_down_blocks_3_resnets_1_norm1_bias), kwargs = {})\n",
      "    %silu_22 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_20,), kwargs = {})\n",
      "    %conv2d_20 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_22, %p_down_blocks_3_resnets_1_conv1_weight, %p_down_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %silu_23 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_1,), kwargs = {})\n",
      "    %linear_81 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%silu_23, %p_down_blocks_3_resnets_1_time_emb_proj_weight, %p_down_blocks_3_resnets_1_time_emb_proj_bias), kwargs = {})\n",
      "    %slice_21 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%linear_81, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %slice_22 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_21, 1, 0, 9223372036854775807), kwargs = {})\n",
      "    %unsqueeze_16 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_22, 2), kwargs = {})\n",
      "    %unsqueeze_17 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_16, 3), kwargs = {})\n",
      "    %add_38 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_20, %unsqueeze_17), kwargs = {})\n",
      "    %group_norm_21 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_38, 32, %p_down_blocks_3_resnets_1_norm2_weight, %p_down_blocks_3_resnets_1_norm2_bias), kwargs = {})\n",
      "    %silu_24 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_21,), kwargs = {})\n",
      "    %dropout_25 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_24, 0.0, False), kwargs = {})\n",
      "    %conv2d_21 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_25, %p_down_blocks_3_resnets_1_conv2_weight, %p_down_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_39 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_19, %conv2d_21), kwargs = {})\n",
      "    %div_20 : [num_users=3] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_39, 1.0), kwargs = {})\n",
      "    %group_norm_22 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_20, 32, %p_mid_block_resnets_0_norm1_weight, %p_mid_block_resnets_0_norm1_bias), kwargs = {})\n",
      "    %silu_25 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_22,), kwargs = {})\n",
      "    %conv2d_22 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_25, %p_mid_block_resnets_0_conv1_weight, %p_mid_block_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %silu_26 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_1,), kwargs = {})\n",
      "    %linear_82 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%silu_26, %p_mid_block_resnets_0_time_emb_proj_weight, %p_mid_block_resnets_0_time_emb_proj_bias), kwargs = {})\n",
      "    %slice_23 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%linear_82, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %slice_24 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_23, 1, 0, 9223372036854775807), kwargs = {})\n",
      "    %unsqueeze_18 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_24, 2), kwargs = {})\n",
      "    %unsqueeze_19 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_18, 3), kwargs = {})\n",
      "    %add_40 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_22, %unsqueeze_19), kwargs = {})\n",
      "    %group_norm_23 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_40, 32, %p_mid_block_resnets_0_norm2_weight, %p_mid_block_resnets_0_norm2_bias), kwargs = {})\n",
      "    %silu_27 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_23,), kwargs = {})\n",
      "    %dropout_26 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_27, 0.0, False), kwargs = {})\n",
      "    %conv2d_23 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_26, %p_mid_block_resnets_0_conv2_weight, %p_mid_block_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_41 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_20, %conv2d_23), kwargs = {})\n",
      "    %div_21 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_41, 1), kwargs = {})\n",
      "    %group_norm_24 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_21, 32, %p_mid_block_attentions_0_norm_weight, %p_mid_block_attentions_0_norm_bias, 1e-06), kwargs = {})\n",
      "    %permute_12 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%group_norm_24, [0, 2, 3, 1]), kwargs = {})\n",
      "    %view_60 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%permute_12, [1, 64, 1280]), kwargs = {})\n",
      "    %linear_83 : [num_users=2] = call_function[target=torch.ops.aten.linear.default](args = (%view_60, %p_mid_block_attentions_0_proj_in_weight, %p_mid_block_attentions_0_proj_in_bias), kwargs = {})\n",
      "    %layer_norm_18 : [num_users=3] = call_function[target=torch.ops.aten.layer_norm.default](args = (%linear_83, [1280], %p_mid_block_attentions_0_transformer_blocks_0_norm1_weight, %p_mid_block_attentions_0_transformer_blocks_0_norm1_bias), kwargs = {})\n",
      "    %linear_84 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_18, %p_mid_block_attentions_0_transformer_blocks_0_attn1_to_q_weight), kwargs = {})\n",
      "    %linear_85 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_18, %p_mid_block_attentions_0_transformer_blocks_0_attn1_to_k_weight), kwargs = {})\n",
      "    %linear_86 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_18, %p_mid_block_attentions_0_transformer_blocks_0_attn1_to_v_weight), kwargs = {})\n",
      "    %view_61 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_84, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_48 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_61, 1, 2), kwargs = {})\n",
      "    %view_62 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_85, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_49 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_62, 1, 2), kwargs = {})\n",
      "    %view_63 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_86, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_50 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_63, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_12 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_48, %transpose_49, %transpose_50), kwargs = {})\n",
      "    %transpose_51 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_12, 1, 2), kwargs = {})\n",
      "    %view_64 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_51, [1, -1, 1280]), kwargs = {})\n",
      "    %_to_copy_14 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_64,), kwargs = {dtype: torch.float32})\n",
      "    %linear_87 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_14, %p_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, %p_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias), kwargs = {})\n",
      "    %dropout_27 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_87, 0.0, False), kwargs = {})\n",
      "    %div_22 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_27, 1.0), kwargs = {})\n",
      "    %add_42 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_22, %linear_83), kwargs = {})\n",
      "    %layer_norm_19 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_42, [1280], %p_mid_block_attentions_0_transformer_blocks_0_norm2_weight, %p_mid_block_attentions_0_transformer_blocks_0_norm2_bias), kwargs = {})\n",
      "    %linear_88 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_19, %p_mid_block_attentions_0_transformer_blocks_0_attn2_to_q_weight), kwargs = {})\n",
      "    %linear_89 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_mid_block_attentions_0_transformer_blocks_0_attn2_to_k_weight), kwargs = {})\n",
      "    %linear_90 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_mid_block_attentions_0_transformer_blocks_0_attn2_to_v_weight), kwargs = {})\n",
      "    %view_65 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_88, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_52 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_65, 1, 2), kwargs = {})\n",
      "    %view_66 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_89, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_53 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_66, 1, 2), kwargs = {})\n",
      "    %view_67 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_90, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_54 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_67, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_13 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_52, %transpose_53, %transpose_54), kwargs = {})\n",
      "    %transpose_55 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_13, 1, 2), kwargs = {})\n",
      "    %view_68 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_55, [1, -1, 1280]), kwargs = {})\n",
      "    %_to_copy_15 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_68,), kwargs = {dtype: torch.float32})\n",
      "    %linear_91 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_15, %p_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, %p_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_bias), kwargs = {})\n",
      "    %dropout_28 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_91, 0.0, False), kwargs = {})\n",
      "    %div_23 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_28, 1.0), kwargs = {})\n",
      "    %add_43 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_23, %add_42), kwargs = {})\n",
      "    %layer_norm_20 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_43, [1280], %p_mid_block_attentions_0_transformer_blocks_0_norm3_weight, %p_mid_block_attentions_0_transformer_blocks_0_norm3_bias), kwargs = {})\n",
      "    %linear_92 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_20, %p_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, %p_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_bias), kwargs = {})\n",
      "    %split_6 : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%linear_92, 5120, -1), kwargs = {})\n",
      "    %getitem_12 : [num_users=1] = call_function[target=operator.getitem](args = (%split_6, 0), kwargs = {})\n",
      "    %getitem_13 : [num_users=1] = call_function[target=operator.getitem](args = (%split_6, 1), kwargs = {})\n",
      "    %gelu_6 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%getitem_13,), kwargs = {})\n",
      "    %mul_9 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_12, %gelu_6), kwargs = {})\n",
      "    %dropout_29 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%mul_9, 0.0, False), kwargs = {})\n",
      "    %linear_93 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%dropout_29, %p_mid_block_attentions_0_transformer_blocks_0_ff_net_2_weight, %p_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias), kwargs = {})\n",
      "    %add_44 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%linear_93, %add_43), kwargs = {})\n",
      "    %linear_94 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%add_44, %p_mid_block_attentions_0_proj_out_weight, %p_mid_block_attentions_0_proj_out_bias), kwargs = {})\n",
      "    %view_69 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_94, [1, 8, 8, 1280]), kwargs = {})\n",
      "    %permute_13 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_69, [0, 3, 1, 2]), kwargs = {})\n",
      "    %clone_6 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_13,), kwargs = {memory_format: torch.contiguous_format})\n",
      "    %add_45 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%clone_6, %div_21), kwargs = {})\n",
      "    %group_norm_25 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_45, 32, %p_mid_block_resnets_slice_1__none__none___0_norm1_weight, %p_mid_block_resnets_slice_1__none__none___0_norm1_bias), kwargs = {})\n",
      "    %silu_28 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_25,), kwargs = {})\n",
      "    %conv2d_24 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_28, %p_mid_block_resnets_slice_1__none__none___0_conv1_weight, %p_mid_block_resnets_slice_1__none__none___0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %silu_29 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_1,), kwargs = {})\n",
      "    %linear_95 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%silu_29, %p_mid_block_resnets_slice_1__none__none___0_time_emb_proj_weight, %p_mid_block_resnets_slice_1__none__none___0_time_emb_proj_bias), kwargs = {})\n",
      "    %slice_25 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%linear_95, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %slice_26 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_25, 1, 0, 9223372036854775807), kwargs = {})\n",
      "    %unsqueeze_20 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_26, 2), kwargs = {})\n",
      "    %unsqueeze_21 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_20, 3), kwargs = {})\n",
      "    %add_46 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_24, %unsqueeze_21), kwargs = {})\n",
      "    %group_norm_26 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_46, 32, %p_mid_block_resnets_slice_1__none__none___0_norm2_weight, %p_mid_block_resnets_slice_1__none__none___0_norm2_bias), kwargs = {})\n",
      "    %silu_30 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_26,), kwargs = {})\n",
      "    %dropout_30 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_30, 0.0, False), kwargs = {})\n",
      "    %conv2d_25 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_30, %p_mid_block_resnets_slice_1__none__none___0_conv2_weight, %p_mid_block_resnets_slice_1__none__none___0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_47 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_45, %conv2d_25), kwargs = {})\n",
      "    %div_24 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_47, 1), kwargs = {})\n",
      "    %cat_2 : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%div_24, %div_20], 1), kwargs = {})\n",
      "    %group_norm_27 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%cat_2, 32, %p_up_blocks_0_resnets_0_norm1_weight, %p_up_blocks_0_resnets_0_norm1_bias), kwargs = {})\n",
      "    %silu_31 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_27,), kwargs = {})\n",
      "    %conv2d_26 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_31, %p_up_blocks_0_resnets_0_conv1_weight, %p_up_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %silu_32 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_1,), kwargs = {})\n",
      "    %linear_96 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%silu_32, %p_up_blocks_0_resnets_0_time_emb_proj_weight, %p_up_blocks_0_resnets_0_time_emb_proj_bias), kwargs = {})\n",
      "    %slice_27 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%linear_96, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %slice_28 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_27, 1, 0, 9223372036854775807), kwargs = {})\n",
      "    %unsqueeze_22 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_28, 2), kwargs = {})\n",
      "    %unsqueeze_23 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_22, 3), kwargs = {})\n",
      "    %add_48 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_26, %unsqueeze_23), kwargs = {})\n",
      "    %group_norm_28 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_48, 32, %p_up_blocks_0_resnets_0_norm2_weight, %p_up_blocks_0_resnets_0_norm2_bias), kwargs = {})\n",
      "    %silu_33 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_28,), kwargs = {})\n",
      "    %dropout_31 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_33, 0.0, False), kwargs = {})\n",
      "    %conv2d_27 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_31, %p_up_blocks_0_resnets_0_conv2_weight, %p_up_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_28 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%cat_2, %p_up_blocks_0_resnets_0_conv_shortcut_weight, %p_up_blocks_0_resnets_0_conv_shortcut_bias), kwargs = {})\n",
      "    %add_49 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_28, %conv2d_27), kwargs = {})\n",
      "    %div_25 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_49, 1.0), kwargs = {})\n",
      "    %cat_3 : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%div_25, %div_19], 1), kwargs = {})\n",
      "    %group_norm_29 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%cat_3, 32, %p_up_blocks_0_resnets_1_norm1_weight, %p_up_blocks_0_resnets_1_norm1_bias), kwargs = {})\n",
      "    %silu_34 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_29,), kwargs = {})\n",
      "    %conv2d_29 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_34, %p_up_blocks_0_resnets_1_conv1_weight, %p_up_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %silu_35 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_1,), kwargs = {})\n",
      "    %linear_97 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%silu_35, %p_up_blocks_0_resnets_1_time_emb_proj_weight, %p_up_blocks_0_resnets_1_time_emb_proj_bias), kwargs = {})\n",
      "    %slice_29 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%linear_97, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %slice_30 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_29, 1, 0, 9223372036854775807), kwargs = {})\n",
      "    %unsqueeze_24 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_30, 2), kwargs = {})\n",
      "    %unsqueeze_25 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_24, 3), kwargs = {})\n",
      "    %add_50 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_29, %unsqueeze_25), kwargs = {})\n",
      "    %group_norm_30 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_50, 32, %p_up_blocks_0_resnets_1_norm2_weight, %p_up_blocks_0_resnets_1_norm2_bias), kwargs = {})\n",
      "    %silu_36 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_30,), kwargs = {})\n",
      "    %dropout_32 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_36, 0.0, False), kwargs = {})\n",
      "    %conv2d_30 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_32, %p_up_blocks_0_resnets_1_conv2_weight, %p_up_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_31 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%cat_3, %p_up_blocks_0_resnets_1_conv_shortcut_weight, %p_up_blocks_0_resnets_1_conv_shortcut_bias), kwargs = {})\n",
      "    %add_51 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_31, %conv2d_30), kwargs = {})\n",
      "    %div_26 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_51, 1.0), kwargs = {})\n",
      "    %cat_4 : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%div_26, %conv2d_17], 1), kwargs = {})\n",
      "    %group_norm_31 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%cat_4, 32, %p_up_blocks_0_resnets_2_norm1_weight, %p_up_blocks_0_resnets_2_norm1_bias), kwargs = {})\n",
      "    %silu_37 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_31,), kwargs = {})\n",
      "    %conv2d_32 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_37, %p_up_blocks_0_resnets_2_conv1_weight, %p_up_blocks_0_resnets_2_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %silu_38 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_1,), kwargs = {})\n",
      "    %linear_98 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%silu_38, %p_up_blocks_0_resnets_2_time_emb_proj_weight, %p_up_blocks_0_resnets_2_time_emb_proj_bias), kwargs = {})\n",
      "    %slice_31 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%linear_98, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %slice_32 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_31, 1, 0, 9223372036854775807), kwargs = {})\n",
      "    %unsqueeze_26 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_32, 2), kwargs = {})\n",
      "    %unsqueeze_27 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_26, 3), kwargs = {})\n",
      "    %add_52 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_32, %unsqueeze_27), kwargs = {})\n",
      "    %group_norm_32 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_52, 32, %p_up_blocks_0_resnets_2_norm2_weight, %p_up_blocks_0_resnets_2_norm2_bias), kwargs = {})\n",
      "    %silu_39 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_32,), kwargs = {})\n",
      "    %dropout_33 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_39, 0.0, False), kwargs = {})\n",
      "    %conv2d_33 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_33, %p_up_blocks_0_resnets_2_conv2_weight, %p_up_blocks_0_resnets_2_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_34 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%cat_4, %p_up_blocks_0_resnets_2_conv_shortcut_weight, %p_up_blocks_0_resnets_2_conv_shortcut_bias), kwargs = {})\n",
      "    %add_53 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_34, %conv2d_33), kwargs = {})\n",
      "    %div_27 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_53, 1.0), kwargs = {})\n",
      "    %upsample_nearest2d : [num_users=1] = call_function[target=torch.ops.aten.upsample_nearest2d.vec](args = (%div_27, None, [2.0, 2.0]), kwargs = {})\n",
      "    %conv2d_35 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%upsample_nearest2d, %p_up_blocks_0_upsamplers_0_conv_weight, %p_up_blocks_0_upsamplers_0_conv_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %cat_5 : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%conv2d_35, %add_35], 1), kwargs = {})\n",
      "    %group_norm_33 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%cat_5, 32, %p_up_blocks_1_resnets_0_norm1_weight, %p_up_blocks_1_resnets_0_norm1_bias), kwargs = {})\n",
      "    %silu_40 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_33,), kwargs = {})\n",
      "    %conv2d_36 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_40, %p_up_blocks_1_resnets_0_conv1_weight, %p_up_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %silu_41 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_1,), kwargs = {})\n",
      "    %linear_99 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%silu_41, %p_up_blocks_1_resnets_0_time_emb_proj_weight, %p_up_blocks_1_resnets_0_time_emb_proj_bias), kwargs = {})\n",
      "    %slice_33 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%linear_99, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %slice_34 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_33, 1, 0, 9223372036854775807), kwargs = {})\n",
      "    %unsqueeze_28 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_34, 2), kwargs = {})\n",
      "    %unsqueeze_29 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_28, 3), kwargs = {})\n",
      "    %add_54 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_36, %unsqueeze_29), kwargs = {})\n",
      "    %group_norm_34 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_54, 32, %p_up_blocks_1_resnets_0_norm2_weight, %p_up_blocks_1_resnets_0_norm2_bias), kwargs = {})\n",
      "    %silu_42 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_34,), kwargs = {})\n",
      "    %dropout_34 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_42, 0.0, False), kwargs = {})\n",
      "    %conv2d_37 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_34, %p_up_blocks_1_resnets_0_conv2_weight, %p_up_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_38 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%cat_5, %p_up_blocks_1_resnets_0_conv_shortcut_weight, %p_up_blocks_1_resnets_0_conv_shortcut_bias), kwargs = {})\n",
      "    %add_55 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_38, %conv2d_37), kwargs = {})\n",
      "    %div_28 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_55, 1.0), kwargs = {})\n",
      "    %group_norm_35 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_28, 32, %p_up_blocks_1_attentions_0_norm_weight, %p_up_blocks_1_attentions_0_norm_bias, 1e-06), kwargs = {})\n",
      "    %permute_14 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%group_norm_35, [0, 2, 3, 1]), kwargs = {})\n",
      "    %view_70 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%permute_14, [1, 256, 1280]), kwargs = {})\n",
      "    %linear_100 : [num_users=2] = call_function[target=torch.ops.aten.linear.default](args = (%view_70, %p_up_blocks_1_attentions_0_proj_in_weight, %p_up_blocks_1_attentions_0_proj_in_bias), kwargs = {})\n",
      "    %layer_norm_21 : [num_users=3] = call_function[target=torch.ops.aten.layer_norm.default](args = (%linear_100, [1280], %p_up_blocks_1_attentions_0_transformer_blocks_0_norm1_weight, %p_up_blocks_1_attentions_0_transformer_blocks_0_norm1_bias), kwargs = {})\n",
      "    %linear_101 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_21, %p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight), kwargs = {})\n",
      "    %linear_102 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_21, %p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight), kwargs = {})\n",
      "    %linear_103 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_21, %p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight), kwargs = {})\n",
      "    %view_71 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_101, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_56 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_71, 1, 2), kwargs = {})\n",
      "    %view_72 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_102, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_57 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_72, 1, 2), kwargs = {})\n",
      "    %view_73 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_103, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_58 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_73, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_14 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_56, %transpose_57, %transpose_58), kwargs = {})\n",
      "    %transpose_59 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_14, 1, 2), kwargs = {})\n",
      "    %view_74 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_59, [1, -1, 1280]), kwargs = {})\n",
      "    %_to_copy_16 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_74,), kwargs = {dtype: torch.float32})\n",
      "    %linear_104 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_16, %p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, %p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias), kwargs = {})\n",
      "    %dropout_35 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_104, 0.0, False), kwargs = {})\n",
      "    %div_29 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_35, 1.0), kwargs = {})\n",
      "    %add_56 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_29, %linear_100), kwargs = {})\n",
      "    %layer_norm_22 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_56, [1280], %p_up_blocks_1_attentions_0_transformer_blocks_0_norm2_weight, %p_up_blocks_1_attentions_0_transformer_blocks_0_norm2_bias), kwargs = {})\n",
      "    %linear_105 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_22, %p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight), kwargs = {})\n",
      "    %linear_106 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight), kwargs = {})\n",
      "    %linear_107 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight), kwargs = {})\n",
      "    %view_75 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_105, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_60 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_75, 1, 2), kwargs = {})\n",
      "    %view_76 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_106, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_61 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_76, 1, 2), kwargs = {})\n",
      "    %view_77 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_107, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_62 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_77, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_15 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_60, %transpose_61, %transpose_62), kwargs = {})\n",
      "    %transpose_63 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_15, 1, 2), kwargs = {})\n",
      "    %view_78 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_63, [1, -1, 1280]), kwargs = {})\n",
      "    %_to_copy_17 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_78,), kwargs = {dtype: torch.float32})\n",
      "    %linear_108 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_17, %p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, %p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias), kwargs = {})\n",
      "    %dropout_36 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_108, 0.0, False), kwargs = {})\n",
      "    %div_30 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_36, 1.0), kwargs = {})\n",
      "    %add_57 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_30, %add_56), kwargs = {})\n",
      "    %layer_norm_23 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_57, [1280], %p_up_blocks_1_attentions_0_transformer_blocks_0_norm3_weight, %p_up_blocks_1_attentions_0_transformer_blocks_0_norm3_bias), kwargs = {})\n",
      "    %linear_109 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_23, %p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, %p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias), kwargs = {})\n",
      "    %split_7 : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%linear_109, 5120, -1), kwargs = {})\n",
      "    %getitem_14 : [num_users=1] = call_function[target=operator.getitem](args = (%split_7, 0), kwargs = {})\n",
      "    %getitem_15 : [num_users=1] = call_function[target=operator.getitem](args = (%split_7, 1), kwargs = {})\n",
      "    %gelu_7 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%getitem_15,), kwargs = {})\n",
      "    %mul_10 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_14, %gelu_7), kwargs = {})\n",
      "    %dropout_37 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%mul_10, 0.0, False), kwargs = {})\n",
      "    %linear_110 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%dropout_37, %p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight, %p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias), kwargs = {})\n",
      "    %add_58 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%linear_110, %add_57), kwargs = {})\n",
      "    %linear_111 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%add_58, %p_up_blocks_1_attentions_0_proj_out_weight, %p_up_blocks_1_attentions_0_proj_out_bias), kwargs = {})\n",
      "    %view_79 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_111, [1, 16, 16, 1280]), kwargs = {})\n",
      "    %permute_15 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_79, [0, 3, 1, 2]), kwargs = {})\n",
      "    %clone_7 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_15,), kwargs = {memory_format: torch.contiguous_format})\n",
      "    %add_59 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%clone_7, %div_28), kwargs = {})\n",
      "    %cat_6 : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%add_59, %add_29], 1), kwargs = {})\n",
      "    %group_norm_36 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%cat_6, 32, %p_up_blocks_1_resnets_1_norm1_weight, %p_up_blocks_1_resnets_1_norm1_bias), kwargs = {})\n",
      "    %silu_43 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_36,), kwargs = {})\n",
      "    %conv2d_39 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_43, %p_up_blocks_1_resnets_1_conv1_weight, %p_up_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %silu_44 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_1,), kwargs = {})\n",
      "    %linear_112 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%silu_44, %p_up_blocks_1_resnets_1_time_emb_proj_weight, %p_up_blocks_1_resnets_1_time_emb_proj_bias), kwargs = {})\n",
      "    %slice_35 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%linear_112, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %slice_36 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_35, 1, 0, 9223372036854775807), kwargs = {})\n",
      "    %unsqueeze_30 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_36, 2), kwargs = {})\n",
      "    %unsqueeze_31 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_30, 3), kwargs = {})\n",
      "    %add_60 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_39, %unsqueeze_31), kwargs = {})\n",
      "    %group_norm_37 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_60, 32, %p_up_blocks_1_resnets_1_norm2_weight, %p_up_blocks_1_resnets_1_norm2_bias), kwargs = {})\n",
      "    %silu_45 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_37,), kwargs = {})\n",
      "    %dropout_38 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_45, 0.0, False), kwargs = {})\n",
      "    %conv2d_40 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_38, %p_up_blocks_1_resnets_1_conv2_weight, %p_up_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_41 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%cat_6, %p_up_blocks_1_resnets_1_conv_shortcut_weight, %p_up_blocks_1_resnets_1_conv_shortcut_bias), kwargs = {})\n",
      "    %add_61 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_41, %conv2d_40), kwargs = {})\n",
      "    %div_31 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_61, 1.0), kwargs = {})\n",
      "    %group_norm_38 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_31, 32, %p_up_blocks_1_attentions_1_norm_weight, %p_up_blocks_1_attentions_1_norm_bias, 1e-06), kwargs = {})\n",
      "    %permute_16 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%group_norm_38, [0, 2, 3, 1]), kwargs = {})\n",
      "    %view_80 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%permute_16, [1, 256, 1280]), kwargs = {})\n",
      "    %linear_113 : [num_users=2] = call_function[target=torch.ops.aten.linear.default](args = (%view_80, %p_up_blocks_1_attentions_1_proj_in_weight, %p_up_blocks_1_attentions_1_proj_in_bias), kwargs = {})\n",
      "    %layer_norm_24 : [num_users=3] = call_function[target=torch.ops.aten.layer_norm.default](args = (%linear_113, [1280], %p_up_blocks_1_attentions_1_transformer_blocks_0_norm1_weight, %p_up_blocks_1_attentions_1_transformer_blocks_0_norm1_bias), kwargs = {})\n",
      "    %linear_114 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_24, %p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight), kwargs = {})\n",
      "    %linear_115 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_24, %p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight), kwargs = {})\n",
      "    %linear_116 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_24, %p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight), kwargs = {})\n",
      "    %view_81 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_114, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_64 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_81, 1, 2), kwargs = {})\n",
      "    %view_82 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_115, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_65 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_82, 1, 2), kwargs = {})\n",
      "    %view_83 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_116, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_66 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_83, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_16 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_64, %transpose_65, %transpose_66), kwargs = {})\n",
      "    %transpose_67 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_16, 1, 2), kwargs = {})\n",
      "    %view_84 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_67, [1, -1, 1280]), kwargs = {})\n",
      "    %_to_copy_18 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_84,), kwargs = {dtype: torch.float32})\n",
      "    %linear_117 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_18, %p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, %p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias), kwargs = {})\n",
      "    %dropout_39 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_117, 0.0, False), kwargs = {})\n",
      "    %div_32 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_39, 1.0), kwargs = {})\n",
      "    %add_62 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_32, %linear_113), kwargs = {})\n",
      "    %layer_norm_25 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_62, [1280], %p_up_blocks_1_attentions_1_transformer_blocks_0_norm2_weight, %p_up_blocks_1_attentions_1_transformer_blocks_0_norm2_bias), kwargs = {})\n",
      "    %linear_118 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_25, %p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight), kwargs = {})\n",
      "    %linear_119 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight), kwargs = {})\n",
      "    %linear_120 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight), kwargs = {})\n",
      "    %view_85 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_118, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_68 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_85, 1, 2), kwargs = {})\n",
      "    %view_86 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_119, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_69 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_86, 1, 2), kwargs = {})\n",
      "    %view_87 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_120, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_70 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_87, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_17 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_68, %transpose_69, %transpose_70), kwargs = {})\n",
      "    %transpose_71 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_17, 1, 2), kwargs = {})\n",
      "    %view_88 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_71, [1, -1, 1280]), kwargs = {})\n",
      "    %_to_copy_19 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_88,), kwargs = {dtype: torch.float32})\n",
      "    %linear_121 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_19, %p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, %p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias), kwargs = {})\n",
      "    %dropout_40 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_121, 0.0, False), kwargs = {})\n",
      "    %div_33 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_40, 1.0), kwargs = {})\n",
      "    %add_63 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_33, %add_62), kwargs = {})\n",
      "    %layer_norm_26 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_63, [1280], %p_up_blocks_1_attentions_1_transformer_blocks_0_norm3_weight, %p_up_blocks_1_attentions_1_transformer_blocks_0_norm3_bias), kwargs = {})\n",
      "    %linear_122 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_26, %p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, %p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias), kwargs = {})\n",
      "    %split_8 : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%linear_122, 5120, -1), kwargs = {})\n",
      "    %getitem_16 : [num_users=1] = call_function[target=operator.getitem](args = (%split_8, 0), kwargs = {})\n",
      "    %getitem_17 : [num_users=1] = call_function[target=operator.getitem](args = (%split_8, 1), kwargs = {})\n",
      "    %gelu_8 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%getitem_17,), kwargs = {})\n",
      "    %mul_11 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_16, %gelu_8), kwargs = {})\n",
      "    %dropout_41 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%mul_11, 0.0, False), kwargs = {})\n",
      "    %linear_123 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%dropout_41, %p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight, %p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias), kwargs = {})\n",
      "    %add_64 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%linear_123, %add_63), kwargs = {})\n",
      "    %linear_124 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%add_64, %p_up_blocks_1_attentions_1_proj_out_weight, %p_up_blocks_1_attentions_1_proj_out_bias), kwargs = {})\n",
      "    %view_89 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_124, [1, 16, 16, 1280]), kwargs = {})\n",
      "    %permute_17 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_89, [0, 3, 1, 2]), kwargs = {})\n",
      "    %clone_8 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_17,), kwargs = {memory_format: torch.contiguous_format})\n",
      "    %add_65 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%clone_8, %div_31), kwargs = {})\n",
      "    %cat_7 : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%add_65, %conv2d_11], 1), kwargs = {})\n",
      "    %group_norm_39 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%cat_7, 32, %p_up_blocks_1_resnets_2_norm1_weight, %p_up_blocks_1_resnets_2_norm1_bias), kwargs = {})\n",
      "    %silu_46 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_39,), kwargs = {})\n",
      "    %conv2d_42 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_46, %p_up_blocks_1_resnets_2_conv1_weight, %p_up_blocks_1_resnets_2_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %silu_47 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_1,), kwargs = {})\n",
      "    %linear_125 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%silu_47, %p_up_blocks_1_resnets_2_time_emb_proj_weight, %p_up_blocks_1_resnets_2_time_emb_proj_bias), kwargs = {})\n",
      "    %slice_37 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%linear_125, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %slice_38 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_37, 1, 0, 9223372036854775807), kwargs = {})\n",
      "    %unsqueeze_32 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_38, 2), kwargs = {})\n",
      "    %unsqueeze_33 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_32, 3), kwargs = {})\n",
      "    %add_66 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_42, %unsqueeze_33), kwargs = {})\n",
      "    %group_norm_40 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_66, 32, %p_up_blocks_1_resnets_2_norm2_weight, %p_up_blocks_1_resnets_2_norm2_bias), kwargs = {})\n",
      "    %silu_48 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_40,), kwargs = {})\n",
      "    %dropout_42 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_48, 0.0, False), kwargs = {})\n",
      "    %conv2d_43 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_42, %p_up_blocks_1_resnets_2_conv2_weight, %p_up_blocks_1_resnets_2_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_44 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%cat_7, %p_up_blocks_1_resnets_2_conv_shortcut_weight, %p_up_blocks_1_resnets_2_conv_shortcut_bias), kwargs = {})\n",
      "    %add_67 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_44, %conv2d_43), kwargs = {})\n",
      "    %div_34 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_67, 1.0), kwargs = {})\n",
      "    %group_norm_41 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_34, 32, %p_up_blocks_1_attentions_2_norm_weight, %p_up_blocks_1_attentions_2_norm_bias, 1e-06), kwargs = {})\n",
      "    %permute_18 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%group_norm_41, [0, 2, 3, 1]), kwargs = {})\n",
      "    %view_90 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%permute_18, [1, 256, 1280]), kwargs = {})\n",
      "    %linear_126 : [num_users=2] = call_function[target=torch.ops.aten.linear.default](args = (%view_90, %p_up_blocks_1_attentions_2_proj_in_weight, %p_up_blocks_1_attentions_2_proj_in_bias), kwargs = {})\n",
      "    %layer_norm_27 : [num_users=3] = call_function[target=torch.ops.aten.layer_norm.default](args = (%linear_126, [1280], %p_up_blocks_1_attentions_2_transformer_blocks_0_norm1_weight, %p_up_blocks_1_attentions_2_transformer_blocks_0_norm1_bias), kwargs = {})\n",
      "    %linear_127 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_27, %p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q_weight), kwargs = {})\n",
      "    %linear_128 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_27, %p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k_weight), kwargs = {})\n",
      "    %linear_129 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_27, %p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v_weight), kwargs = {})\n",
      "    %view_91 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_127, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_72 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_91, 1, 2), kwargs = {})\n",
      "    %view_92 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_128, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_73 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_92, 1, 2), kwargs = {})\n",
      "    %view_93 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_129, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_74 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_93, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_18 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_72, %transpose_73, %transpose_74), kwargs = {})\n",
      "    %transpose_75 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_18, 1, 2), kwargs = {})\n",
      "    %view_94 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_75, [1, -1, 1280]), kwargs = {})\n",
      "    %_to_copy_20 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_94,), kwargs = {dtype: torch.float32})\n",
      "    %linear_130 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_20, %p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_weight, %p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_bias), kwargs = {})\n",
      "    %dropout_43 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_130, 0.0, False), kwargs = {})\n",
      "    %div_35 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_43, 1.0), kwargs = {})\n",
      "    %add_68 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_35, %linear_126), kwargs = {})\n",
      "    %layer_norm_28 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_68, [1280], %p_up_blocks_1_attentions_2_transformer_blocks_0_norm2_weight, %p_up_blocks_1_attentions_2_transformer_blocks_0_norm2_bias), kwargs = {})\n",
      "    %linear_131 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_28, %p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q_weight), kwargs = {})\n",
      "    %linear_132 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k_weight), kwargs = {})\n",
      "    %linear_133 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v_weight), kwargs = {})\n",
      "    %view_95 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_131, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_76 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_95, 1, 2), kwargs = {})\n",
      "    %view_96 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_132, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_77 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_96, 1, 2), kwargs = {})\n",
      "    %view_97 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_133, [1, -1, 20, 64]), kwargs = {})\n",
      "    %transpose_78 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_97, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_19 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_76, %transpose_77, %transpose_78), kwargs = {})\n",
      "    %transpose_79 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_19, 1, 2), kwargs = {})\n",
      "    %view_98 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_79, [1, -1, 1280]), kwargs = {})\n",
      "    %_to_copy_21 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_98,), kwargs = {dtype: torch.float32})\n",
      "    %linear_134 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_21, %p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_weight, %p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_bias), kwargs = {})\n",
      "    %dropout_44 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_134, 0.0, False), kwargs = {})\n",
      "    %div_36 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_44, 1.0), kwargs = {})\n",
      "    %add_69 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_36, %add_68), kwargs = {})\n",
      "    %layer_norm_29 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_69, [1280], %p_up_blocks_1_attentions_2_transformer_blocks_0_norm3_weight, %p_up_blocks_1_attentions_2_transformer_blocks_0_norm3_bias), kwargs = {})\n",
      "    %linear_135 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_29, %p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_weight, %p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_bias), kwargs = {})\n",
      "    %split_9 : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%linear_135, 5120, -1), kwargs = {})\n",
      "    %getitem_18 : [num_users=1] = call_function[target=operator.getitem](args = (%split_9, 0), kwargs = {})\n",
      "    %getitem_19 : [num_users=1] = call_function[target=operator.getitem](args = (%split_9, 1), kwargs = {})\n",
      "    %gelu_9 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%getitem_19,), kwargs = {})\n",
      "    %mul_12 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_18, %gelu_9), kwargs = {})\n",
      "    %dropout_45 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%mul_12, 0.0, False), kwargs = {})\n",
      "    %linear_136 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%dropout_45, %p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_weight, %p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_bias), kwargs = {})\n",
      "    %add_70 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%linear_136, %add_69), kwargs = {})\n",
      "    %linear_137 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%add_70, %p_up_blocks_1_attentions_2_proj_out_weight, %p_up_blocks_1_attentions_2_proj_out_bias), kwargs = {})\n",
      "    %view_99 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_137, [1, 16, 16, 1280]), kwargs = {})\n",
      "    %permute_19 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_99, [0, 3, 1, 2]), kwargs = {})\n",
      "    %clone_9 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_19,), kwargs = {memory_format: torch.contiguous_format})\n",
      "    %add_71 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%clone_9, %div_34), kwargs = {})\n",
      "    %upsample_nearest2d_1 : [num_users=1] = call_function[target=torch.ops.aten.upsample_nearest2d.vec](args = (%add_71, None, [2.0, 2.0]), kwargs = {})\n",
      "    %conv2d_45 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%upsample_nearest2d_1, %p_up_blocks_1_upsamplers_0_conv_weight, %p_up_blocks_1_upsamplers_0_conv_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %cat_8 : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%conv2d_45, %add_23], 1), kwargs = {})\n",
      "    %group_norm_42 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%cat_8, 32, %p_up_blocks_2_resnets_0_norm1_weight, %p_up_blocks_2_resnets_0_norm1_bias), kwargs = {})\n",
      "    %silu_49 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_42,), kwargs = {})\n",
      "    %conv2d_46 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_49, %p_up_blocks_2_resnets_0_conv1_weight, %p_up_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %silu_50 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_1,), kwargs = {})\n",
      "    %linear_138 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%silu_50, %p_up_blocks_2_resnets_0_time_emb_proj_weight, %p_up_blocks_2_resnets_0_time_emb_proj_bias), kwargs = {})\n",
      "    %slice_39 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%linear_138, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %slice_40 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_39, 1, 0, 9223372036854775807), kwargs = {})\n",
      "    %unsqueeze_34 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_40, 2), kwargs = {})\n",
      "    %unsqueeze_35 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_34, 3), kwargs = {})\n",
      "    %add_72 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_46, %unsqueeze_35), kwargs = {})\n",
      "    %group_norm_43 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_72, 32, %p_up_blocks_2_resnets_0_norm2_weight, %p_up_blocks_2_resnets_0_norm2_bias), kwargs = {})\n",
      "    %silu_51 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_43,), kwargs = {})\n",
      "    %dropout_46 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_51, 0.0, False), kwargs = {})\n",
      "    %conv2d_47 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_46, %p_up_blocks_2_resnets_0_conv2_weight, %p_up_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_48 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%cat_8, %p_up_blocks_2_resnets_0_conv_shortcut_weight, %p_up_blocks_2_resnets_0_conv_shortcut_bias), kwargs = {})\n",
      "    %add_73 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_48, %conv2d_47), kwargs = {})\n",
      "    %div_37 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_73, 1.0), kwargs = {})\n",
      "    %group_norm_44 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_37, 32, %p_up_blocks_2_attentions_0_norm_weight, %p_up_blocks_2_attentions_0_norm_bias, 1e-06), kwargs = {})\n",
      "    %permute_20 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%group_norm_44, [0, 2, 3, 1]), kwargs = {})\n",
      "    %view_100 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%permute_20, [1, 1024, 640]), kwargs = {})\n",
      "    %linear_139 : [num_users=2] = call_function[target=torch.ops.aten.linear.default](args = (%view_100, %p_up_blocks_2_attentions_0_proj_in_weight, %p_up_blocks_2_attentions_0_proj_in_bias), kwargs = {})\n",
      "    %layer_norm_30 : [num_users=3] = call_function[target=torch.ops.aten.layer_norm.default](args = (%linear_139, [640], %p_up_blocks_2_attentions_0_transformer_blocks_0_norm1_weight, %p_up_blocks_2_attentions_0_transformer_blocks_0_norm1_bias), kwargs = {})\n",
      "    %linear_140 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_30, %p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight), kwargs = {})\n",
      "    %linear_141 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_30, %p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight), kwargs = {})\n",
      "    %linear_142 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_30, %p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight), kwargs = {})\n",
      "    %view_101 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_140, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_80 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_101, 1, 2), kwargs = {})\n",
      "    %view_102 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_141, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_81 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_102, 1, 2), kwargs = {})\n",
      "    %view_103 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_142, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_82 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_103, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_20 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_80, %transpose_81, %transpose_82), kwargs = {})\n",
      "    %transpose_83 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_20, 1, 2), kwargs = {})\n",
      "    %view_104 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_83, [1, -1, 640]), kwargs = {})\n",
      "    %_to_copy_22 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_104,), kwargs = {dtype: torch.float32})\n",
      "    %linear_143 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_22, %p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, %p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias), kwargs = {})\n",
      "    %dropout_47 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_143, 0.0, False), kwargs = {})\n",
      "    %div_38 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_47, 1.0), kwargs = {})\n",
      "    %add_74 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_38, %linear_139), kwargs = {})\n",
      "    %layer_norm_31 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_74, [640], %p_up_blocks_2_attentions_0_transformer_blocks_0_norm2_weight, %p_up_blocks_2_attentions_0_transformer_blocks_0_norm2_bias), kwargs = {})\n",
      "    %linear_144 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_31, %p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight), kwargs = {})\n",
      "    %linear_145 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight), kwargs = {})\n",
      "    %linear_146 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight), kwargs = {})\n",
      "    %view_105 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_144, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_84 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_105, 1, 2), kwargs = {})\n",
      "    %view_106 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_145, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_85 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_106, 1, 2), kwargs = {})\n",
      "    %view_107 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_146, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_86 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_107, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_21 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_84, %transpose_85, %transpose_86), kwargs = {})\n",
      "    %transpose_87 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_21, 1, 2), kwargs = {})\n",
      "    %view_108 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_87, [1, -1, 640]), kwargs = {})\n",
      "    %_to_copy_23 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_108,), kwargs = {dtype: torch.float32})\n",
      "    %linear_147 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_23, %p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, %p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias), kwargs = {})\n",
      "    %dropout_48 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_147, 0.0, False), kwargs = {})\n",
      "    %div_39 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_48, 1.0), kwargs = {})\n",
      "    %add_75 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_39, %add_74), kwargs = {})\n",
      "    %layer_norm_32 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_75, [640], %p_up_blocks_2_attentions_0_transformer_blocks_0_norm3_weight, %p_up_blocks_2_attentions_0_transformer_blocks_0_norm3_bias), kwargs = {})\n",
      "    %linear_148 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_32, %p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, %p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias), kwargs = {})\n",
      "    %split_10 : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%linear_148, 2560, -1), kwargs = {})\n",
      "    %getitem_20 : [num_users=1] = call_function[target=operator.getitem](args = (%split_10, 0), kwargs = {})\n",
      "    %getitem_21 : [num_users=1] = call_function[target=operator.getitem](args = (%split_10, 1), kwargs = {})\n",
      "    %gelu_10 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%getitem_21,), kwargs = {})\n",
      "    %mul_13 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_20, %gelu_10), kwargs = {})\n",
      "    %dropout_49 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%mul_13, 0.0, False), kwargs = {})\n",
      "    %linear_149 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%dropout_49, %p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight, %p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias), kwargs = {})\n",
      "    %add_76 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%linear_149, %add_75), kwargs = {})\n",
      "    %linear_150 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%add_76, %p_up_blocks_2_attentions_0_proj_out_weight, %p_up_blocks_2_attentions_0_proj_out_bias), kwargs = {})\n",
      "    %view_109 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_150, [1, 32, 32, 640]), kwargs = {})\n",
      "    %permute_21 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_109, [0, 3, 1, 2]), kwargs = {})\n",
      "    %clone_10 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_21,), kwargs = {memory_format: torch.contiguous_format})\n",
      "    %add_77 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%clone_10, %div_37), kwargs = {})\n",
      "    %cat_9 : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%add_77, %add_17], 1), kwargs = {})\n",
      "    %group_norm_45 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%cat_9, 32, %p_up_blocks_2_resnets_1_norm1_weight, %p_up_blocks_2_resnets_1_norm1_bias), kwargs = {})\n",
      "    %silu_52 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_45,), kwargs = {})\n",
      "    %conv2d_49 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_52, %p_up_blocks_2_resnets_1_conv1_weight, %p_up_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %silu_53 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_1,), kwargs = {})\n",
      "    %linear_151 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%silu_53, %p_up_blocks_2_resnets_1_time_emb_proj_weight, %p_up_blocks_2_resnets_1_time_emb_proj_bias), kwargs = {})\n",
      "    %slice_41 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%linear_151, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %slice_42 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_41, 1, 0, 9223372036854775807), kwargs = {})\n",
      "    %unsqueeze_36 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_42, 2), kwargs = {})\n",
      "    %unsqueeze_37 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_36, 3), kwargs = {})\n",
      "    %add_78 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_49, %unsqueeze_37), kwargs = {})\n",
      "    %group_norm_46 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_78, 32, %p_up_blocks_2_resnets_1_norm2_weight, %p_up_blocks_2_resnets_1_norm2_bias), kwargs = {})\n",
      "    %silu_54 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_46,), kwargs = {})\n",
      "    %dropout_50 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_54, 0.0, False), kwargs = {})\n",
      "    %conv2d_50 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_50, %p_up_blocks_2_resnets_1_conv2_weight, %p_up_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_51 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%cat_9, %p_up_blocks_2_resnets_1_conv_shortcut_weight, %p_up_blocks_2_resnets_1_conv_shortcut_bias), kwargs = {})\n",
      "    %add_79 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_51, %conv2d_50), kwargs = {})\n",
      "    %div_40 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_79, 1.0), kwargs = {})\n",
      "    %group_norm_47 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_40, 32, %p_up_blocks_2_attentions_1_norm_weight, %p_up_blocks_2_attentions_1_norm_bias, 1e-06), kwargs = {})\n",
      "    %permute_22 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%group_norm_47, [0, 2, 3, 1]), kwargs = {})\n",
      "    %view_110 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%permute_22, [1, 1024, 640]), kwargs = {})\n",
      "    %linear_152 : [num_users=2] = call_function[target=torch.ops.aten.linear.default](args = (%view_110, %p_up_blocks_2_attentions_1_proj_in_weight, %p_up_blocks_2_attentions_1_proj_in_bias), kwargs = {})\n",
      "    %layer_norm_33 : [num_users=3] = call_function[target=torch.ops.aten.layer_norm.default](args = (%linear_152, [640], %p_up_blocks_2_attentions_1_transformer_blocks_0_norm1_weight, %p_up_blocks_2_attentions_1_transformer_blocks_0_norm1_bias), kwargs = {})\n",
      "    %linear_153 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_33, %p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight), kwargs = {})\n",
      "    %linear_154 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_33, %p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight), kwargs = {})\n",
      "    %linear_155 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_33, %p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight), kwargs = {})\n",
      "    %view_111 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_153, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_88 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_111, 1, 2), kwargs = {})\n",
      "    %view_112 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_154, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_89 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_112, 1, 2), kwargs = {})\n",
      "    %view_113 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_155, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_90 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_113, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_22 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_88, %transpose_89, %transpose_90), kwargs = {})\n",
      "    %transpose_91 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_22, 1, 2), kwargs = {})\n",
      "    %view_114 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_91, [1, -1, 640]), kwargs = {})\n",
      "    %_to_copy_24 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_114,), kwargs = {dtype: torch.float32})\n",
      "    %linear_156 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_24, %p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, %p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias), kwargs = {})\n",
      "    %dropout_51 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_156, 0.0, False), kwargs = {})\n",
      "    %div_41 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_51, 1.0), kwargs = {})\n",
      "    %add_80 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_41, %linear_152), kwargs = {})\n",
      "    %layer_norm_34 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_80, [640], %p_up_blocks_2_attentions_1_transformer_blocks_0_norm2_weight, %p_up_blocks_2_attentions_1_transformer_blocks_0_norm2_bias), kwargs = {})\n",
      "    %linear_157 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_34, %p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight), kwargs = {})\n",
      "    %linear_158 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight), kwargs = {})\n",
      "    %linear_159 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight), kwargs = {})\n",
      "    %view_115 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_157, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_92 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_115, 1, 2), kwargs = {})\n",
      "    %view_116 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_158, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_93 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_116, 1, 2), kwargs = {})\n",
      "    %view_117 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_159, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_94 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_117, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_23 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_92, %transpose_93, %transpose_94), kwargs = {})\n",
      "    %transpose_95 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_23, 1, 2), kwargs = {})\n",
      "    %view_118 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_95, [1, -1, 640]), kwargs = {})\n",
      "    %_to_copy_25 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_118,), kwargs = {dtype: torch.float32})\n",
      "    %linear_160 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_25, %p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, %p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias), kwargs = {})\n",
      "    %dropout_52 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_160, 0.0, False), kwargs = {})\n",
      "    %div_42 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_52, 1.0), kwargs = {})\n",
      "    %add_81 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_42, %add_80), kwargs = {})\n",
      "    %layer_norm_35 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_81, [640], %p_up_blocks_2_attentions_1_transformer_blocks_0_norm3_weight, %p_up_blocks_2_attentions_1_transformer_blocks_0_norm3_bias), kwargs = {})\n",
      "    %linear_161 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_35, %p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, %p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias), kwargs = {})\n",
      "    %split_11 : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%linear_161, 2560, -1), kwargs = {})\n",
      "    %getitem_22 : [num_users=1] = call_function[target=operator.getitem](args = (%split_11, 0), kwargs = {})\n",
      "    %getitem_23 : [num_users=1] = call_function[target=operator.getitem](args = (%split_11, 1), kwargs = {})\n",
      "    %gelu_11 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%getitem_23,), kwargs = {})\n",
      "    %mul_14 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_22, %gelu_11), kwargs = {})\n",
      "    %dropout_53 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%mul_14, 0.0, False), kwargs = {})\n",
      "    %linear_162 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%dropout_53, %p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight, %p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias), kwargs = {})\n",
      "    %add_82 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%linear_162, %add_81), kwargs = {})\n",
      "    %linear_163 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%add_82, %p_up_blocks_2_attentions_1_proj_out_weight, %p_up_blocks_2_attentions_1_proj_out_bias), kwargs = {})\n",
      "    %view_119 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_163, [1, 32, 32, 640]), kwargs = {})\n",
      "    %permute_23 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_119, [0, 3, 1, 2]), kwargs = {})\n",
      "    %clone_11 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_23,), kwargs = {memory_format: torch.contiguous_format})\n",
      "    %add_83 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%clone_11, %div_40), kwargs = {})\n",
      "    %cat_10 : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%add_83, %conv2d_5], 1), kwargs = {})\n",
      "    %group_norm_48 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%cat_10, 32, %p_up_blocks_2_resnets_2_norm1_weight, %p_up_blocks_2_resnets_2_norm1_bias), kwargs = {})\n",
      "    %silu_55 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_48,), kwargs = {})\n",
      "    %conv2d_52 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_55, %p_up_blocks_2_resnets_2_conv1_weight, %p_up_blocks_2_resnets_2_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %silu_56 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_1,), kwargs = {})\n",
      "    %linear_164 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%silu_56, %p_up_blocks_2_resnets_2_time_emb_proj_weight, %p_up_blocks_2_resnets_2_time_emb_proj_bias), kwargs = {})\n",
      "    %slice_43 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%linear_164, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %slice_44 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_43, 1, 0, 9223372036854775807), kwargs = {})\n",
      "    %unsqueeze_38 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_44, 2), kwargs = {})\n",
      "    %unsqueeze_39 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_38, 3), kwargs = {})\n",
      "    %add_84 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_52, %unsqueeze_39), kwargs = {})\n",
      "    %group_norm_49 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_84, 32, %p_up_blocks_2_resnets_2_norm2_weight, %p_up_blocks_2_resnets_2_norm2_bias), kwargs = {})\n",
      "    %silu_57 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_49,), kwargs = {})\n",
      "    %dropout_54 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_57, 0.0, False), kwargs = {})\n",
      "    %conv2d_53 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_54, %p_up_blocks_2_resnets_2_conv2_weight, %p_up_blocks_2_resnets_2_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_54 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%cat_10, %p_up_blocks_2_resnets_2_conv_shortcut_weight, %p_up_blocks_2_resnets_2_conv_shortcut_bias), kwargs = {})\n",
      "    %add_85 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_54, %conv2d_53), kwargs = {})\n",
      "    %div_43 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_85, 1.0), kwargs = {})\n",
      "    %group_norm_50 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_43, 32, %p_up_blocks_2_attentions_2_norm_weight, %p_up_blocks_2_attentions_2_norm_bias, 1e-06), kwargs = {})\n",
      "    %permute_24 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%group_norm_50, [0, 2, 3, 1]), kwargs = {})\n",
      "    %view_120 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%permute_24, [1, 1024, 640]), kwargs = {})\n",
      "    %linear_165 : [num_users=2] = call_function[target=torch.ops.aten.linear.default](args = (%view_120, %p_up_blocks_2_attentions_2_proj_in_weight, %p_up_blocks_2_attentions_2_proj_in_bias), kwargs = {})\n",
      "    %layer_norm_36 : [num_users=3] = call_function[target=torch.ops.aten.layer_norm.default](args = (%linear_165, [640], %p_up_blocks_2_attentions_2_transformer_blocks_0_norm1_weight, %p_up_blocks_2_attentions_2_transformer_blocks_0_norm1_bias), kwargs = {})\n",
      "    %linear_166 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_36, %p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q_weight), kwargs = {})\n",
      "    %linear_167 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_36, %p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k_weight), kwargs = {})\n",
      "    %linear_168 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_36, %p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v_weight), kwargs = {})\n",
      "    %view_121 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_166, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_96 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_121, 1, 2), kwargs = {})\n",
      "    %view_122 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_167, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_97 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_122, 1, 2), kwargs = {})\n",
      "    %view_123 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_168, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_98 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_123, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_24 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_96, %transpose_97, %transpose_98), kwargs = {})\n",
      "    %transpose_99 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_24, 1, 2), kwargs = {})\n",
      "    %view_124 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_99, [1, -1, 640]), kwargs = {})\n",
      "    %_to_copy_26 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_124,), kwargs = {dtype: torch.float32})\n",
      "    %linear_169 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_26, %p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_weight, %p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_bias), kwargs = {})\n",
      "    %dropout_55 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_169, 0.0, False), kwargs = {})\n",
      "    %div_44 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_55, 1.0), kwargs = {})\n",
      "    %add_86 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_44, %linear_165), kwargs = {})\n",
      "    %layer_norm_37 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_86, [640], %p_up_blocks_2_attentions_2_transformer_blocks_0_norm2_weight, %p_up_blocks_2_attentions_2_transformer_blocks_0_norm2_bias), kwargs = {})\n",
      "    %linear_170 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_37, %p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q_weight), kwargs = {})\n",
      "    %linear_171 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k_weight), kwargs = {})\n",
      "    %linear_172 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v_weight), kwargs = {})\n",
      "    %view_125 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_170, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_100 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_125, 1, 2), kwargs = {})\n",
      "    %view_126 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_171, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_101 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_126, 1, 2), kwargs = {})\n",
      "    %view_127 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_172, [1, -1, 10, 64]), kwargs = {})\n",
      "    %transpose_102 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_127, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_25 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_100, %transpose_101, %transpose_102), kwargs = {})\n",
      "    %transpose_103 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_25, 1, 2), kwargs = {})\n",
      "    %view_128 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_103, [1, -1, 640]), kwargs = {})\n",
      "    %_to_copy_27 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_128,), kwargs = {dtype: torch.float32})\n",
      "    %linear_173 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_27, %p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_weight, %p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_bias), kwargs = {})\n",
      "    %dropout_56 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_173, 0.0, False), kwargs = {})\n",
      "    %div_45 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_56, 1.0), kwargs = {})\n",
      "    %add_87 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_45, %add_86), kwargs = {})\n",
      "    %layer_norm_38 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_87, [640], %p_up_blocks_2_attentions_2_transformer_blocks_0_norm3_weight, %p_up_blocks_2_attentions_2_transformer_blocks_0_norm3_bias), kwargs = {})\n",
      "    %linear_174 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_38, %p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_weight, %p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_bias), kwargs = {})\n",
      "    %split_12 : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%linear_174, 2560, -1), kwargs = {})\n",
      "    %getitem_24 : [num_users=1] = call_function[target=operator.getitem](args = (%split_12, 0), kwargs = {})\n",
      "    %getitem_25 : [num_users=1] = call_function[target=operator.getitem](args = (%split_12, 1), kwargs = {})\n",
      "    %gelu_12 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%getitem_25,), kwargs = {})\n",
      "    %mul_15 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_24, %gelu_12), kwargs = {})\n",
      "    %dropout_57 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%mul_15, 0.0, False), kwargs = {})\n",
      "    %linear_175 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%dropout_57, %p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_weight, %p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_bias), kwargs = {})\n",
      "    %add_88 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%linear_175, %add_87), kwargs = {})\n",
      "    %linear_176 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%add_88, %p_up_blocks_2_attentions_2_proj_out_weight, %p_up_blocks_2_attentions_2_proj_out_bias), kwargs = {})\n",
      "    %view_129 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_176, [1, 32, 32, 640]), kwargs = {})\n",
      "    %permute_25 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_129, [0, 3, 1, 2]), kwargs = {})\n",
      "    %clone_12 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_25,), kwargs = {memory_format: torch.contiguous_format})\n",
      "    %add_89 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%clone_12, %div_43), kwargs = {})\n",
      "    %upsample_nearest2d_2 : [num_users=1] = call_function[target=torch.ops.aten.upsample_nearest2d.vec](args = (%add_89, None, [2.0, 2.0]), kwargs = {})\n",
      "    %conv2d_55 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%upsample_nearest2d_2, %p_up_blocks_2_upsamplers_0_conv_weight, %p_up_blocks_2_upsamplers_0_conv_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %cat_11 : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%conv2d_55, %add_11], 1), kwargs = {})\n",
      "    %group_norm_51 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%cat_11, 32, %p_up_blocks_3_resnets_0_norm1_weight, %p_up_blocks_3_resnets_0_norm1_bias), kwargs = {})\n",
      "    %silu_58 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_51,), kwargs = {})\n",
      "    %conv2d_56 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_58, %p_up_blocks_3_resnets_0_conv1_weight, %p_up_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %silu_59 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_1,), kwargs = {})\n",
      "    %linear_177 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%silu_59, %p_up_blocks_3_resnets_0_time_emb_proj_weight, %p_up_blocks_3_resnets_0_time_emb_proj_bias), kwargs = {})\n",
      "    %slice_45 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%linear_177, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %slice_46 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_45, 1, 0, 9223372036854775807), kwargs = {})\n",
      "    %unsqueeze_40 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_46, 2), kwargs = {})\n",
      "    %unsqueeze_41 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_40, 3), kwargs = {})\n",
      "    %add_90 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_56, %unsqueeze_41), kwargs = {})\n",
      "    %group_norm_52 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_90, 32, %p_up_blocks_3_resnets_0_norm2_weight, %p_up_blocks_3_resnets_0_norm2_bias), kwargs = {})\n",
      "    %silu_60 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_52,), kwargs = {})\n",
      "    %dropout_58 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_60, 0.0, False), kwargs = {})\n",
      "    %conv2d_57 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_58, %p_up_blocks_3_resnets_0_conv2_weight, %p_up_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_58 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%cat_11, %p_up_blocks_3_resnets_0_conv_shortcut_weight, %p_up_blocks_3_resnets_0_conv_shortcut_bias), kwargs = {})\n",
      "    %add_91 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_58, %conv2d_57), kwargs = {})\n",
      "    %div_46 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_91, 1.0), kwargs = {})\n",
      "    %group_norm_53 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_46, 32, %p_up_blocks_3_attentions_0_norm_weight, %p_up_blocks_3_attentions_0_norm_bias, 1e-06), kwargs = {})\n",
      "    %permute_26 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%group_norm_53, [0, 2, 3, 1]), kwargs = {})\n",
      "    %view_130 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%permute_26, [1, 4096, 320]), kwargs = {})\n",
      "    %linear_178 : [num_users=2] = call_function[target=torch.ops.aten.linear.default](args = (%view_130, %p_up_blocks_3_attentions_0_proj_in_weight, %p_up_blocks_3_attentions_0_proj_in_bias), kwargs = {})\n",
      "    %layer_norm_39 : [num_users=3] = call_function[target=torch.ops.aten.layer_norm.default](args = (%linear_178, [320], %p_up_blocks_3_attentions_0_transformer_blocks_0_norm1_weight, %p_up_blocks_3_attentions_0_transformer_blocks_0_norm1_bias), kwargs = {})\n",
      "    %linear_179 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_39, %p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q_weight), kwargs = {})\n",
      "    %linear_180 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_39, %p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k_weight), kwargs = {})\n",
      "    %linear_181 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_39, %p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v_weight), kwargs = {})\n",
      "    %view_131 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_179, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_104 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_131, 1, 2), kwargs = {})\n",
      "    %view_132 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_180, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_105 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_132, 1, 2), kwargs = {})\n",
      "    %view_133 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_181, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_106 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_133, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_26 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_104, %transpose_105, %transpose_106), kwargs = {})\n",
      "    %transpose_107 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_26, 1, 2), kwargs = {})\n",
      "    %view_134 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_107, [1, -1, 320]), kwargs = {})\n",
      "    %_to_copy_28 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_134,), kwargs = {dtype: torch.float32})\n",
      "    %linear_182 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_28, %p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, %p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_bias), kwargs = {})\n",
      "    %dropout_59 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_182, 0.0, False), kwargs = {})\n",
      "    %div_47 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_59, 1.0), kwargs = {})\n",
      "    %add_92 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_47, %linear_178), kwargs = {})\n",
      "    %layer_norm_40 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_92, [320], %p_up_blocks_3_attentions_0_transformer_blocks_0_norm2_weight, %p_up_blocks_3_attentions_0_transformer_blocks_0_norm2_bias), kwargs = {})\n",
      "    %linear_183 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_40, %p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q_weight), kwargs = {})\n",
      "    %linear_184 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k_weight), kwargs = {})\n",
      "    %linear_185 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v_weight), kwargs = {})\n",
      "    %view_135 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_183, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_108 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_135, 1, 2), kwargs = {})\n",
      "    %view_136 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_184, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_109 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_136, 1, 2), kwargs = {})\n",
      "    %view_137 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_185, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_110 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_137, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_27 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_108, %transpose_109, %transpose_110), kwargs = {})\n",
      "    %transpose_111 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_27, 1, 2), kwargs = {})\n",
      "    %view_138 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_111, [1, -1, 320]), kwargs = {})\n",
      "    %_to_copy_29 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_138,), kwargs = {dtype: torch.float32})\n",
      "    %linear_186 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_29, %p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, %p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_bias), kwargs = {})\n",
      "    %dropout_60 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_186, 0.0, False), kwargs = {})\n",
      "    %div_48 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_60, 1.0), kwargs = {})\n",
      "    %add_93 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_48, %add_92), kwargs = {})\n",
      "    %layer_norm_41 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_93, [320], %p_up_blocks_3_attentions_0_transformer_blocks_0_norm3_weight, %p_up_blocks_3_attentions_0_transformer_blocks_0_norm3_bias), kwargs = {})\n",
      "    %linear_187 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_41, %p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, %p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_bias), kwargs = {})\n",
      "    %split_13 : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%linear_187, 1280, -1), kwargs = {})\n",
      "    %getitem_26 : [num_users=1] = call_function[target=operator.getitem](args = (%split_13, 0), kwargs = {})\n",
      "    %getitem_27 : [num_users=1] = call_function[target=operator.getitem](args = (%split_13, 1), kwargs = {})\n",
      "    %gelu_13 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%getitem_27,), kwargs = {})\n",
      "    %mul_16 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_26, %gelu_13), kwargs = {})\n",
      "    %dropout_61 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%mul_16, 0.0, False), kwargs = {})\n",
      "    %linear_188 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%dropout_61, %p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_weight, %p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_bias), kwargs = {})\n",
      "    %add_94 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%linear_188, %add_93), kwargs = {})\n",
      "    %linear_189 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%add_94, %p_up_blocks_3_attentions_0_proj_out_weight, %p_up_blocks_3_attentions_0_proj_out_bias), kwargs = {})\n",
      "    %view_139 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_189, [1, 64, 64, 320]), kwargs = {})\n",
      "    %permute_27 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_139, [0, 3, 1, 2]), kwargs = {})\n",
      "    %clone_13 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_27,), kwargs = {memory_format: torch.contiguous_format})\n",
      "    %add_95 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%clone_13, %div_46), kwargs = {})\n",
      "    %cat_12 : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%add_95, %add_5], 1), kwargs = {})\n",
      "    %group_norm_54 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%cat_12, 32, %p_up_blocks_3_resnets_1_norm1_weight, %p_up_blocks_3_resnets_1_norm1_bias), kwargs = {})\n",
      "    %silu_61 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_54,), kwargs = {})\n",
      "    %conv2d_59 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_61, %p_up_blocks_3_resnets_1_conv1_weight, %p_up_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %silu_62 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_1,), kwargs = {})\n",
      "    %linear_190 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%silu_62, %p_up_blocks_3_resnets_1_time_emb_proj_weight, %p_up_blocks_3_resnets_1_time_emb_proj_bias), kwargs = {})\n",
      "    %slice_47 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%linear_190, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %slice_48 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_47, 1, 0, 9223372036854775807), kwargs = {})\n",
      "    %unsqueeze_42 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_48, 2), kwargs = {})\n",
      "    %unsqueeze_43 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_42, 3), kwargs = {})\n",
      "    %add_96 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_59, %unsqueeze_43), kwargs = {})\n",
      "    %group_norm_55 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_96, 32, %p_up_blocks_3_resnets_1_norm2_weight, %p_up_blocks_3_resnets_1_norm2_bias), kwargs = {})\n",
      "    %silu_63 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_55,), kwargs = {})\n",
      "    %dropout_62 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_63, 0.0, False), kwargs = {})\n",
      "    %conv2d_60 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_62, %p_up_blocks_3_resnets_1_conv2_weight, %p_up_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_61 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%cat_12, %p_up_blocks_3_resnets_1_conv_shortcut_weight, %p_up_blocks_3_resnets_1_conv_shortcut_bias), kwargs = {})\n",
      "    %add_97 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_61, %conv2d_60), kwargs = {})\n",
      "    %div_49 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_97, 1.0), kwargs = {})\n",
      "    %group_norm_56 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_49, 32, %p_up_blocks_3_attentions_1_norm_weight, %p_up_blocks_3_attentions_1_norm_bias, 1e-06), kwargs = {})\n",
      "    %permute_28 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%group_norm_56, [0, 2, 3, 1]), kwargs = {})\n",
      "    %view_140 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%permute_28, [1, 4096, 320]), kwargs = {})\n",
      "    %linear_191 : [num_users=2] = call_function[target=torch.ops.aten.linear.default](args = (%view_140, %p_up_blocks_3_attentions_1_proj_in_weight, %p_up_blocks_3_attentions_1_proj_in_bias), kwargs = {})\n",
      "    %layer_norm_42 : [num_users=3] = call_function[target=torch.ops.aten.layer_norm.default](args = (%linear_191, [320], %p_up_blocks_3_attentions_1_transformer_blocks_0_norm1_weight, %p_up_blocks_3_attentions_1_transformer_blocks_0_norm1_bias), kwargs = {})\n",
      "    %linear_192 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_42, %p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q_weight), kwargs = {})\n",
      "    %linear_193 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_42, %p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k_weight), kwargs = {})\n",
      "    %linear_194 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_42, %p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v_weight), kwargs = {})\n",
      "    %view_141 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_192, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_112 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_141, 1, 2), kwargs = {})\n",
      "    %view_142 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_193, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_113 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_142, 1, 2), kwargs = {})\n",
      "    %view_143 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_194, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_114 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_143, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_28 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_112, %transpose_113, %transpose_114), kwargs = {})\n",
      "    %transpose_115 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_28, 1, 2), kwargs = {})\n",
      "    %view_144 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_115, [1, -1, 320]), kwargs = {})\n",
      "    %_to_copy_30 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_144,), kwargs = {dtype: torch.float32})\n",
      "    %linear_195 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_30, %p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, %p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_bias), kwargs = {})\n",
      "    %dropout_63 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_195, 0.0, False), kwargs = {})\n",
      "    %div_50 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_63, 1.0), kwargs = {})\n",
      "    %add_98 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_50, %linear_191), kwargs = {})\n",
      "    %layer_norm_43 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_98, [320], %p_up_blocks_3_attentions_1_transformer_blocks_0_norm2_weight, %p_up_blocks_3_attentions_1_transformer_blocks_0_norm2_bias), kwargs = {})\n",
      "    %linear_196 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_43, %p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q_weight), kwargs = {})\n",
      "    %linear_197 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k_weight), kwargs = {})\n",
      "    %linear_198 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v_weight), kwargs = {})\n",
      "    %view_145 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_196, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_116 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_145, 1, 2), kwargs = {})\n",
      "    %view_146 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_197, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_117 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_146, 1, 2), kwargs = {})\n",
      "    %view_147 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_198, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_118 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_147, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_29 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_116, %transpose_117, %transpose_118), kwargs = {})\n",
      "    %transpose_119 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_29, 1, 2), kwargs = {})\n",
      "    %view_148 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_119, [1, -1, 320]), kwargs = {})\n",
      "    %_to_copy_31 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_148,), kwargs = {dtype: torch.float32})\n",
      "    %linear_199 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_31, %p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, %p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_bias), kwargs = {})\n",
      "    %dropout_64 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_199, 0.0, False), kwargs = {})\n",
      "    %div_51 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_64, 1.0), kwargs = {})\n",
      "    %add_99 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_51, %add_98), kwargs = {})\n",
      "    %layer_norm_44 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_99, [320], %p_up_blocks_3_attentions_1_transformer_blocks_0_norm3_weight, %p_up_blocks_3_attentions_1_transformer_blocks_0_norm3_bias), kwargs = {})\n",
      "    %linear_200 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_44, %p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, %p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_bias), kwargs = {})\n",
      "    %split_14 : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%linear_200, 1280, -1), kwargs = {})\n",
      "    %getitem_28 : [num_users=1] = call_function[target=operator.getitem](args = (%split_14, 0), kwargs = {})\n",
      "    %getitem_29 : [num_users=1] = call_function[target=operator.getitem](args = (%split_14, 1), kwargs = {})\n",
      "    %gelu_14 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%getitem_29,), kwargs = {})\n",
      "    %mul_17 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_28, %gelu_14), kwargs = {})\n",
      "    %dropout_65 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%mul_17, 0.0, False), kwargs = {})\n",
      "    %linear_201 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%dropout_65, %p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_weight, %p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_bias), kwargs = {})\n",
      "    %add_100 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%linear_201, %add_99), kwargs = {})\n",
      "    %linear_202 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%add_100, %p_up_blocks_3_attentions_1_proj_out_weight, %p_up_blocks_3_attentions_1_proj_out_bias), kwargs = {})\n",
      "    %view_149 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_202, [1, 64, 64, 320]), kwargs = {})\n",
      "    %permute_29 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_149, [0, 3, 1, 2]), kwargs = {})\n",
      "    %clone_14 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_29,), kwargs = {memory_format: torch.contiguous_format})\n",
      "    %add_101 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%clone_14, %div_49), kwargs = {})\n",
      "    %cat_13 : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%add_101, %conv2d], 1), kwargs = {})\n",
      "    %group_norm_57 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%cat_13, 32, %p_up_blocks_3_resnets_2_norm1_weight, %p_up_blocks_3_resnets_2_norm1_bias), kwargs = {})\n",
      "    %silu_64 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_57,), kwargs = {})\n",
      "    %conv2d_62 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_64, %p_up_blocks_3_resnets_2_conv1_weight, %p_up_blocks_3_resnets_2_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %silu_65 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%linear_1,), kwargs = {})\n",
      "    %linear_203 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%silu_65, %p_up_blocks_3_resnets_2_time_emb_proj_weight, %p_up_blocks_3_resnets_2_time_emb_proj_bias), kwargs = {})\n",
      "    %slice_49 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%linear_203, 0, 0, 9223372036854775807), kwargs = {})\n",
      "    %slice_50 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%slice_49, 1, 0, 9223372036854775807), kwargs = {})\n",
      "    %unsqueeze_44 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_50, 2), kwargs = {})\n",
      "    %unsqueeze_45 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze_44, 3), kwargs = {})\n",
      "    %add_102 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_62, %unsqueeze_45), kwargs = {})\n",
      "    %group_norm_58 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_102, 32, %p_up_blocks_3_resnets_2_norm2_weight, %p_up_blocks_3_resnets_2_norm2_bias), kwargs = {})\n",
      "    %silu_66 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_58,), kwargs = {})\n",
      "    %dropout_66 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_66, 0.0, False), kwargs = {})\n",
      "    %conv2d_63 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_66, %p_up_blocks_3_resnets_2_conv2_weight, %p_up_blocks_3_resnets_2_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_64 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%cat_13, %p_up_blocks_3_resnets_2_conv_shortcut_weight, %p_up_blocks_3_resnets_2_conv_shortcut_bias), kwargs = {})\n",
      "    %add_103 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_64, %conv2d_63), kwargs = {})\n",
      "    %div_52 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_103, 1.0), kwargs = {})\n",
      "    %group_norm_59 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_52, 32, %p_up_blocks_3_attentions_2_norm_weight, %p_up_blocks_3_attentions_2_norm_bias, 1e-06), kwargs = {})\n",
      "    %permute_30 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%group_norm_59, [0, 2, 3, 1]), kwargs = {})\n",
      "    %view_150 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%permute_30, [1, 4096, 320]), kwargs = {})\n",
      "    %linear_204 : [num_users=2] = call_function[target=torch.ops.aten.linear.default](args = (%view_150, %p_up_blocks_3_attentions_2_proj_in_weight, %p_up_blocks_3_attentions_2_proj_in_bias), kwargs = {})\n",
      "    %layer_norm_45 : [num_users=3] = call_function[target=torch.ops.aten.layer_norm.default](args = (%linear_204, [320], %p_up_blocks_3_attentions_2_transformer_blocks_0_norm1_weight, %p_up_blocks_3_attentions_2_transformer_blocks_0_norm1_bias), kwargs = {})\n",
      "    %linear_205 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_45, %p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q_weight), kwargs = {})\n",
      "    %linear_206 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_45, %p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k_weight), kwargs = {})\n",
      "    %linear_207 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_45, %p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v_weight), kwargs = {})\n",
      "    %view_151 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_205, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_120 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_151, 1, 2), kwargs = {})\n",
      "    %view_152 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_206, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_121 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_152, 1, 2), kwargs = {})\n",
      "    %view_153 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_207, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_122 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_153, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_30 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_120, %transpose_121, %transpose_122), kwargs = {})\n",
      "    %transpose_123 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_30, 1, 2), kwargs = {})\n",
      "    %view_154 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_123, [1, -1, 320]), kwargs = {})\n",
      "    %_to_copy_32 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_154,), kwargs = {dtype: torch.float32})\n",
      "    %linear_208 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_32, %p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_weight, %p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_bias), kwargs = {})\n",
      "    %dropout_67 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_208, 0.0, False), kwargs = {})\n",
      "    %div_53 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_67, 1.0), kwargs = {})\n",
      "    %add_104 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_53, %linear_204), kwargs = {})\n",
      "    %layer_norm_46 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_104, [320], %p_up_blocks_3_attentions_2_transformer_blocks_0_norm2_weight, %p_up_blocks_3_attentions_2_transformer_blocks_0_norm2_bias), kwargs = {})\n",
      "    %linear_209 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_46, %p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q_weight), kwargs = {})\n",
      "    %linear_210 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k_weight), kwargs = {})\n",
      "    %linear_211 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%encoder_hidden_states, %p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v_weight), kwargs = {})\n",
      "    %view_155 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_209, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_124 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_155, 1, 2), kwargs = {})\n",
      "    %view_156 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_210, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_125 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_156, 1, 2), kwargs = {})\n",
      "    %view_157 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_211, [1, -1, 5, 64]), kwargs = {})\n",
      "    %transpose_126 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_157, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_31 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_124, %transpose_125, %transpose_126), kwargs = {})\n",
      "    %transpose_127 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_31, 1, 2), kwargs = {})\n",
      "    %view_158 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_127, [1, -1, 320]), kwargs = {})\n",
      "    %_to_copy_33 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_158,), kwargs = {dtype: torch.float32})\n",
      "    %linear_212 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_33, %p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_weight, %p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_bias), kwargs = {})\n",
      "    %dropout_68 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_212, 0.0, False), kwargs = {})\n",
      "    %div_54 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%dropout_68, 1.0), kwargs = {})\n",
      "    %add_105 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_54, %add_104), kwargs = {})\n",
      "    %layer_norm_47 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_105, [320], %p_up_blocks_3_attentions_2_transformer_blocks_0_norm3_weight, %p_up_blocks_3_attentions_2_transformer_blocks_0_norm3_bias), kwargs = {})\n",
      "    %linear_213 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_47, %p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_weight, %p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_bias), kwargs = {})\n",
      "    %split_15 : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%linear_213, 1280, -1), kwargs = {})\n",
      "    %getitem_30 : [num_users=1] = call_function[target=operator.getitem](args = (%split_15, 0), kwargs = {})\n",
      "    %getitem_31 : [num_users=1] = call_function[target=operator.getitem](args = (%split_15, 1), kwargs = {})\n",
      "    %gelu_15 : [num_users=1] = call_function[target=torch.ops.aten.gelu.default](args = (%getitem_31,), kwargs = {})\n",
      "    %mul_18 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_30, %gelu_15), kwargs = {})\n",
      "    %dropout_69 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%mul_18, 0.0, False), kwargs = {})\n",
      "    %linear_214 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%dropout_69, %p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_weight, %p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_bias), kwargs = {})\n",
      "    %add_106 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%linear_214, %add_105), kwargs = {})\n",
      "    %linear_215 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%add_106, %p_up_blocks_3_attentions_2_proj_out_weight, %p_up_blocks_3_attentions_2_proj_out_bias), kwargs = {})\n",
      "    %view_159 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_215, [1, 64, 64, 320]), kwargs = {})\n",
      "    %permute_31 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%view_159, [0, 3, 1, 2]), kwargs = {})\n",
      "    %clone_15 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%permute_31,), kwargs = {memory_format: torch.contiguous_format})\n",
      "    %add_107 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%clone_15, %div_52), kwargs = {})\n",
      "    %group_norm_60 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%add_107, 32, %p_conv_norm_out_weight, %p_conv_norm_out_bias), kwargs = {})\n",
      "    %silu_67 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_60,), kwargs = {})\n",
      "    %conv2d_65 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_67, %p_conv_out_weight, %p_conv_out_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    return (conv2d_65,)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import UNet2DConditionModel\n",
    "from torch.export import export, save\n",
    "# working\n",
    "CKPT  = \"prs-eth/marigold-depth-v1-1\"\n",
    "unet  = UNet2DConditionModel.from_pretrained(CKPT, subfolder=\"unet\").cpu()\n",
    "unet.disable_xformers_memory_efficient_attention()\n",
    "\n",
    "example = (\n",
    "    torch.randn(1, 8, 64, 64),   # latent\n",
    "    torch.tensor([0]),           # timestep\n",
    "    torch.randn(1, 77, 1024)     # text enc\n",
    ")\n",
    "\n",
    "gm_unet = export(unet, example)       # ← this is already a GraphModule\n",
    "gm_unet.graph.print_tabular()         # nicely formatted table\n",
    "# or:\n",
    "print(gm_unet.graph)                  # raw ATen graph\n",
    "\n",
    "save(gm_unet, \"unet_fp32.ep\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2193a0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max |Δ| = tensor(0., grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "from torch.export import load\n",
    "\n",
    "# 1. load the exported program\n",
    "ep = load(\"unet_fp32.ep\")\n",
    "\n",
    "# 2. get the GraphModule that does the work\n",
    "gm = ep.module()          # <-- now it's an ordinary nn.Module\n",
    "\n",
    "# 3. run the same inputs you used for export\n",
    "example = (\n",
    "    torch.randn(1, 8, 64, 64),   # latent\n",
    "    torch.tensor([0]),           # timestep\n",
    "    torch.randn(1, 77, 1024)     # text enc\n",
    ")\n",
    "\n",
    "out1 = gm(*example)       # exported graph\n",
    "out2 = unet(*example)     # original eager model\n",
    "\n",
    "print(\"max |Δ| =\", (out1.sample - out2.sample).abs().max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eae4fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abradshaw/Marigold/venv/marigold/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Exported UNet saved to: unet_fp32.ep\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import torch\n",
    "from diffusers import UNet2DConditionModel\n",
    "from torch.export import export, save\n",
    "from torch.utils._pytree import (\n",
    "    register_pytree_node, SUPPORTED_NODES   # SUPPORTED_NODES = current registry\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Locate UNet2DConditionOutput regardless of diffusers version\n",
    "# ------------------------------------------------------------------\n",
    "def _find_unet_output_class():\n",
    "    # new layout (>=0.24)\n",
    "    try:\n",
    "        return importlib.import_module(\n",
    "            \"diffusers.models.unets.unet_2d_condition\"\n",
    "        ).UNet2DConditionOutput\n",
    "    except (ModuleNotFoundError, AttributeError):\n",
    "        pass\n",
    "    # old layout (<=0.23)\n",
    "    try:\n",
    "        return importlib.import_module(\n",
    "            \"diffusers.models.unet_2d_condition\"\n",
    "        ).UNet2DConditionOutput\n",
    "    except (ModuleNotFoundError, AttributeError):\n",
    "        pass\n",
    "    raise RuntimeError(\"Could not locate UNet2DConditionOutput\")\n",
    "\n",
    "UNet2DConditionOutput = _find_unet_output_class()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Register as a pytree **only if not registered yet**\n",
    "# ------------------------------------------------------------------\n",
    "if UNet2DConditionOutput not in SUPPORTED_NODES:\n",
    "    def _flatten(o: UNet2DConditionOutput):\n",
    "        return ((o.sample,), None)      # children, context\n",
    "\n",
    "    def _unflatten(ctx, children):\n",
    "        (sample,) = children\n",
    "        return UNet2DConditionOutput(sample=sample)\n",
    "\n",
    "    register_pytree_node(\n",
    "        UNet2DConditionOutput, _flatten, _unflatten,\n",
    "        serialized_type_name=\"UNet2DConditionOutput\"\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Load the fp32 UNet\n",
    "# ------------------------------------------------------------------\n",
    "CKPT = \"prs-eth/marigold-depth-v1-1\"\n",
    "unet = UNet2DConditionModel.from_pretrained(CKPT, subfolder=\"unet\").cpu()\n",
    "unet.disable_xformers_memory_efficient_attention()   # avoid un-exportable ops\n",
    "unet.eval()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Example inputs (realistic shapes)\n",
    "# ------------------------------------------------------------------\n",
    "example_inputs = (\n",
    "    torch.randn(1, 8, 64, 64),   # latent\n",
    "    torch.tensor([0]),           # timestep\n",
    "    torch.randn(1, 77, 1024)     # text embedding\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5) Export  ➜  ExportedProgram  ➜  .ep file\n",
    "# ------------------------------------------------------------------\n",
    "ep = export(unet, example_inputs)\n",
    "save(ep, \"unet_fp32.ep\")\n",
    "\n",
    "print(\"✅  Exported UNet saved to: unet_fp32.ep\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e1c0180",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.fx.passes.graph_draw'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Find ops that lack a quantisation pattern\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_draw\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FxGraphDrawer\n\u001b[1;32m      4\u001b[0m unsupported \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m gm\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mnodes:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch.fx.passes.graph_draw'"
     ]
    }
   ],
   "source": [
    "# Find ops that lack a quantisation pattern\n",
    "from torch.fx.passes.graph_draw import FxGraphDrawer\n",
    " \n",
    "unsupported = []\n",
    "for n in gm.graph.nodes:\n",
    "    if n.op == \"call_function\":\n",
    "        # Replace with your backend's supported op list\n",
    "        if n.target not in torch._inductor.lowering._registered_ops:\n",
    "            unsupported.append(n.target)\n",
    "\n",
    "print(\"Ops with no Inductor lowering:\", set(unsupported))\n",
    "# view the graph visually\n",
    "FxGraphDrawer(gm, \"unet\").run()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a878afda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hwhe nweneed ot wuanitze\n",
    "from torch.ao.quantization.qconfig_mapping import get_default_qat_qconfig_mapping\n",
    "from torch.ao.quantization.quantizer import X86InductorQuantizer\n",
    "from torch.ao.quantization.quantize_pt2e import prepare_qat_pt2e, convert_pt2e\n",
    "\n",
    "qmap      = get_default_qat_qconfig_mapping(\"x86\")\n",
    "quantizer = X86InductorQuantizer().set_global(qmap)\n",
    "\n",
    "gm_qat = prepare_qat_pt2e(gm, quantizer)   # inserts fake-quant nodes\n",
    "# … fine-tune gm_qat for a few epochs …\n",
    "gm_int8 = convert_pt2e(gm_qat)\n",
    "gm_int8.save(\"unet_int8.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033b9804",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3012efa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name                                                              target                                                            args                                                                                                                                                            kwargs\n",
      "-------------  ----------------------------------------------------------------  ----------------------------------------------------------------  --------------------------------------------------------------------------------------------------------------------------------------------------------------  ------------------------------------------\n",
      "placeholder    p_encoder_conv_in_weight                                          p_encoder_conv_in_weight                                          ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_conv_in_bias                                            p_encoder_conv_in_bias                                            ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_norm1_weight                    p_encoder_down_blocks_0_resnets_0_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_norm1_bias                      p_encoder_down_blocks_0_resnets_0_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_conv1_weight                    p_encoder_down_blocks_0_resnets_0_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_conv1_bias                      p_encoder_down_blocks_0_resnets_0_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_norm2_weight                    p_encoder_down_blocks_0_resnets_0_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_norm2_bias                      p_encoder_down_blocks_0_resnets_0_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_conv2_weight                    p_encoder_down_blocks_0_resnets_0_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_conv2_bias                      p_encoder_down_blocks_0_resnets_0_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_norm1_weight                    p_encoder_down_blocks_0_resnets_1_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_norm1_bias                      p_encoder_down_blocks_0_resnets_1_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_conv1_weight                    p_encoder_down_blocks_0_resnets_1_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_conv1_bias                      p_encoder_down_blocks_0_resnets_1_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_norm2_weight                    p_encoder_down_blocks_0_resnets_1_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_norm2_bias                      p_encoder_down_blocks_0_resnets_1_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_conv2_weight                    p_encoder_down_blocks_0_resnets_1_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_conv2_bias                      p_encoder_down_blocks_0_resnets_1_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_downsamplers_0_conv_weight                p_encoder_down_blocks_0_downsamplers_0_conv_weight                ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_downsamplers_0_conv_bias                  p_encoder_down_blocks_0_downsamplers_0_conv_bias                  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_norm1_weight                    p_encoder_down_blocks_1_resnets_0_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_norm1_bias                      p_encoder_down_blocks_1_resnets_0_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_conv1_weight                    p_encoder_down_blocks_1_resnets_0_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_conv1_bias                      p_encoder_down_blocks_1_resnets_0_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_norm2_weight                    p_encoder_down_blocks_1_resnets_0_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_norm2_bias                      p_encoder_down_blocks_1_resnets_0_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_conv2_weight                    p_encoder_down_blocks_1_resnets_0_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_conv2_bias                      p_encoder_down_blocks_1_resnets_0_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_conv_shortcut_weight            p_encoder_down_blocks_1_resnets_0_conv_shortcut_weight            ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_conv_shortcut_bias              p_encoder_down_blocks_1_resnets_0_conv_shortcut_bias              ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_norm1_weight                    p_encoder_down_blocks_1_resnets_1_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_norm1_bias                      p_encoder_down_blocks_1_resnets_1_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_conv1_weight                    p_encoder_down_blocks_1_resnets_1_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_conv1_bias                      p_encoder_down_blocks_1_resnets_1_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_norm2_weight                    p_encoder_down_blocks_1_resnets_1_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_norm2_bias                      p_encoder_down_blocks_1_resnets_1_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_conv2_weight                    p_encoder_down_blocks_1_resnets_1_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_conv2_bias                      p_encoder_down_blocks_1_resnets_1_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_downsamplers_0_conv_weight                p_encoder_down_blocks_1_downsamplers_0_conv_weight                ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_downsamplers_0_conv_bias                  p_encoder_down_blocks_1_downsamplers_0_conv_bias                  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_norm1_weight                    p_encoder_down_blocks_2_resnets_0_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_norm1_bias                      p_encoder_down_blocks_2_resnets_0_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_conv1_weight                    p_encoder_down_blocks_2_resnets_0_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_conv1_bias                      p_encoder_down_blocks_2_resnets_0_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_norm2_weight                    p_encoder_down_blocks_2_resnets_0_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_norm2_bias                      p_encoder_down_blocks_2_resnets_0_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_conv2_weight                    p_encoder_down_blocks_2_resnets_0_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_conv2_bias                      p_encoder_down_blocks_2_resnets_0_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_conv_shortcut_weight            p_encoder_down_blocks_2_resnets_0_conv_shortcut_weight            ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_conv_shortcut_bias              p_encoder_down_blocks_2_resnets_0_conv_shortcut_bias              ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_norm1_weight                    p_encoder_down_blocks_2_resnets_1_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_norm1_bias                      p_encoder_down_blocks_2_resnets_1_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_conv1_weight                    p_encoder_down_blocks_2_resnets_1_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_conv1_bias                      p_encoder_down_blocks_2_resnets_1_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_norm2_weight                    p_encoder_down_blocks_2_resnets_1_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_norm2_bias                      p_encoder_down_blocks_2_resnets_1_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_conv2_weight                    p_encoder_down_blocks_2_resnets_1_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_conv2_bias                      p_encoder_down_blocks_2_resnets_1_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_downsamplers_0_conv_weight                p_encoder_down_blocks_2_downsamplers_0_conv_weight                ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_downsamplers_0_conv_bias                  p_encoder_down_blocks_2_downsamplers_0_conv_bias                  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_norm1_weight                    p_encoder_down_blocks_3_resnets_0_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_norm1_bias                      p_encoder_down_blocks_3_resnets_0_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_conv1_weight                    p_encoder_down_blocks_3_resnets_0_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_conv1_bias                      p_encoder_down_blocks_3_resnets_0_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_norm2_weight                    p_encoder_down_blocks_3_resnets_0_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_norm2_bias                      p_encoder_down_blocks_3_resnets_0_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_conv2_weight                    p_encoder_down_blocks_3_resnets_0_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_conv2_bias                      p_encoder_down_blocks_3_resnets_0_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_norm1_weight                    p_encoder_down_blocks_3_resnets_1_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_norm1_bias                      p_encoder_down_blocks_3_resnets_1_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_conv1_weight                    p_encoder_down_blocks_3_resnets_1_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_conv1_bias                      p_encoder_down_blocks_3_resnets_1_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_norm2_weight                    p_encoder_down_blocks_3_resnets_1_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_norm2_bias                      p_encoder_down_blocks_3_resnets_1_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_conv2_weight                    p_encoder_down_blocks_3_resnets_1_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_conv2_bias                      p_encoder_down_blocks_3_resnets_1_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_norm1_weight                        p_encoder_mid_block_resnets_0_norm1_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_norm1_bias                          p_encoder_mid_block_resnets_0_norm1_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_conv1_weight                        p_encoder_mid_block_resnets_0_conv1_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_conv1_bias                          p_encoder_mid_block_resnets_0_conv1_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_norm2_weight                        p_encoder_mid_block_resnets_0_norm2_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_norm2_bias                          p_encoder_mid_block_resnets_0_norm2_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_conv2_weight                        p_encoder_mid_block_resnets_0_conv2_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_conv2_bias                          p_encoder_mid_block_resnets_0_conv2_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_group_norm_weight                p_encoder_mid_block_attentions_0_group_norm_weight                ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_group_norm_bias                  p_encoder_mid_block_attentions_0_group_norm_bias                  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_q_weight                      p_encoder_mid_block_attentions_0_to_q_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_q_bias                        p_encoder_mid_block_attentions_0_to_q_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_k_weight                      p_encoder_mid_block_attentions_0_to_k_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_k_bias                        p_encoder_mid_block_attentions_0_to_k_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_v_weight                      p_encoder_mid_block_attentions_0_to_v_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_v_bias                        p_encoder_mid_block_attentions_0_to_v_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_out_0_weight                  p_encoder_mid_block_attentions_0_to_out_0_weight                  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_out_0_bias                    p_encoder_mid_block_attentions_0_to_out_0_bias                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_weight  p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_bias    p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_weight  p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_bias    p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_weight  p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_bias    p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_weight  p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_bias    p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_conv_norm_out_weight                                    p_encoder_conv_norm_out_weight                                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_conv_norm_out_bias                                      p_encoder_conv_norm_out_bias                                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_conv_out_weight                                         p_encoder_conv_out_weight                                         ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_conv_out_bias                                           p_encoder_conv_out_bias                                           ()                                                                                                                                                              {}\n",
      "placeholder    p_quant_conv_weight                                               p_quant_conv_weight                                               ()                                                                                                                                                              {}\n",
      "placeholder    p_quant_conv_bias                                                 p_quant_conv_bias                                                 ()                                                                                                                                                              {}\n",
      "placeholder    p_post_quant_conv_weight                                          p_post_quant_conv_weight                                          ()                                                                                                                                                              {}\n",
      "placeholder    p_post_quant_conv_bias                                            p_post_quant_conv_bias                                            ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_conv_in_weight                                          p_decoder_conv_in_weight                                          ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_conv_in_bias                                            p_decoder_conv_in_bias                                            ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_norm1_weight                        p_decoder_mid_block_resnets_0_norm1_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_norm1_bias                          p_decoder_mid_block_resnets_0_norm1_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_conv1_weight                        p_decoder_mid_block_resnets_0_conv1_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_conv1_bias                          p_decoder_mid_block_resnets_0_conv1_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_norm2_weight                        p_decoder_mid_block_resnets_0_norm2_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_norm2_bias                          p_decoder_mid_block_resnets_0_norm2_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_conv2_weight                        p_decoder_mid_block_resnets_0_conv2_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_conv2_bias                          p_decoder_mid_block_resnets_0_conv2_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_group_norm_weight                p_decoder_mid_block_attentions_0_group_norm_weight                ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_group_norm_bias                  p_decoder_mid_block_attentions_0_group_norm_bias                  ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_q_weight                      p_decoder_mid_block_attentions_0_to_q_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_q_bias                        p_decoder_mid_block_attentions_0_to_q_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_k_weight                      p_decoder_mid_block_attentions_0_to_k_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_k_bias                        p_decoder_mid_block_attentions_0_to_k_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_v_weight                      p_decoder_mid_block_attentions_0_to_v_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_v_bias                        p_decoder_mid_block_attentions_0_to_v_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_out_0_weight                  p_decoder_mid_block_attentions_0_to_out_0_weight                  ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_out_0_bias                    p_decoder_mid_block_attentions_0_to_out_0_bias                    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_weight  p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_bias    p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_weight  p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_bias    p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_weight  p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_bias    p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_weight  p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_bias    p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_norm1_weight                      p_decoder_up_blocks_0_resnets_0_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_norm1_bias                        p_decoder_up_blocks_0_resnets_0_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_conv1_weight                      p_decoder_up_blocks_0_resnets_0_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_conv1_bias                        p_decoder_up_blocks_0_resnets_0_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_norm2_weight                      p_decoder_up_blocks_0_resnets_0_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_norm2_bias                        p_decoder_up_blocks_0_resnets_0_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_conv2_weight                      p_decoder_up_blocks_0_resnets_0_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_conv2_bias                        p_decoder_up_blocks_0_resnets_0_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_norm1_weight                      p_decoder_up_blocks_0_resnets_1_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_norm1_bias                        p_decoder_up_blocks_0_resnets_1_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_conv1_weight                      p_decoder_up_blocks_0_resnets_1_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_conv1_bias                        p_decoder_up_blocks_0_resnets_1_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_norm2_weight                      p_decoder_up_blocks_0_resnets_1_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_norm2_bias                        p_decoder_up_blocks_0_resnets_1_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_conv2_weight                      p_decoder_up_blocks_0_resnets_1_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_conv2_bias                        p_decoder_up_blocks_0_resnets_1_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_norm1_weight                      p_decoder_up_blocks_0_resnets_2_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_norm1_bias                        p_decoder_up_blocks_0_resnets_2_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_conv1_weight                      p_decoder_up_blocks_0_resnets_2_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_conv1_bias                        p_decoder_up_blocks_0_resnets_2_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_norm2_weight                      p_decoder_up_blocks_0_resnets_2_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_norm2_bias                        p_decoder_up_blocks_0_resnets_2_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_conv2_weight                      p_decoder_up_blocks_0_resnets_2_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_conv2_bias                        p_decoder_up_blocks_0_resnets_2_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_upsamplers_0_conv_weight                    p_decoder_up_blocks_0_upsamplers_0_conv_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_upsamplers_0_conv_bias                      p_decoder_up_blocks_0_upsamplers_0_conv_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_norm1_weight                      p_decoder_up_blocks_1_resnets_0_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_norm1_bias                        p_decoder_up_blocks_1_resnets_0_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_conv1_weight                      p_decoder_up_blocks_1_resnets_0_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_conv1_bias                        p_decoder_up_blocks_1_resnets_0_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_norm2_weight                      p_decoder_up_blocks_1_resnets_0_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_norm2_bias                        p_decoder_up_blocks_1_resnets_0_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_conv2_weight                      p_decoder_up_blocks_1_resnets_0_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_conv2_bias                        p_decoder_up_blocks_1_resnets_0_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_norm1_weight                      p_decoder_up_blocks_1_resnets_1_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_norm1_bias                        p_decoder_up_blocks_1_resnets_1_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_conv1_weight                      p_decoder_up_blocks_1_resnets_1_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_conv1_bias                        p_decoder_up_blocks_1_resnets_1_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_norm2_weight                      p_decoder_up_blocks_1_resnets_1_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_norm2_bias                        p_decoder_up_blocks_1_resnets_1_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_conv2_weight                      p_decoder_up_blocks_1_resnets_1_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_conv2_bias                        p_decoder_up_blocks_1_resnets_1_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_norm1_weight                      p_decoder_up_blocks_1_resnets_2_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_norm1_bias                        p_decoder_up_blocks_1_resnets_2_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_conv1_weight                      p_decoder_up_blocks_1_resnets_2_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_conv1_bias                        p_decoder_up_blocks_1_resnets_2_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_norm2_weight                      p_decoder_up_blocks_1_resnets_2_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_norm2_bias                        p_decoder_up_blocks_1_resnets_2_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_conv2_weight                      p_decoder_up_blocks_1_resnets_2_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_conv2_bias                        p_decoder_up_blocks_1_resnets_2_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_upsamplers_0_conv_weight                    p_decoder_up_blocks_1_upsamplers_0_conv_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_upsamplers_0_conv_bias                      p_decoder_up_blocks_1_upsamplers_0_conv_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_norm1_weight                      p_decoder_up_blocks_2_resnets_0_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_norm1_bias                        p_decoder_up_blocks_2_resnets_0_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_conv1_weight                      p_decoder_up_blocks_2_resnets_0_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_conv1_bias                        p_decoder_up_blocks_2_resnets_0_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_norm2_weight                      p_decoder_up_blocks_2_resnets_0_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_norm2_bias                        p_decoder_up_blocks_2_resnets_0_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_conv2_weight                      p_decoder_up_blocks_2_resnets_0_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_conv2_bias                        p_decoder_up_blocks_2_resnets_0_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_conv_shortcut_weight              p_decoder_up_blocks_2_resnets_0_conv_shortcut_weight              ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_conv_shortcut_bias                p_decoder_up_blocks_2_resnets_0_conv_shortcut_bias                ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_norm1_weight                      p_decoder_up_blocks_2_resnets_1_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_norm1_bias                        p_decoder_up_blocks_2_resnets_1_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_conv1_weight                      p_decoder_up_blocks_2_resnets_1_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_conv1_bias                        p_decoder_up_blocks_2_resnets_1_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_norm2_weight                      p_decoder_up_blocks_2_resnets_1_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_norm2_bias                        p_decoder_up_blocks_2_resnets_1_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_conv2_weight                      p_decoder_up_blocks_2_resnets_1_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_conv2_bias                        p_decoder_up_blocks_2_resnets_1_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_norm1_weight                      p_decoder_up_blocks_2_resnets_2_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_norm1_bias                        p_decoder_up_blocks_2_resnets_2_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_conv1_weight                      p_decoder_up_blocks_2_resnets_2_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_conv1_bias                        p_decoder_up_blocks_2_resnets_2_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_norm2_weight                      p_decoder_up_blocks_2_resnets_2_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_norm2_bias                        p_decoder_up_blocks_2_resnets_2_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_conv2_weight                      p_decoder_up_blocks_2_resnets_2_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_conv2_bias                        p_decoder_up_blocks_2_resnets_2_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_upsamplers_0_conv_weight                    p_decoder_up_blocks_2_upsamplers_0_conv_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_upsamplers_0_conv_bias                      p_decoder_up_blocks_2_upsamplers_0_conv_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_norm1_weight                      p_decoder_up_blocks_3_resnets_0_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_norm1_bias                        p_decoder_up_blocks_3_resnets_0_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_conv1_weight                      p_decoder_up_blocks_3_resnets_0_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_conv1_bias                        p_decoder_up_blocks_3_resnets_0_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_norm2_weight                      p_decoder_up_blocks_3_resnets_0_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_norm2_bias                        p_decoder_up_blocks_3_resnets_0_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_conv2_weight                      p_decoder_up_blocks_3_resnets_0_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_conv2_bias                        p_decoder_up_blocks_3_resnets_0_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_conv_shortcut_weight              p_decoder_up_blocks_3_resnets_0_conv_shortcut_weight              ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_conv_shortcut_bias                p_decoder_up_blocks_3_resnets_0_conv_shortcut_bias                ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_norm1_weight                      p_decoder_up_blocks_3_resnets_1_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_norm1_bias                        p_decoder_up_blocks_3_resnets_1_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_conv1_weight                      p_decoder_up_blocks_3_resnets_1_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_conv1_bias                        p_decoder_up_blocks_3_resnets_1_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_norm2_weight                      p_decoder_up_blocks_3_resnets_1_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_norm2_bias                        p_decoder_up_blocks_3_resnets_1_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_conv2_weight                      p_decoder_up_blocks_3_resnets_1_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_conv2_bias                        p_decoder_up_blocks_3_resnets_1_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_norm1_weight                      p_decoder_up_blocks_3_resnets_2_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_norm1_bias                        p_decoder_up_blocks_3_resnets_2_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_conv1_weight                      p_decoder_up_blocks_3_resnets_2_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_conv1_bias                        p_decoder_up_blocks_3_resnets_2_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_norm2_weight                      p_decoder_up_blocks_3_resnets_2_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_norm2_bias                        p_decoder_up_blocks_3_resnets_2_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_conv2_weight                      p_decoder_up_blocks_3_resnets_2_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_conv2_bias                        p_decoder_up_blocks_3_resnets_2_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_conv_norm_out_weight                                    p_decoder_conv_norm_out_weight                                    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_conv_norm_out_bias                                      p_decoder_conv_norm_out_bias                                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_conv_out_weight                                         p_decoder_conv_out_weight                                         ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_conv_out_bias                                           p_decoder_conv_out_bias                                           ()                                                                                                                                                              {}\n",
      "placeholder    sample                                                            sample                                                            ()                                                                                                                                                              {}\n",
      "call_function  conv2d                                                            aten.conv2d.default                                               (sample, p_encoder_conv_in_weight, p_encoder_conv_in_bias, [1, 1], [1, 1])                                                                                      {}\n",
      "call_function  group_norm                                                        aten.group_norm.default                                           (conv2d, 32, p_encoder_down_blocks_0_resnets_0_norm1_weight, p_encoder_down_blocks_0_resnets_0_norm1_bias, 1e-06)                                               {}\n",
      "call_function  silu                                                              aten.silu.default                                                 (group_norm,)                                                                                                                                                   {}\n",
      "call_function  conv2d_1                                                          aten.conv2d.default                                               (silu, p_encoder_down_blocks_0_resnets_0_conv1_weight, p_encoder_down_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1])                                            {}\n",
      "call_function  group_norm_1                                                      aten.group_norm.default                                           (conv2d_1, 32, p_encoder_down_blocks_0_resnets_0_norm2_weight, p_encoder_down_blocks_0_resnets_0_norm2_bias, 1e-06)                                             {}\n",
      "call_function  silu_1                                                            aten.silu.default                                                 (group_norm_1,)                                                                                                                                                 {}\n",
      "call_function  dropout                                                           aten.dropout.default                                              (silu_1, 0.0, False)                                                                                                                                            {}\n",
      "call_function  conv2d_2                                                          aten.conv2d.default                                               (dropout, p_encoder_down_blocks_0_resnets_0_conv2_weight, p_encoder_down_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1])                                         {}\n",
      "call_function  add                                                               aten.add.Tensor                                                   (conv2d, conv2d_2)                                                                                                                                              {}\n",
      "call_function  div                                                               aten.div.Tensor                                                   (add, 1.0)                                                                                                                                                      {}\n",
      "call_function  group_norm_2                                                      aten.group_norm.default                                           (div, 32, p_encoder_down_blocks_0_resnets_1_norm1_weight, p_encoder_down_blocks_0_resnets_1_norm1_bias, 1e-06)                                                  {}\n",
      "call_function  silu_2                                                            aten.silu.default                                                 (group_norm_2,)                                                                                                                                                 {}\n",
      "call_function  conv2d_3                                                          aten.conv2d.default                                               (silu_2, p_encoder_down_blocks_0_resnets_1_conv1_weight, p_encoder_down_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  group_norm_3                                                      aten.group_norm.default                                           (conv2d_3, 32, p_encoder_down_blocks_0_resnets_1_norm2_weight, p_encoder_down_blocks_0_resnets_1_norm2_bias, 1e-06)                                             {}\n",
      "call_function  silu_3                                                            aten.silu.default                                                 (group_norm_3,)                                                                                                                                                 {}\n",
      "call_function  dropout_1                                                         aten.dropout.default                                              (silu_3, 0.0, False)                                                                                                                                            {}\n",
      "call_function  conv2d_4                                                          aten.conv2d.default                                               (dropout_1, p_encoder_down_blocks_0_resnets_1_conv2_weight, p_encoder_down_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  add_1                                                             aten.add.Tensor                                                   (div, conv2d_4)                                                                                                                                                 {}\n",
      "call_function  div_1                                                             aten.div.Tensor                                                   (add_1, 1.0)                                                                                                                                                    {}\n",
      "call_function  pad                                                               aten.pad.default                                                  (div_1, [0, 1, 0, 1], 'constant', 0.0)                                                                                                                          {}\n",
      "call_function  conv2d_5                                                          aten.conv2d.default                                               (pad, p_encoder_down_blocks_0_downsamplers_0_conv_weight, p_encoder_down_blocks_0_downsamplers_0_conv_bias, [2, 2])                                             {}\n",
      "call_function  group_norm_4                                                      aten.group_norm.default                                           (conv2d_5, 32, p_encoder_down_blocks_1_resnets_0_norm1_weight, p_encoder_down_blocks_1_resnets_0_norm1_bias, 1e-06)                                             {}\n",
      "call_function  silu_4                                                            aten.silu.default                                                 (group_norm_4,)                                                                                                                                                 {}\n",
      "call_function  conv2d_6                                                          aten.conv2d.default                                               (silu_4, p_encoder_down_blocks_1_resnets_0_conv1_weight, p_encoder_down_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  group_norm_5                                                      aten.group_norm.default                                           (conv2d_6, 32, p_encoder_down_blocks_1_resnets_0_norm2_weight, p_encoder_down_blocks_1_resnets_0_norm2_bias, 1e-06)                                             {}\n",
      "call_function  silu_5                                                            aten.silu.default                                                 (group_norm_5,)                                                                                                                                                 {}\n",
      "call_function  dropout_2                                                         aten.dropout.default                                              (silu_5, 0.0, False)                                                                                                                                            {}\n",
      "call_function  conv2d_7                                                          aten.conv2d.default                                               (dropout_2, p_encoder_down_blocks_1_resnets_0_conv2_weight, p_encoder_down_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  conv2d_8                                                          aten.conv2d.default                                               (conv2d_5, p_encoder_down_blocks_1_resnets_0_conv_shortcut_weight, p_encoder_down_blocks_1_resnets_0_conv_shortcut_bias)                                        {}\n",
      "call_function  add_2                                                             aten.add.Tensor                                                   (conv2d_8, conv2d_7)                                                                                                                                            {}\n",
      "call_function  div_2                                                             aten.div.Tensor                                                   (add_2, 1.0)                                                                                                                                                    {}\n",
      "call_function  group_norm_6                                                      aten.group_norm.default                                           (div_2, 32, p_encoder_down_blocks_1_resnets_1_norm1_weight, p_encoder_down_blocks_1_resnets_1_norm1_bias, 1e-06)                                                {}\n",
      "call_function  silu_6                                                            aten.silu.default                                                 (group_norm_6,)                                                                                                                                                 {}\n",
      "call_function  conv2d_9                                                          aten.conv2d.default                                               (silu_6, p_encoder_down_blocks_1_resnets_1_conv1_weight, p_encoder_down_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  group_norm_7                                                      aten.group_norm.default                                           (conv2d_9, 32, p_encoder_down_blocks_1_resnets_1_norm2_weight, p_encoder_down_blocks_1_resnets_1_norm2_bias, 1e-06)                                             {}\n",
      "call_function  silu_7                                                            aten.silu.default                                                 (group_norm_7,)                                                                                                                                                 {}\n",
      "call_function  dropout_3                                                         aten.dropout.default                                              (silu_7, 0.0, False)                                                                                                                                            {}\n",
      "call_function  conv2d_10                                                         aten.conv2d.default                                               (dropout_3, p_encoder_down_blocks_1_resnets_1_conv2_weight, p_encoder_down_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  add_3                                                             aten.add.Tensor                                                   (div_2, conv2d_10)                                                                                                                                              {}\n",
      "call_function  div_3                                                             aten.div.Tensor                                                   (add_3, 1.0)                                                                                                                                                    {}\n",
      "call_function  pad_1                                                             aten.pad.default                                                  (div_3, [0, 1, 0, 1], 'constant', 0.0)                                                                                                                          {}\n",
      "call_function  conv2d_11                                                         aten.conv2d.default                                               (pad_1, p_encoder_down_blocks_1_downsamplers_0_conv_weight, p_encoder_down_blocks_1_downsamplers_0_conv_bias, [2, 2])                                           {}\n",
      "call_function  group_norm_8                                                      aten.group_norm.default                                           (conv2d_11, 32, p_encoder_down_blocks_2_resnets_0_norm1_weight, p_encoder_down_blocks_2_resnets_0_norm1_bias, 1e-06)                                            {}\n",
      "call_function  silu_8                                                            aten.silu.default                                                 (group_norm_8,)                                                                                                                                                 {}\n",
      "call_function  conv2d_12                                                         aten.conv2d.default                                               (silu_8, p_encoder_down_blocks_2_resnets_0_conv1_weight, p_encoder_down_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  group_norm_9                                                      aten.group_norm.default                                           (conv2d_12, 32, p_encoder_down_blocks_2_resnets_0_norm2_weight, p_encoder_down_blocks_2_resnets_0_norm2_bias, 1e-06)                                            {}\n",
      "call_function  silu_9                                                            aten.silu.default                                                 (group_norm_9,)                                                                                                                                                 {}\n",
      "call_function  dropout_4                                                         aten.dropout.default                                              (silu_9, 0.0, False)                                                                                                                                            {}\n",
      "call_function  conv2d_13                                                         aten.conv2d.default                                               (dropout_4, p_encoder_down_blocks_2_resnets_0_conv2_weight, p_encoder_down_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  conv2d_14                                                         aten.conv2d.default                                               (conv2d_11, p_encoder_down_blocks_2_resnets_0_conv_shortcut_weight, p_encoder_down_blocks_2_resnets_0_conv_shortcut_bias)                                       {}\n",
      "call_function  add_4                                                             aten.add.Tensor                                                   (conv2d_14, conv2d_13)                                                                                                                                          {}\n",
      "call_function  div_4                                                             aten.div.Tensor                                                   (add_4, 1.0)                                                                                                                                                    {}\n",
      "call_function  group_norm_10                                                     aten.group_norm.default                                           (div_4, 32, p_encoder_down_blocks_2_resnets_1_norm1_weight, p_encoder_down_blocks_2_resnets_1_norm1_bias, 1e-06)                                                {}\n",
      "call_function  silu_10                                                           aten.silu.default                                                 (group_norm_10,)                                                                                                                                                {}\n",
      "call_function  conv2d_15                                                         aten.conv2d.default                                               (silu_10, p_encoder_down_blocks_2_resnets_1_conv1_weight, p_encoder_down_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1])                                         {}\n",
      "call_function  group_norm_11                                                     aten.group_norm.default                                           (conv2d_15, 32, p_encoder_down_blocks_2_resnets_1_norm2_weight, p_encoder_down_blocks_2_resnets_1_norm2_bias, 1e-06)                                            {}\n",
      "call_function  silu_11                                                           aten.silu.default                                                 (group_norm_11,)                                                                                                                                                {}\n",
      "call_function  dropout_5                                                         aten.dropout.default                                              (silu_11, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_16                                                         aten.conv2d.default                                               (dropout_5, p_encoder_down_blocks_2_resnets_1_conv2_weight, p_encoder_down_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  add_5                                                             aten.add.Tensor                                                   (div_4, conv2d_16)                                                                                                                                              {}\n",
      "call_function  div_5                                                             aten.div.Tensor                                                   (add_5, 1.0)                                                                                                                                                    {}\n",
      "call_function  pad_2                                                             aten.pad.default                                                  (div_5, [0, 1, 0, 1], 'constant', 0.0)                                                                                                                          {}\n",
      "call_function  conv2d_17                                                         aten.conv2d.default                                               (pad_2, p_encoder_down_blocks_2_downsamplers_0_conv_weight, p_encoder_down_blocks_2_downsamplers_0_conv_bias, [2, 2])                                           {}\n",
      "call_function  group_norm_12                                                     aten.group_norm.default                                           (conv2d_17, 32, p_encoder_down_blocks_3_resnets_0_norm1_weight, p_encoder_down_blocks_3_resnets_0_norm1_bias, 1e-06)                                            {}\n",
      "call_function  silu_12                                                           aten.silu.default                                                 (group_norm_12,)                                                                                                                                                {}\n",
      "call_function  conv2d_18                                                         aten.conv2d.default                                               (silu_12, p_encoder_down_blocks_3_resnets_0_conv1_weight, p_encoder_down_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1])                                         {}\n",
      "call_function  group_norm_13                                                     aten.group_norm.default                                           (conv2d_18, 32, p_encoder_down_blocks_3_resnets_0_norm2_weight, p_encoder_down_blocks_3_resnets_0_norm2_bias, 1e-06)                                            {}\n",
      "call_function  silu_13                                                           aten.silu.default                                                 (group_norm_13,)                                                                                                                                                {}\n",
      "call_function  dropout_6                                                         aten.dropout.default                                              (silu_13, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_19                                                         aten.conv2d.default                                               (dropout_6, p_encoder_down_blocks_3_resnets_0_conv2_weight, p_encoder_down_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  add_6                                                             aten.add.Tensor                                                   (conv2d_17, conv2d_19)                                                                                                                                          {}\n",
      "call_function  div_6                                                             aten.div.Tensor                                                   (add_6, 1.0)                                                                                                                                                    {}\n",
      "call_function  group_norm_14                                                     aten.group_norm.default                                           (div_6, 32, p_encoder_down_blocks_3_resnets_1_norm1_weight, p_encoder_down_blocks_3_resnets_1_norm1_bias, 1e-06)                                                {}\n",
      "call_function  silu_14                                                           aten.silu.default                                                 (group_norm_14,)                                                                                                                                                {}\n",
      "call_function  conv2d_20                                                         aten.conv2d.default                                               (silu_14, p_encoder_down_blocks_3_resnets_1_conv1_weight, p_encoder_down_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1])                                         {}\n",
      "call_function  group_norm_15                                                     aten.group_norm.default                                           (conv2d_20, 32, p_encoder_down_blocks_3_resnets_1_norm2_weight, p_encoder_down_blocks_3_resnets_1_norm2_bias, 1e-06)                                            {}\n",
      "call_function  silu_15                                                           aten.silu.default                                                 (group_norm_15,)                                                                                                                                                {}\n",
      "call_function  dropout_7                                                         aten.dropout.default                                              (silu_15, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_21                                                         aten.conv2d.default                                               (dropout_7, p_encoder_down_blocks_3_resnets_1_conv2_weight, p_encoder_down_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  add_7                                                             aten.add.Tensor                                                   (div_6, conv2d_21)                                                                                                                                              {}\n",
      "call_function  div_7                                                             aten.div.Tensor                                                   (add_7, 1.0)                                                                                                                                                    {}\n",
      "call_function  group_norm_16                                                     aten.group_norm.default                                           (div_7, 32, p_encoder_mid_block_resnets_0_norm1_weight, p_encoder_mid_block_resnets_0_norm1_bias, 1e-06)                                                        {}\n",
      "call_function  silu_16                                                           aten.silu.default                                                 (group_norm_16,)                                                                                                                                                {}\n",
      "call_function  conv2d_22                                                         aten.conv2d.default                                               (silu_16, p_encoder_mid_block_resnets_0_conv1_weight, p_encoder_mid_block_resnets_0_conv1_bias, [1, 1], [1, 1])                                                 {}\n",
      "call_function  group_norm_17                                                     aten.group_norm.default                                           (conv2d_22, 32, p_encoder_mid_block_resnets_0_norm2_weight, p_encoder_mid_block_resnets_0_norm2_bias, 1e-06)                                                    {}\n",
      "call_function  silu_17                                                           aten.silu.default                                                 (group_norm_17,)                                                                                                                                                {}\n",
      "call_function  dropout_8                                                         aten.dropout.default                                              (silu_17, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_23                                                         aten.conv2d.default                                               (dropout_8, p_encoder_mid_block_resnets_0_conv2_weight, p_encoder_mid_block_resnets_0_conv2_bias, [1, 1], [1, 1])                                               {}\n",
      "call_function  add_8                                                             aten.add.Tensor                                                   (div_7, conv2d_23)                                                                                                                                              {}\n",
      "call_function  div_8                                                             aten.div.Tensor                                                   (add_8, 1)                                                                                                                                                      {}\n",
      "call_function  view                                                              aten.view.default                                                 (div_8, [1, 512, 4096])                                                                                                                                         {}\n",
      "call_function  transpose                                                         aten.transpose.int                                                (view, 1, 2)                                                                                                                                                    {}\n",
      "call_function  transpose_1                                                       aten.transpose.int                                                (transpose, 1, 2)                                                                                                                                               {}\n",
      "call_function  group_norm_18                                                     aten.group_norm.default                                           (transpose_1, 32, p_encoder_mid_block_attentions_0_group_norm_weight, p_encoder_mid_block_attentions_0_group_norm_bias, 1e-06)                                  {}\n",
      "call_function  transpose_2                                                       aten.transpose.int                                                (group_norm_18, 1, 2)                                                                                                                                           {}\n",
      "call_function  linear                                                            aten.linear.default                                               (transpose_2, p_encoder_mid_block_attentions_0_to_q_weight, p_encoder_mid_block_attentions_0_to_q_bias)                                                         {}\n",
      "call_function  linear_1                                                          aten.linear.default                                               (transpose_2, p_encoder_mid_block_attentions_0_to_k_weight, p_encoder_mid_block_attentions_0_to_k_bias)                                                         {}\n",
      "call_function  linear_2                                                          aten.linear.default                                               (transpose_2, p_encoder_mid_block_attentions_0_to_v_weight, p_encoder_mid_block_attentions_0_to_v_bias)                                                         {}\n",
      "call_function  view_1                                                            aten.view.default                                                 (linear, [1, -1, 1, 512])                                                                                                                                       {}\n",
      "call_function  transpose_3                                                       aten.transpose.int                                                (view_1, 1, 2)                                                                                                                                                  {}\n",
      "call_function  view_2                                                            aten.view.default                                                 (linear_1, [1, -1, 1, 512])                                                                                                                                     {}\n",
      "call_function  transpose_4                                                       aten.transpose.int                                                (view_2, 1, 2)                                                                                                                                                  {}\n",
      "call_function  view_3                                                            aten.view.default                                                 (linear_2, [1, -1, 1, 512])                                                                                                                                     {}\n",
      "call_function  transpose_5                                                       aten.transpose.int                                                (view_3, 1, 2)                                                                                                                                                  {}\n",
      "call_function  scaled_dot_product_attention                                      aten.scaled_dot_product_attention.default                         (transpose_3, transpose_4, transpose_5)                                                                                                                         {}\n",
      "call_function  transpose_6                                                       aten.transpose.int                                                (scaled_dot_product_attention, 1, 2)                                                                                                                            {}\n",
      "call_function  view_4                                                            aten.view.default                                                 (transpose_6, [1, -1, 512])                                                                                                                                     {}\n",
      "call_function  _to_copy                                                          aten._to_copy.default                                             (view_4,)                                                                                                                                                       {'dtype': torch.float32}\n",
      "call_function  linear_3                                                          aten.linear.default                                               (_to_copy, p_encoder_mid_block_attentions_0_to_out_0_weight, p_encoder_mid_block_attentions_0_to_out_0_bias)                                                    {}\n",
      "call_function  dropout_9                                                         aten.dropout.default                                              (linear_3, 0.0, False)                                                                                                                                          {}\n",
      "call_function  transpose_7                                                       aten.transpose.int                                                (dropout_9, -1, -2)                                                                                                                                             {}\n",
      "call_function  view_5                                                            aten.view.default                                                 (transpose_7, [1, 512, 64, 64])                                                                                                                                 {}\n",
      "call_function  add_9                                                             aten.add.Tensor                                                   (view_5, div_8)                                                                                                                                                 {}\n",
      "call_function  div_9                                                             aten.div.Tensor                                                   (add_9, 1)                                                                                                                                                      {}\n",
      "call_function  group_norm_19                                                     aten.group_norm.default                                           (div_9, 32, p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_weight, p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_bias, 1e-06)            {}\n",
      "call_function  silu_18                                                           aten.silu.default                                                 (group_norm_19,)                                                                                                                                                {}\n",
      "call_function  conv2d_24                                                         aten.conv2d.default                                               (silu_18, p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_weight, p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_bias, [1, 1], [1, 1])     {}\n",
      "call_function  group_norm_20                                                     aten.group_norm.default                                           (conv2d_24, 32, p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_weight, p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_bias, 1e-06)        {}\n",
      "call_function  silu_19                                                           aten.silu.default                                                 (group_norm_20,)                                                                                                                                                {}\n",
      "call_function  dropout_10                                                        aten.dropout.default                                              (silu_19, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_25                                                         aten.conv2d.default                                               (dropout_10, p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_weight, p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_bias, [1, 1], [1, 1])  {}\n",
      "call_function  add_10                                                            aten.add.Tensor                                                   (div_9, conv2d_25)                                                                                                                                              {}\n",
      "call_function  div_10                                                            aten.div.Tensor                                                   (add_10, 1)                                                                                                                                                     {}\n",
      "call_function  group_norm_21                                                     aten.group_norm.default                                           (div_10, 32, p_encoder_conv_norm_out_weight, p_encoder_conv_norm_out_bias, 1e-06)                                                                               {}\n",
      "call_function  silu_20                                                           aten.silu.default                                                 (group_norm_21,)                                                                                                                                                {}\n",
      "call_function  conv2d_26                                                         aten.conv2d.default                                               (silu_20, p_encoder_conv_out_weight, p_encoder_conv_out_bias, [1, 1], [1, 1])                                                                                   {}\n",
      "call_function  conv2d_27                                                         aten.conv2d.default                                               (conv2d_26, p_quant_conv_weight, p_quant_conv_bias)                                                                                                             {}\n",
      "call_function  split                                                             aten.split.Tensor                                                 (conv2d_27, 4, 1)                                                                                                                                               {}\n",
      "call_function  getitem                                                           <built-in function getitem>                                       (split, 0)                                                                                                                                                      {}\n",
      "call_function  conv2d_28                                                         aten.conv2d.default                                               (getitem, p_post_quant_conv_weight, p_post_quant_conv_bias)                                                                                                     {}\n",
      "call_function  conv2d_29                                                         aten.conv2d.default                                               (conv2d_28, p_decoder_conv_in_weight, p_decoder_conv_in_bias, [1, 1], [1, 1])                                                                                   {}\n",
      "call_function  group_norm_22                                                     aten.group_norm.default                                           (conv2d_29, 32, p_decoder_mid_block_resnets_0_norm1_weight, p_decoder_mid_block_resnets_0_norm1_bias, 1e-06)                                                    {}\n",
      "call_function  silu_21                                                           aten.silu.default                                                 (group_norm_22,)                                                                                                                                                {}\n",
      "call_function  conv2d_30                                                         aten.conv2d.default                                               (silu_21, p_decoder_mid_block_resnets_0_conv1_weight, p_decoder_mid_block_resnets_0_conv1_bias, [1, 1], [1, 1])                                                 {}\n",
      "call_function  group_norm_23                                                     aten.group_norm.default                                           (conv2d_30, 32, p_decoder_mid_block_resnets_0_norm2_weight, p_decoder_mid_block_resnets_0_norm2_bias, 1e-06)                                                    {}\n",
      "call_function  silu_22                                                           aten.silu.default                                                 (group_norm_23,)                                                                                                                                                {}\n",
      "call_function  dropout_11                                                        aten.dropout.default                                              (silu_22, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_31                                                         aten.conv2d.default                                               (dropout_11, p_decoder_mid_block_resnets_0_conv2_weight, p_decoder_mid_block_resnets_0_conv2_bias, [1, 1], [1, 1])                                              {}\n",
      "call_function  add_11                                                            aten.add.Tensor                                                   (conv2d_29, conv2d_31)                                                                                                                                          {}\n",
      "call_function  div_11                                                            aten.div.Tensor                                                   (add_11, 1)                                                                                                                                                     {}\n",
      "call_function  view_6                                                            aten.view.default                                                 (div_11, [1, 512, 4096])                                                                                                                                        {}\n",
      "call_function  transpose_8                                                       aten.transpose.int                                                (view_6, 1, 2)                                                                                                                                                  {}\n",
      "call_function  transpose_9                                                       aten.transpose.int                                                (transpose_8, 1, 2)                                                                                                                                             {}\n",
      "call_function  group_norm_24                                                     aten.group_norm.default                                           (transpose_9, 32, p_decoder_mid_block_attentions_0_group_norm_weight, p_decoder_mid_block_attentions_0_group_norm_bias, 1e-06)                                  {}\n",
      "call_function  transpose_10                                                      aten.transpose.int                                                (group_norm_24, 1, 2)                                                                                                                                           {}\n",
      "call_function  linear_4                                                          aten.linear.default                                               (transpose_10, p_decoder_mid_block_attentions_0_to_q_weight, p_decoder_mid_block_attentions_0_to_q_bias)                                                        {}\n",
      "call_function  linear_5                                                          aten.linear.default                                               (transpose_10, p_decoder_mid_block_attentions_0_to_k_weight, p_decoder_mid_block_attentions_0_to_k_bias)                                                        {}\n",
      "call_function  linear_6                                                          aten.linear.default                                               (transpose_10, p_decoder_mid_block_attentions_0_to_v_weight, p_decoder_mid_block_attentions_0_to_v_bias)                                                        {}\n",
      "call_function  view_7                                                            aten.view.default                                                 (linear_4, [1, -1, 1, 512])                                                                                                                                     {}\n",
      "call_function  transpose_11                                                      aten.transpose.int                                                (view_7, 1, 2)                                                                                                                                                  {}\n",
      "call_function  view_8                                                            aten.view.default                                                 (linear_5, [1, -1, 1, 512])                                                                                                                                     {}\n",
      "call_function  transpose_12                                                      aten.transpose.int                                                (view_8, 1, 2)                                                                                                                                                  {}\n",
      "call_function  view_9                                                            aten.view.default                                                 (linear_6, [1, -1, 1, 512])                                                                                                                                     {}\n",
      "call_function  transpose_13                                                      aten.transpose.int                                                (view_9, 1, 2)                                                                                                                                                  {}\n",
      "call_function  scaled_dot_product_attention_1                                    aten.scaled_dot_product_attention.default                         (transpose_11, transpose_12, transpose_13)                                                                                                                      {}\n",
      "call_function  transpose_14                                                      aten.transpose.int                                                (scaled_dot_product_attention_1, 1, 2)                                                                                                                          {}\n",
      "call_function  view_10                                                           aten.view.default                                                 (transpose_14, [1, -1, 512])                                                                                                                                    {}\n",
      "call_function  _to_copy_1                                                        aten._to_copy.default                                             (view_10,)                                                                                                                                                      {'dtype': torch.float32}\n",
      "call_function  linear_7                                                          aten.linear.default                                               (_to_copy_1, p_decoder_mid_block_attentions_0_to_out_0_weight, p_decoder_mid_block_attentions_0_to_out_0_bias)                                                  {}\n",
      "call_function  dropout_12                                                        aten.dropout.default                                              (linear_7, 0.0, False)                                                                                                                                          {}\n",
      "call_function  transpose_15                                                      aten.transpose.int                                                (dropout_12, -1, -2)                                                                                                                                            {}\n",
      "call_function  view_11                                                           aten.view.default                                                 (transpose_15, [1, 512, 64, 64])                                                                                                                                {}\n",
      "call_function  add_12                                                            aten.add.Tensor                                                   (view_11, div_11)                                                                                                                                               {}\n",
      "call_function  div_12                                                            aten.div.Tensor                                                   (add_12, 1)                                                                                                                                                     {}\n",
      "call_function  group_norm_25                                                     aten.group_norm.default                                           (div_12, 32, p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_weight, p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_bias, 1e-06)           {}\n",
      "call_function  silu_23                                                           aten.silu.default                                                 (group_norm_25,)                                                                                                                                                {}\n",
      "call_function  conv2d_32                                                         aten.conv2d.default                                               (silu_23, p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_weight, p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_bias, [1, 1], [1, 1])     {}\n",
      "call_function  group_norm_26                                                     aten.group_norm.default                                           (conv2d_32, 32, p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_weight, p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_bias, 1e-06)        {}\n",
      "call_function  silu_24                                                           aten.silu.default                                                 (group_norm_26,)                                                                                                                                                {}\n",
      "call_function  dropout_13                                                        aten.dropout.default                                              (silu_24, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_33                                                         aten.conv2d.default                                               (dropout_13, p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_weight, p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_bias, [1, 1], [1, 1])  {}\n",
      "call_function  add_13                                                            aten.add.Tensor                                                   (div_12, conv2d_33)                                                                                                                                             {}\n",
      "call_function  div_13                                                            aten.div.Tensor                                                   (add_13, 1)                                                                                                                                                     {}\n",
      "call_function  _to_copy_2                                                        aten._to_copy.default                                             (div_13,)                                                                                                                                                       {'dtype': torch.float32}\n",
      "call_function  group_norm_27                                                     aten.group_norm.default                                           (_to_copy_2, 32, p_decoder_up_blocks_0_resnets_0_norm1_weight, p_decoder_up_blocks_0_resnets_0_norm1_bias, 1e-06)                                               {}\n",
      "call_function  silu_25                                                           aten.silu.default                                                 (group_norm_27,)                                                                                                                                                {}\n",
      "call_function  conv2d_34                                                         aten.conv2d.default                                               (silu_25, p_decoder_up_blocks_0_resnets_0_conv1_weight, p_decoder_up_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_28                                                     aten.group_norm.default                                           (conv2d_34, 32, p_decoder_up_blocks_0_resnets_0_norm2_weight, p_decoder_up_blocks_0_resnets_0_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_26                                                           aten.silu.default                                                 (group_norm_28,)                                                                                                                                                {}\n",
      "call_function  dropout_14                                                        aten.dropout.default                                              (silu_26, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_35                                                         aten.conv2d.default                                               (dropout_14, p_decoder_up_blocks_0_resnets_0_conv2_weight, p_decoder_up_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_14                                                            aten.add.Tensor                                                   (_to_copy_2, conv2d_35)                                                                                                                                         {}\n",
      "call_function  div_14                                                            aten.div.Tensor                                                   (add_14, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_29                                                     aten.group_norm.default                                           (div_14, 32, p_decoder_up_blocks_0_resnets_1_norm1_weight, p_decoder_up_blocks_0_resnets_1_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  silu_27                                                           aten.silu.default                                                 (group_norm_29,)                                                                                                                                                {}\n",
      "call_function  conv2d_36                                                         aten.conv2d.default                                               (silu_27, p_decoder_up_blocks_0_resnets_1_conv1_weight, p_decoder_up_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_30                                                     aten.group_norm.default                                           (conv2d_36, 32, p_decoder_up_blocks_0_resnets_1_norm2_weight, p_decoder_up_blocks_0_resnets_1_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_28                                                           aten.silu.default                                                 (group_norm_30,)                                                                                                                                                {}\n",
      "call_function  dropout_15                                                        aten.dropout.default                                              (silu_28, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_37                                                         aten.conv2d.default                                               (dropout_15, p_decoder_up_blocks_0_resnets_1_conv2_weight, p_decoder_up_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_15                                                            aten.add.Tensor                                                   (div_14, conv2d_37)                                                                                                                                             {}\n",
      "call_function  div_15                                                            aten.div.Tensor                                                   (add_15, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_31                                                     aten.group_norm.default                                           (div_15, 32, p_decoder_up_blocks_0_resnets_2_norm1_weight, p_decoder_up_blocks_0_resnets_2_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  silu_29                                                           aten.silu.default                                                 (group_norm_31,)                                                                                                                                                {}\n",
      "call_function  conv2d_38                                                         aten.conv2d.default                                               (silu_29, p_decoder_up_blocks_0_resnets_2_conv1_weight, p_decoder_up_blocks_0_resnets_2_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_32                                                     aten.group_norm.default                                           (conv2d_38, 32, p_decoder_up_blocks_0_resnets_2_norm2_weight, p_decoder_up_blocks_0_resnets_2_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_30                                                           aten.silu.default                                                 (group_norm_32,)                                                                                                                                                {}\n",
      "call_function  dropout_16                                                        aten.dropout.default                                              (silu_30, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_39                                                         aten.conv2d.default                                               (dropout_16, p_decoder_up_blocks_0_resnets_2_conv2_weight, p_decoder_up_blocks_0_resnets_2_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_16                                                            aten.add.Tensor                                                   (div_15, conv2d_39)                                                                                                                                             {}\n",
      "call_function  div_16                                                            aten.div.Tensor                                                   (add_16, 1.0)                                                                                                                                                   {}\n",
      "call_function  upsample_nearest2d                                                aten.upsample_nearest2d.vec                                       (div_16, None, [2.0, 2.0])                                                                                                                                      {}\n",
      "call_function  conv2d_40                                                         aten.conv2d.default                                               (upsample_nearest2d, p_decoder_up_blocks_0_upsamplers_0_conv_weight, p_decoder_up_blocks_0_upsamplers_0_conv_bias, [1, 1], [1, 1])                              {}\n",
      "call_function  group_norm_33                                                     aten.group_norm.default                                           (conv2d_40, 32, p_decoder_up_blocks_1_resnets_0_norm1_weight, p_decoder_up_blocks_1_resnets_0_norm1_bias, 1e-06)                                                {}\n",
      "call_function  silu_31                                                           aten.silu.default                                                 (group_norm_33,)                                                                                                                                                {}\n",
      "call_function  conv2d_41                                                         aten.conv2d.default                                               (silu_31, p_decoder_up_blocks_1_resnets_0_conv1_weight, p_decoder_up_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_34                                                     aten.group_norm.default                                           (conv2d_41, 32, p_decoder_up_blocks_1_resnets_0_norm2_weight, p_decoder_up_blocks_1_resnets_0_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_32                                                           aten.silu.default                                                 (group_norm_34,)                                                                                                                                                {}\n",
      "call_function  dropout_17                                                        aten.dropout.default                                              (silu_32, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_42                                                         aten.conv2d.default                                               (dropout_17, p_decoder_up_blocks_1_resnets_0_conv2_weight, p_decoder_up_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_17                                                            aten.add.Tensor                                                   (conv2d_40, conv2d_42)                                                                                                                                          {}\n",
      "call_function  div_17                                                            aten.div.Tensor                                                   (add_17, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_35                                                     aten.group_norm.default                                           (div_17, 32, p_decoder_up_blocks_1_resnets_1_norm1_weight, p_decoder_up_blocks_1_resnets_1_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  silu_33                                                           aten.silu.default                                                 (group_norm_35,)                                                                                                                                                {}\n",
      "call_function  conv2d_43                                                         aten.conv2d.default                                               (silu_33, p_decoder_up_blocks_1_resnets_1_conv1_weight, p_decoder_up_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_36                                                     aten.group_norm.default                                           (conv2d_43, 32, p_decoder_up_blocks_1_resnets_1_norm2_weight, p_decoder_up_blocks_1_resnets_1_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_34                                                           aten.silu.default                                                 (group_norm_36,)                                                                                                                                                {}\n",
      "call_function  dropout_18                                                        aten.dropout.default                                              (silu_34, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_44                                                         aten.conv2d.default                                               (dropout_18, p_decoder_up_blocks_1_resnets_1_conv2_weight, p_decoder_up_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_18                                                            aten.add.Tensor                                                   (div_17, conv2d_44)                                                                                                                                             {}\n",
      "call_function  div_18                                                            aten.div.Tensor                                                   (add_18, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_37                                                     aten.group_norm.default                                           (div_18, 32, p_decoder_up_blocks_1_resnets_2_norm1_weight, p_decoder_up_blocks_1_resnets_2_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  silu_35                                                           aten.silu.default                                                 (group_norm_37,)                                                                                                                                                {}\n",
      "call_function  conv2d_45                                                         aten.conv2d.default                                               (silu_35, p_decoder_up_blocks_1_resnets_2_conv1_weight, p_decoder_up_blocks_1_resnets_2_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_38                                                     aten.group_norm.default                                           (conv2d_45, 32, p_decoder_up_blocks_1_resnets_2_norm2_weight, p_decoder_up_blocks_1_resnets_2_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_36                                                           aten.silu.default                                                 (group_norm_38,)                                                                                                                                                {}\n",
      "call_function  dropout_19                                                        aten.dropout.default                                              (silu_36, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_46                                                         aten.conv2d.default                                               (dropout_19, p_decoder_up_blocks_1_resnets_2_conv2_weight, p_decoder_up_blocks_1_resnets_2_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_19                                                            aten.add.Tensor                                                   (div_18, conv2d_46)                                                                                                                                             {}\n",
      "call_function  div_19                                                            aten.div.Tensor                                                   (add_19, 1.0)                                                                                                                                                   {}\n",
      "call_function  upsample_nearest2d_1                                              aten.upsample_nearest2d.vec                                       (div_19, None, [2.0, 2.0])                                                                                                                                      {}\n",
      "call_function  conv2d_47                                                         aten.conv2d.default                                               (upsample_nearest2d_1, p_decoder_up_blocks_1_upsamplers_0_conv_weight, p_decoder_up_blocks_1_upsamplers_0_conv_bias, [1, 1], [1, 1])                            {}\n",
      "call_function  group_norm_39                                                     aten.group_norm.default                                           (conv2d_47, 32, p_decoder_up_blocks_2_resnets_0_norm1_weight, p_decoder_up_blocks_2_resnets_0_norm1_bias, 1e-06)                                                {}\n",
      "call_function  silu_37                                                           aten.silu.default                                                 (group_norm_39,)                                                                                                                                                {}\n",
      "call_function  conv2d_48                                                         aten.conv2d.default                                               (silu_37, p_decoder_up_blocks_2_resnets_0_conv1_weight, p_decoder_up_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_40                                                     aten.group_norm.default                                           (conv2d_48, 32, p_decoder_up_blocks_2_resnets_0_norm2_weight, p_decoder_up_blocks_2_resnets_0_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_38                                                           aten.silu.default                                                 (group_norm_40,)                                                                                                                                                {}\n",
      "call_function  dropout_20                                                        aten.dropout.default                                              (silu_38, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_49                                                         aten.conv2d.default                                               (dropout_20, p_decoder_up_blocks_2_resnets_0_conv2_weight, p_decoder_up_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  clone                                                             aten.clone.default                                                (conv2d_47,)                                                                                                                                                    {'memory_format': torch.contiguous_format}\n",
      "call_function  conv2d_50                                                         aten.conv2d.default                                               (clone, p_decoder_up_blocks_2_resnets_0_conv_shortcut_weight, p_decoder_up_blocks_2_resnets_0_conv_shortcut_bias)                                               {}\n",
      "call_function  add_20                                                            aten.add.Tensor                                                   (conv2d_50, conv2d_49)                                                                                                                                          {}\n",
      "call_function  div_20                                                            aten.div.Tensor                                                   (add_20, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_41                                                     aten.group_norm.default                                           (div_20, 32, p_decoder_up_blocks_2_resnets_1_norm1_weight, p_decoder_up_blocks_2_resnets_1_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  silu_39                                                           aten.silu.default                                                 (group_norm_41,)                                                                                                                                                {}\n",
      "call_function  conv2d_51                                                         aten.conv2d.default                                               (silu_39, p_decoder_up_blocks_2_resnets_1_conv1_weight, p_decoder_up_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_42                                                     aten.group_norm.default                                           (conv2d_51, 32, p_decoder_up_blocks_2_resnets_1_norm2_weight, p_decoder_up_blocks_2_resnets_1_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_40                                                           aten.silu.default                                                 (group_norm_42,)                                                                                                                                                {}\n",
      "call_function  dropout_21                                                        aten.dropout.default                                              (silu_40, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_52                                                         aten.conv2d.default                                               (dropout_21, p_decoder_up_blocks_2_resnets_1_conv2_weight, p_decoder_up_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_21                                                            aten.add.Tensor                                                   (div_20, conv2d_52)                                                                                                                                             {}\n",
      "call_function  div_21                                                            aten.div.Tensor                                                   (add_21, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_43                                                     aten.group_norm.default                                           (div_21, 32, p_decoder_up_blocks_2_resnets_2_norm1_weight, p_decoder_up_blocks_2_resnets_2_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  silu_41                                                           aten.silu.default                                                 (group_norm_43,)                                                                                                                                                {}\n",
      "call_function  conv2d_53                                                         aten.conv2d.default                                               (silu_41, p_decoder_up_blocks_2_resnets_2_conv1_weight, p_decoder_up_blocks_2_resnets_2_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_44                                                     aten.group_norm.default                                           (conv2d_53, 32, p_decoder_up_blocks_2_resnets_2_norm2_weight, p_decoder_up_blocks_2_resnets_2_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_42                                                           aten.silu.default                                                 (group_norm_44,)                                                                                                                                                {}\n",
      "call_function  dropout_22                                                        aten.dropout.default                                              (silu_42, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_54                                                         aten.conv2d.default                                               (dropout_22, p_decoder_up_blocks_2_resnets_2_conv2_weight, p_decoder_up_blocks_2_resnets_2_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_22                                                            aten.add.Tensor                                                   (div_21, conv2d_54)                                                                                                                                             {}\n",
      "call_function  div_22                                                            aten.div.Tensor                                                   (add_22, 1.0)                                                                                                                                                   {}\n",
      "call_function  upsample_nearest2d_2                                              aten.upsample_nearest2d.vec                                       (div_22, None, [2.0, 2.0])                                                                                                                                      {}\n",
      "call_function  conv2d_55                                                         aten.conv2d.default                                               (upsample_nearest2d_2, p_decoder_up_blocks_2_upsamplers_0_conv_weight, p_decoder_up_blocks_2_upsamplers_0_conv_bias, [1, 1], [1, 1])                            {}\n",
      "call_function  group_norm_45                                                     aten.group_norm.default                                           (conv2d_55, 32, p_decoder_up_blocks_3_resnets_0_norm1_weight, p_decoder_up_blocks_3_resnets_0_norm1_bias, 1e-06)                                                {}\n",
      "call_function  silu_43                                                           aten.silu.default                                                 (group_norm_45,)                                                                                                                                                {}\n",
      "call_function  conv2d_56                                                         aten.conv2d.default                                               (silu_43, p_decoder_up_blocks_3_resnets_0_conv1_weight, p_decoder_up_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_46                                                     aten.group_norm.default                                           (conv2d_56, 32, p_decoder_up_blocks_3_resnets_0_norm2_weight, p_decoder_up_blocks_3_resnets_0_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_44                                                           aten.silu.default                                                 (group_norm_46,)                                                                                                                                                {}\n",
      "call_function  dropout_23                                                        aten.dropout.default                                              (silu_44, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_57                                                         aten.conv2d.default                                               (dropout_23, p_decoder_up_blocks_3_resnets_0_conv2_weight, p_decoder_up_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  conv2d_58                                                         aten.conv2d.default                                               (conv2d_55, p_decoder_up_blocks_3_resnets_0_conv_shortcut_weight, p_decoder_up_blocks_3_resnets_0_conv_shortcut_bias)                                           {}\n",
      "call_function  add_23                                                            aten.add.Tensor                                                   (conv2d_58, conv2d_57)                                                                                                                                          {}\n",
      "call_function  div_23                                                            aten.div.Tensor                                                   (add_23, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_47                                                     aten.group_norm.default                                           (div_23, 32, p_decoder_up_blocks_3_resnets_1_norm1_weight, p_decoder_up_blocks_3_resnets_1_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  silu_45                                                           aten.silu.default                                                 (group_norm_47,)                                                                                                                                                {}\n",
      "call_function  conv2d_59                                                         aten.conv2d.default                                               (silu_45, p_decoder_up_blocks_3_resnets_1_conv1_weight, p_decoder_up_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_48                                                     aten.group_norm.default                                           (conv2d_59, 32, p_decoder_up_blocks_3_resnets_1_norm2_weight, p_decoder_up_blocks_3_resnets_1_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_46                                                           aten.silu.default                                                 (group_norm_48,)                                                                                                                                                {}\n",
      "call_function  dropout_24                                                        aten.dropout.default                                              (silu_46, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_60                                                         aten.conv2d.default                                               (dropout_24, p_decoder_up_blocks_3_resnets_1_conv2_weight, p_decoder_up_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_24                                                            aten.add.Tensor                                                   (div_23, conv2d_60)                                                                                                                                             {}\n",
      "call_function  div_24                                                            aten.div.Tensor                                                   (add_24, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_49                                                     aten.group_norm.default                                           (div_24, 32, p_decoder_up_blocks_3_resnets_2_norm1_weight, p_decoder_up_blocks_3_resnets_2_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  silu_47                                                           aten.silu.default                                                 (group_norm_49,)                                                                                                                                                {}\n",
      "call_function  conv2d_61                                                         aten.conv2d.default                                               (silu_47, p_decoder_up_blocks_3_resnets_2_conv1_weight, p_decoder_up_blocks_3_resnets_2_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_50                                                     aten.group_norm.default                                           (conv2d_61, 32, p_decoder_up_blocks_3_resnets_2_norm2_weight, p_decoder_up_blocks_3_resnets_2_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_48                                                           aten.silu.default                                                 (group_norm_50,)                                                                                                                                                {}\n",
      "call_function  dropout_25                                                        aten.dropout.default                                              (silu_48, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_62                                                         aten.conv2d.default                                               (dropout_25, p_decoder_up_blocks_3_resnets_2_conv2_weight, p_decoder_up_blocks_3_resnets_2_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_25                                                            aten.add.Tensor                                                   (div_24, conv2d_62)                                                                                                                                             {}\n",
      "call_function  div_25                                                            aten.div.Tensor                                                   (add_25, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_51                                                     aten.group_norm.default                                           (div_25, 32, p_decoder_conv_norm_out_weight, p_decoder_conv_norm_out_bias, 1e-06)                                                                               {}\n",
      "call_function  silu_49                                                           aten.silu.default                                                 (group_norm_51,)                                                                                                                                                {}\n",
      "call_function  conv2d_63                                                         aten.conv2d.default                                               (silu_49, p_decoder_conv_out_weight, p_decoder_conv_out_bias, [1, 1], [1, 1])                                                                                   {}\n",
      "output         output                                                            output                                                            ((conv2d_63,),)                                                                                                                                                 {}\n",
      "graph():\n",
      "    %p_encoder_conv_in_weight : [num_users=1] = placeholder[target=p_encoder_conv_in_weight]\n",
      "    %p_encoder_conv_in_bias : [num_users=1] = placeholder[target=p_encoder_conv_in_bias]\n",
      "    %p_encoder_down_blocks_0_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_0_norm1_weight]\n",
      "    %p_encoder_down_blocks_0_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_0_norm1_bias]\n",
      "    %p_encoder_down_blocks_0_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_0_conv1_weight]\n",
      "    %p_encoder_down_blocks_0_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_0_conv1_bias]\n",
      "    %p_encoder_down_blocks_0_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_0_norm2_weight]\n",
      "    %p_encoder_down_blocks_0_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_0_norm2_bias]\n",
      "    %p_encoder_down_blocks_0_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_0_conv2_weight]\n",
      "    %p_encoder_down_blocks_0_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_0_conv2_bias]\n",
      "    %p_encoder_down_blocks_0_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_1_norm1_weight]\n",
      "    %p_encoder_down_blocks_0_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_1_norm1_bias]\n",
      "    %p_encoder_down_blocks_0_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_1_conv1_weight]\n",
      "    %p_encoder_down_blocks_0_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_1_conv1_bias]\n",
      "    %p_encoder_down_blocks_0_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_1_norm2_weight]\n",
      "    %p_encoder_down_blocks_0_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_1_norm2_bias]\n",
      "    %p_encoder_down_blocks_0_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_1_conv2_weight]\n",
      "    %p_encoder_down_blocks_0_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_1_conv2_bias]\n",
      "    %p_encoder_down_blocks_0_downsamplers_0_conv_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_downsamplers_0_conv_weight]\n",
      "    %p_encoder_down_blocks_0_downsamplers_0_conv_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_downsamplers_0_conv_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_norm1_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_norm1_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_conv1_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_conv1_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_norm2_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_norm2_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_conv2_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_conv2_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_0_conv_shortcut_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_conv_shortcut_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_0_conv_shortcut_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_conv_shortcut_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_1_norm1_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_1_norm1_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_1_conv1_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_1_conv1_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_1_norm2_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_1_norm2_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_1_conv2_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_1_conv2_bias]\n",
      "    %p_encoder_down_blocks_1_downsamplers_0_conv_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_downsamplers_0_conv_weight]\n",
      "    %p_encoder_down_blocks_1_downsamplers_0_conv_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_downsamplers_0_conv_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_norm1_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_norm1_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_conv1_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_conv1_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_norm2_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_norm2_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_conv2_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_conv2_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_0_conv_shortcut_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_conv_shortcut_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_0_conv_shortcut_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_conv_shortcut_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_1_norm1_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_1_norm1_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_1_conv1_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_1_conv1_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_1_norm2_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_1_norm2_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_1_conv2_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_1_conv2_bias]\n",
      "    %p_encoder_down_blocks_2_downsamplers_0_conv_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_downsamplers_0_conv_weight]\n",
      "    %p_encoder_down_blocks_2_downsamplers_0_conv_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_downsamplers_0_conv_bias]\n",
      "    %p_encoder_down_blocks_3_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_0_norm1_weight]\n",
      "    %p_encoder_down_blocks_3_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_0_norm1_bias]\n",
      "    %p_encoder_down_blocks_3_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_0_conv1_weight]\n",
      "    %p_encoder_down_blocks_3_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_0_conv1_bias]\n",
      "    %p_encoder_down_blocks_3_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_0_norm2_weight]\n",
      "    %p_encoder_down_blocks_3_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_0_norm2_bias]\n",
      "    %p_encoder_down_blocks_3_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_0_conv2_weight]\n",
      "    %p_encoder_down_blocks_3_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_0_conv2_bias]\n",
      "    %p_encoder_down_blocks_3_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_1_norm1_weight]\n",
      "    %p_encoder_down_blocks_3_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_1_norm1_bias]\n",
      "    %p_encoder_down_blocks_3_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_1_conv1_weight]\n",
      "    %p_encoder_down_blocks_3_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_1_conv1_bias]\n",
      "    %p_encoder_down_blocks_3_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_1_norm2_weight]\n",
      "    %p_encoder_down_blocks_3_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_1_norm2_bias]\n",
      "    %p_encoder_down_blocks_3_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_1_conv2_weight]\n",
      "    %p_encoder_down_blocks_3_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_1_conv2_bias]\n",
      "    %p_encoder_mid_block_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_0_norm1_weight]\n",
      "    %p_encoder_mid_block_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_0_norm1_bias]\n",
      "    %p_encoder_mid_block_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_0_conv1_weight]\n",
      "    %p_encoder_mid_block_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_0_conv1_bias]\n",
      "    %p_encoder_mid_block_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_0_norm2_weight]\n",
      "    %p_encoder_mid_block_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_0_norm2_bias]\n",
      "    %p_encoder_mid_block_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_0_conv2_weight]\n",
      "    %p_encoder_mid_block_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_0_conv2_bias]\n",
      "    %p_encoder_mid_block_attentions_0_group_norm_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_group_norm_weight]\n",
      "    %p_encoder_mid_block_attentions_0_group_norm_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_group_norm_bias]\n",
      "    %p_encoder_mid_block_attentions_0_to_q_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_to_q_weight]\n",
      "    %p_encoder_mid_block_attentions_0_to_q_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_to_q_bias]\n",
      "    %p_encoder_mid_block_attentions_0_to_k_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_to_k_weight]\n",
      "    %p_encoder_mid_block_attentions_0_to_k_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_to_k_bias]\n",
      "    %p_encoder_mid_block_attentions_0_to_v_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_to_v_weight]\n",
      "    %p_encoder_mid_block_attentions_0_to_v_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_to_v_bias]\n",
      "    %p_encoder_mid_block_attentions_0_to_out_0_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_to_out_0_weight]\n",
      "    %p_encoder_mid_block_attentions_0_to_out_0_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_to_out_0_bias]\n",
      "    %p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_weight]\n",
      "    %p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_bias]\n",
      "    %p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_weight]\n",
      "    %p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_bias]\n",
      "    %p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_weight]\n",
      "    %p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_bias]\n",
      "    %p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_weight]\n",
      "    %p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_bias]\n",
      "    %p_encoder_conv_norm_out_weight : [num_users=1] = placeholder[target=p_encoder_conv_norm_out_weight]\n",
      "    %p_encoder_conv_norm_out_bias : [num_users=1] = placeholder[target=p_encoder_conv_norm_out_bias]\n",
      "    %p_encoder_conv_out_weight : [num_users=1] = placeholder[target=p_encoder_conv_out_weight]\n",
      "    %p_encoder_conv_out_bias : [num_users=1] = placeholder[target=p_encoder_conv_out_bias]\n",
      "    %p_quant_conv_weight : [num_users=1] = placeholder[target=p_quant_conv_weight]\n",
      "    %p_quant_conv_bias : [num_users=1] = placeholder[target=p_quant_conv_bias]\n",
      "    %p_post_quant_conv_weight : [num_users=1] = placeholder[target=p_post_quant_conv_weight]\n",
      "    %p_post_quant_conv_bias : [num_users=1] = placeholder[target=p_post_quant_conv_bias]\n",
      "    %p_decoder_conv_in_weight : [num_users=1] = placeholder[target=p_decoder_conv_in_weight]\n",
      "    %p_decoder_conv_in_bias : [num_users=1] = placeholder[target=p_decoder_conv_in_bias]\n",
      "    %p_decoder_mid_block_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_0_norm1_weight]\n",
      "    %p_decoder_mid_block_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_0_norm1_bias]\n",
      "    %p_decoder_mid_block_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_0_conv1_weight]\n",
      "    %p_decoder_mid_block_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_0_conv1_bias]\n",
      "    %p_decoder_mid_block_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_0_norm2_weight]\n",
      "    %p_decoder_mid_block_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_0_norm2_bias]\n",
      "    %p_decoder_mid_block_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_0_conv2_weight]\n",
      "    %p_decoder_mid_block_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_0_conv2_bias]\n",
      "    %p_decoder_mid_block_attentions_0_group_norm_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_group_norm_weight]\n",
      "    %p_decoder_mid_block_attentions_0_group_norm_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_group_norm_bias]\n",
      "    %p_decoder_mid_block_attentions_0_to_q_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_to_q_weight]\n",
      "    %p_decoder_mid_block_attentions_0_to_q_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_to_q_bias]\n",
      "    %p_decoder_mid_block_attentions_0_to_k_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_to_k_weight]\n",
      "    %p_decoder_mid_block_attentions_0_to_k_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_to_k_bias]\n",
      "    %p_decoder_mid_block_attentions_0_to_v_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_to_v_weight]\n",
      "    %p_decoder_mid_block_attentions_0_to_v_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_to_v_bias]\n",
      "    %p_decoder_mid_block_attentions_0_to_out_0_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_to_out_0_weight]\n",
      "    %p_decoder_mid_block_attentions_0_to_out_0_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_to_out_0_bias]\n",
      "    %p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_weight]\n",
      "    %p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_bias]\n",
      "    %p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_weight]\n",
      "    %p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_bias]\n",
      "    %p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_weight]\n",
      "    %p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_bias]\n",
      "    %p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_weight]\n",
      "    %p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_0_norm1_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_0_norm1_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_0_conv1_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_0_conv1_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_0_norm2_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_0_norm2_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_0_conv2_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_0_conv2_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_1_norm1_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_1_norm1_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_1_conv1_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_1_conv1_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_1_norm2_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_1_norm2_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_1_conv2_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_1_conv2_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_2_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_2_norm1_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_2_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_2_norm1_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_2_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_2_conv1_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_2_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_2_conv1_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_2_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_2_norm2_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_2_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_2_norm2_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_2_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_2_conv2_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_2_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_2_conv2_bias]\n",
      "    %p_decoder_up_blocks_0_upsamplers_0_conv_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_upsamplers_0_conv_weight]\n",
      "    %p_decoder_up_blocks_0_upsamplers_0_conv_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_upsamplers_0_conv_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_0_norm1_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_0_norm1_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_0_conv1_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_0_conv1_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_0_norm2_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_0_norm2_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_0_conv2_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_0_conv2_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_1_norm1_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_1_norm1_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_1_conv1_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_1_conv1_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_1_norm2_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_1_norm2_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_1_conv2_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_1_conv2_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_2_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_2_norm1_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_2_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_2_norm1_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_2_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_2_conv1_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_2_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_2_conv1_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_2_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_2_norm2_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_2_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_2_norm2_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_2_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_2_conv2_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_2_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_2_conv2_bias]\n",
      "    %p_decoder_up_blocks_1_upsamplers_0_conv_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_upsamplers_0_conv_weight]\n",
      "    %p_decoder_up_blocks_1_upsamplers_0_conv_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_upsamplers_0_conv_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_norm1_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_norm1_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_conv1_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_conv1_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_norm2_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_norm2_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_conv2_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_conv2_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_0_conv_shortcut_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_conv_shortcut_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_0_conv_shortcut_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_conv_shortcut_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_1_norm1_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_1_norm1_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_1_conv1_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_1_conv1_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_1_norm2_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_1_norm2_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_1_conv2_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_1_conv2_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_2_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_2_norm1_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_2_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_2_norm1_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_2_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_2_conv1_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_2_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_2_conv1_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_2_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_2_norm2_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_2_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_2_norm2_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_2_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_2_conv2_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_2_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_2_conv2_bias]\n",
      "    %p_decoder_up_blocks_2_upsamplers_0_conv_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_upsamplers_0_conv_weight]\n",
      "    %p_decoder_up_blocks_2_upsamplers_0_conv_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_upsamplers_0_conv_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_norm1_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_norm1_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_conv1_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_conv1_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_norm2_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_norm2_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_conv2_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_conv2_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_0_conv_shortcut_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_conv_shortcut_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_0_conv_shortcut_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_conv_shortcut_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_1_norm1_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_1_norm1_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_1_conv1_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_1_conv1_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_1_norm2_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_1_norm2_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_1_conv2_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_1_conv2_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_2_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_2_norm1_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_2_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_2_norm1_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_2_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_2_conv1_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_2_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_2_conv1_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_2_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_2_norm2_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_2_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_2_norm2_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_2_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_2_conv2_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_2_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_2_conv2_bias]\n",
      "    %p_decoder_conv_norm_out_weight : [num_users=1] = placeholder[target=p_decoder_conv_norm_out_weight]\n",
      "    %p_decoder_conv_norm_out_bias : [num_users=1] = placeholder[target=p_decoder_conv_norm_out_bias]\n",
      "    %p_decoder_conv_out_weight : [num_users=1] = placeholder[target=p_decoder_conv_out_weight]\n",
      "    %p_decoder_conv_out_bias : [num_users=1] = placeholder[target=p_decoder_conv_out_bias]\n",
      "    %sample : [num_users=1] = placeholder[target=sample]\n",
      "    %conv2d : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%sample, %p_encoder_conv_in_weight, %p_encoder_conv_in_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d, 32, %p_encoder_down_blocks_0_resnets_0_norm1_weight, %p_encoder_down_blocks_0_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm,), kwargs = {})\n",
      "    %conv2d_1 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu, %p_encoder_down_blocks_0_resnets_0_conv1_weight, %p_encoder_down_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_1 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_1, 32, %p_encoder_down_blocks_0_resnets_0_norm2_weight, %p_encoder_down_blocks_0_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_1 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_1,), kwargs = {})\n",
      "    %dropout : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_1, 0.0, False), kwargs = {})\n",
      "    %conv2d_2 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout, %p_encoder_down_blocks_0_resnets_0_conv2_weight, %p_encoder_down_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d, %conv2d_2), kwargs = {})\n",
      "    %div : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add, 1.0), kwargs = {})\n",
      "    %group_norm_2 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div, 32, %p_encoder_down_blocks_0_resnets_1_norm1_weight, %p_encoder_down_blocks_0_resnets_1_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_2 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_2,), kwargs = {})\n",
      "    %conv2d_3 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_2, %p_encoder_down_blocks_0_resnets_1_conv1_weight, %p_encoder_down_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_3 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_3, 32, %p_encoder_down_blocks_0_resnets_1_norm2_weight, %p_encoder_down_blocks_0_resnets_1_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_3 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_3,), kwargs = {})\n",
      "    %dropout_1 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_3, 0.0, False), kwargs = {})\n",
      "    %conv2d_4 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_1, %p_encoder_down_blocks_0_resnets_1_conv2_weight, %p_encoder_down_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div, %conv2d_4), kwargs = {})\n",
      "    %div_1 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_1, 1.0), kwargs = {})\n",
      "    %pad : [num_users=1] = call_function[target=torch.ops.aten.pad.default](args = (%div_1, [0, 1, 0, 1], constant, 0.0), kwargs = {})\n",
      "    %conv2d_5 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%pad, %p_encoder_down_blocks_0_downsamplers_0_conv_weight, %p_encoder_down_blocks_0_downsamplers_0_conv_bias, [2, 2]), kwargs = {})\n",
      "    %group_norm_4 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_5, 32, %p_encoder_down_blocks_1_resnets_0_norm1_weight, %p_encoder_down_blocks_1_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_4 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_4,), kwargs = {})\n",
      "    %conv2d_6 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_4, %p_encoder_down_blocks_1_resnets_0_conv1_weight, %p_encoder_down_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_5 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_6, 32, %p_encoder_down_blocks_1_resnets_0_norm2_weight, %p_encoder_down_blocks_1_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_5 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_5,), kwargs = {})\n",
      "    %dropout_2 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_5, 0.0, False), kwargs = {})\n",
      "    %conv2d_7 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_2, %p_encoder_down_blocks_1_resnets_0_conv2_weight, %p_encoder_down_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_8 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%conv2d_5, %p_encoder_down_blocks_1_resnets_0_conv_shortcut_weight, %p_encoder_down_blocks_1_resnets_0_conv_shortcut_bias), kwargs = {})\n",
      "    %add_2 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_8, %conv2d_7), kwargs = {})\n",
      "    %div_2 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_2, 1.0), kwargs = {})\n",
      "    %group_norm_6 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_2, 32, %p_encoder_down_blocks_1_resnets_1_norm1_weight, %p_encoder_down_blocks_1_resnets_1_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_6 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_6,), kwargs = {})\n",
      "    %conv2d_9 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_6, %p_encoder_down_blocks_1_resnets_1_conv1_weight, %p_encoder_down_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_7 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_9, 32, %p_encoder_down_blocks_1_resnets_1_norm2_weight, %p_encoder_down_blocks_1_resnets_1_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_7 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_7,), kwargs = {})\n",
      "    %dropout_3 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_7, 0.0, False), kwargs = {})\n",
      "    %conv2d_10 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_3, %p_encoder_down_blocks_1_resnets_1_conv2_weight, %p_encoder_down_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_3 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_2, %conv2d_10), kwargs = {})\n",
      "    %div_3 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_3, 1.0), kwargs = {})\n",
      "    %pad_1 : [num_users=1] = call_function[target=torch.ops.aten.pad.default](args = (%div_3, [0, 1, 0, 1], constant, 0.0), kwargs = {})\n",
      "    %conv2d_11 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%pad_1, %p_encoder_down_blocks_1_downsamplers_0_conv_weight, %p_encoder_down_blocks_1_downsamplers_0_conv_bias, [2, 2]), kwargs = {})\n",
      "    %group_norm_8 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_11, 32, %p_encoder_down_blocks_2_resnets_0_norm1_weight, %p_encoder_down_blocks_2_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_8 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_8,), kwargs = {})\n",
      "    %conv2d_12 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_8, %p_encoder_down_blocks_2_resnets_0_conv1_weight, %p_encoder_down_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_9 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_12, 32, %p_encoder_down_blocks_2_resnets_0_norm2_weight, %p_encoder_down_blocks_2_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_9 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_9,), kwargs = {})\n",
      "    %dropout_4 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_9, 0.0, False), kwargs = {})\n",
      "    %conv2d_13 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_4, %p_encoder_down_blocks_2_resnets_0_conv2_weight, %p_encoder_down_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_14 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%conv2d_11, %p_encoder_down_blocks_2_resnets_0_conv_shortcut_weight, %p_encoder_down_blocks_2_resnets_0_conv_shortcut_bias), kwargs = {})\n",
      "    %add_4 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_14, %conv2d_13), kwargs = {})\n",
      "    %div_4 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_4, 1.0), kwargs = {})\n",
      "    %group_norm_10 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_4, 32, %p_encoder_down_blocks_2_resnets_1_norm1_weight, %p_encoder_down_blocks_2_resnets_1_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_10 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_10,), kwargs = {})\n",
      "    %conv2d_15 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_10, %p_encoder_down_blocks_2_resnets_1_conv1_weight, %p_encoder_down_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_11 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_15, 32, %p_encoder_down_blocks_2_resnets_1_norm2_weight, %p_encoder_down_blocks_2_resnets_1_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_11 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_11,), kwargs = {})\n",
      "    %dropout_5 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_11, 0.0, False), kwargs = {})\n",
      "    %conv2d_16 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_5, %p_encoder_down_blocks_2_resnets_1_conv2_weight, %p_encoder_down_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_5 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_4, %conv2d_16), kwargs = {})\n",
      "    %div_5 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_5, 1.0), kwargs = {})\n",
      "    %pad_2 : [num_users=1] = call_function[target=torch.ops.aten.pad.default](args = (%div_5, [0, 1, 0, 1], constant, 0.0), kwargs = {})\n",
      "    %conv2d_17 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%pad_2, %p_encoder_down_blocks_2_downsamplers_0_conv_weight, %p_encoder_down_blocks_2_downsamplers_0_conv_bias, [2, 2]), kwargs = {})\n",
      "    %group_norm_12 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_17, 32, %p_encoder_down_blocks_3_resnets_0_norm1_weight, %p_encoder_down_blocks_3_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_12 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_12,), kwargs = {})\n",
      "    %conv2d_18 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_12, %p_encoder_down_blocks_3_resnets_0_conv1_weight, %p_encoder_down_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_13 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_18, 32, %p_encoder_down_blocks_3_resnets_0_norm2_weight, %p_encoder_down_blocks_3_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_13 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_13,), kwargs = {})\n",
      "    %dropout_6 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_13, 0.0, False), kwargs = {})\n",
      "    %conv2d_19 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_6, %p_encoder_down_blocks_3_resnets_0_conv2_weight, %p_encoder_down_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_6 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_17, %conv2d_19), kwargs = {})\n",
      "    %div_6 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_6, 1.0), kwargs = {})\n",
      "    %group_norm_14 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_6, 32, %p_encoder_down_blocks_3_resnets_1_norm1_weight, %p_encoder_down_blocks_3_resnets_1_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_14 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_14,), kwargs = {})\n",
      "    %conv2d_20 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_14, %p_encoder_down_blocks_3_resnets_1_conv1_weight, %p_encoder_down_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_15 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_20, 32, %p_encoder_down_blocks_3_resnets_1_norm2_weight, %p_encoder_down_blocks_3_resnets_1_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_15 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_15,), kwargs = {})\n",
      "    %dropout_7 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_15, 0.0, False), kwargs = {})\n",
      "    %conv2d_21 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_7, %p_encoder_down_blocks_3_resnets_1_conv2_weight, %p_encoder_down_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_7 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_6, %conv2d_21), kwargs = {})\n",
      "    %div_7 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_7, 1.0), kwargs = {})\n",
      "    %group_norm_16 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_7, 32, %p_encoder_mid_block_resnets_0_norm1_weight, %p_encoder_mid_block_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_16 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_16,), kwargs = {})\n",
      "    %conv2d_22 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_16, %p_encoder_mid_block_resnets_0_conv1_weight, %p_encoder_mid_block_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_17 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_22, 32, %p_encoder_mid_block_resnets_0_norm2_weight, %p_encoder_mid_block_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_17 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_17,), kwargs = {})\n",
      "    %dropout_8 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_17, 0.0, False), kwargs = {})\n",
      "    %conv2d_23 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_8, %p_encoder_mid_block_resnets_0_conv2_weight, %p_encoder_mid_block_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_8 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_7, %conv2d_23), kwargs = {})\n",
      "    %div_8 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_8, 1), kwargs = {})\n",
      "    %view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%div_8, [1, 512, 4096]), kwargs = {})\n",
      "    %transpose : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view, 1, 2), kwargs = {})\n",
      "    %transpose_1 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%transpose, 1, 2), kwargs = {})\n",
      "    %group_norm_18 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%transpose_1, 32, %p_encoder_mid_block_attentions_0_group_norm_weight, %p_encoder_mid_block_attentions_0_group_norm_bias, 1e-06), kwargs = {})\n",
      "    %transpose_2 : [num_users=3] = call_function[target=torch.ops.aten.transpose.int](args = (%group_norm_18, 1, 2), kwargs = {})\n",
      "    %linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_2, %p_encoder_mid_block_attentions_0_to_q_weight, %p_encoder_mid_block_attentions_0_to_q_bias), kwargs = {})\n",
      "    %linear_1 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_2, %p_encoder_mid_block_attentions_0_to_k_weight, %p_encoder_mid_block_attentions_0_to_k_bias), kwargs = {})\n",
      "    %linear_2 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_2, %p_encoder_mid_block_attentions_0_to_v_weight, %p_encoder_mid_block_attentions_0_to_v_bias), kwargs = {})\n",
      "    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear, [1, -1, 1, 512]), kwargs = {})\n",
      "    %transpose_3 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_1, 1, 2), kwargs = {})\n",
      "    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_1, [1, -1, 1, 512]), kwargs = {})\n",
      "    %transpose_4 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_2, 1, 2), kwargs = {})\n",
      "    %view_3 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_2, [1, -1, 1, 512]), kwargs = {})\n",
      "    %transpose_5 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_3, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_3, %transpose_4, %transpose_5), kwargs = {})\n",
      "    %transpose_6 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention, 1, 2), kwargs = {})\n",
      "    %view_4 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_6, [1, -1, 512]), kwargs = {})\n",
      "    %_to_copy : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_4,), kwargs = {dtype: torch.float32})\n",
      "    %linear_3 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy, %p_encoder_mid_block_attentions_0_to_out_0_weight, %p_encoder_mid_block_attentions_0_to_out_0_bias), kwargs = {})\n",
      "    %dropout_9 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_3, 0.0, False), kwargs = {})\n",
      "    %transpose_7 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%dropout_9, -1, -2), kwargs = {})\n",
      "    %view_5 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_7, [1, 512, 64, 64]), kwargs = {})\n",
      "    %add_9 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_5, %div_8), kwargs = {})\n",
      "    %div_9 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_9, 1), kwargs = {})\n",
      "    %group_norm_19 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_9, 32, %p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_weight, %p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_18 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_19,), kwargs = {})\n",
      "    %conv2d_24 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_18, %p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_weight, %p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_20 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_24, 32, %p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_weight, %p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_19 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_20,), kwargs = {})\n",
      "    %dropout_10 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_19, 0.0, False), kwargs = {})\n",
      "    %conv2d_25 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_10, %p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_weight, %p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_10 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_9, %conv2d_25), kwargs = {})\n",
      "    %div_10 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_10, 1), kwargs = {})\n",
      "    %group_norm_21 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_10, 32, %p_encoder_conv_norm_out_weight, %p_encoder_conv_norm_out_bias, 1e-06), kwargs = {})\n",
      "    %silu_20 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_21,), kwargs = {})\n",
      "    %conv2d_26 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_20, %p_encoder_conv_out_weight, %p_encoder_conv_out_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_27 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%conv2d_26, %p_quant_conv_weight, %p_quant_conv_bias), kwargs = {})\n",
      "    %split : [num_users=1] = call_function[target=torch.ops.aten.split.Tensor](args = (%conv2d_27, 4, 1), kwargs = {})\n",
      "    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n",
      "    %conv2d_28 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%getitem, %p_post_quant_conv_weight, %p_post_quant_conv_bias), kwargs = {})\n",
      "    %conv2d_29 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%conv2d_28, %p_decoder_conv_in_weight, %p_decoder_conv_in_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_22 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_29, 32, %p_decoder_mid_block_resnets_0_norm1_weight, %p_decoder_mid_block_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_21 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_22,), kwargs = {})\n",
      "    %conv2d_30 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_21, %p_decoder_mid_block_resnets_0_conv1_weight, %p_decoder_mid_block_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_23 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_30, 32, %p_decoder_mid_block_resnets_0_norm2_weight, %p_decoder_mid_block_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_22 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_23,), kwargs = {})\n",
      "    %dropout_11 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_22, 0.0, False), kwargs = {})\n",
      "    %conv2d_31 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_11, %p_decoder_mid_block_resnets_0_conv2_weight, %p_decoder_mid_block_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_11 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_29, %conv2d_31), kwargs = {})\n",
      "    %div_11 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_11, 1), kwargs = {})\n",
      "    %view_6 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%div_11, [1, 512, 4096]), kwargs = {})\n",
      "    %transpose_8 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_6, 1, 2), kwargs = {})\n",
      "    %transpose_9 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%transpose_8, 1, 2), kwargs = {})\n",
      "    %group_norm_24 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%transpose_9, 32, %p_decoder_mid_block_attentions_0_group_norm_weight, %p_decoder_mid_block_attentions_0_group_norm_bias, 1e-06), kwargs = {})\n",
      "    %transpose_10 : [num_users=3] = call_function[target=torch.ops.aten.transpose.int](args = (%group_norm_24, 1, 2), kwargs = {})\n",
      "    %linear_4 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_10, %p_decoder_mid_block_attentions_0_to_q_weight, %p_decoder_mid_block_attentions_0_to_q_bias), kwargs = {})\n",
      "    %linear_5 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_10, %p_decoder_mid_block_attentions_0_to_k_weight, %p_decoder_mid_block_attentions_0_to_k_bias), kwargs = {})\n",
      "    %linear_6 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_10, %p_decoder_mid_block_attentions_0_to_v_weight, %p_decoder_mid_block_attentions_0_to_v_bias), kwargs = {})\n",
      "    %view_7 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_4, [1, -1, 1, 512]), kwargs = {})\n",
      "    %transpose_11 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_7, 1, 2), kwargs = {})\n",
      "    %view_8 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_5, [1, -1, 1, 512]), kwargs = {})\n",
      "    %transpose_12 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_8, 1, 2), kwargs = {})\n",
      "    %view_9 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_6, [1, -1, 1, 512]), kwargs = {})\n",
      "    %transpose_13 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_9, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_11, %transpose_12, %transpose_13), kwargs = {})\n",
      "    %transpose_14 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_1, 1, 2), kwargs = {})\n",
      "    %view_10 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_14, [1, -1, 512]), kwargs = {})\n",
      "    %_to_copy_1 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_10,), kwargs = {dtype: torch.float32})\n",
      "    %linear_7 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_1, %p_decoder_mid_block_attentions_0_to_out_0_weight, %p_decoder_mid_block_attentions_0_to_out_0_bias), kwargs = {})\n",
      "    %dropout_12 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_7, 0.0, False), kwargs = {})\n",
      "    %transpose_15 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%dropout_12, -1, -2), kwargs = {})\n",
      "    %view_11 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_15, [1, 512, 64, 64]), kwargs = {})\n",
      "    %add_12 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_11, %div_11), kwargs = {})\n",
      "    %div_12 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_12, 1), kwargs = {})\n",
      "    %group_norm_25 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_12, 32, %p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_weight, %p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_23 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_25,), kwargs = {})\n",
      "    %conv2d_32 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_23, %p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_weight, %p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_26 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_32, 32, %p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_weight, %p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_24 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_26,), kwargs = {})\n",
      "    %dropout_13 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_24, 0.0, False), kwargs = {})\n",
      "    %conv2d_33 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_13, %p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_weight, %p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_13 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_12, %conv2d_33), kwargs = {})\n",
      "    %div_13 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_13, 1), kwargs = {})\n",
      "    %_to_copy_2 : [num_users=2] = call_function[target=torch.ops.aten._to_copy.default](args = (%div_13,), kwargs = {dtype: torch.float32})\n",
      "    %group_norm_27 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%_to_copy_2, 32, %p_decoder_up_blocks_0_resnets_0_norm1_weight, %p_decoder_up_blocks_0_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_25 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_27,), kwargs = {})\n",
      "    %conv2d_34 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_25, %p_decoder_up_blocks_0_resnets_0_conv1_weight, %p_decoder_up_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_28 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_34, 32, %p_decoder_up_blocks_0_resnets_0_norm2_weight, %p_decoder_up_blocks_0_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_26 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_28,), kwargs = {})\n",
      "    %dropout_14 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_26, 0.0, False), kwargs = {})\n",
      "    %conv2d_35 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_14, %p_decoder_up_blocks_0_resnets_0_conv2_weight, %p_decoder_up_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_14 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%_to_copy_2, %conv2d_35), kwargs = {})\n",
      "    %div_14 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_14, 1.0), kwargs = {})\n",
      "    %group_norm_29 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_14, 32, %p_decoder_up_blocks_0_resnets_1_norm1_weight, %p_decoder_up_blocks_0_resnets_1_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_27 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_29,), kwargs = {})\n",
      "    %conv2d_36 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_27, %p_decoder_up_blocks_0_resnets_1_conv1_weight, %p_decoder_up_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_30 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_36, 32, %p_decoder_up_blocks_0_resnets_1_norm2_weight, %p_decoder_up_blocks_0_resnets_1_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_28 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_30,), kwargs = {})\n",
      "    %dropout_15 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_28, 0.0, False), kwargs = {})\n",
      "    %conv2d_37 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_15, %p_decoder_up_blocks_0_resnets_1_conv2_weight, %p_decoder_up_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_15 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_14, %conv2d_37), kwargs = {})\n",
      "    %div_15 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_15, 1.0), kwargs = {})\n",
      "    %group_norm_31 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_15, 32, %p_decoder_up_blocks_0_resnets_2_norm1_weight, %p_decoder_up_blocks_0_resnets_2_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_29 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_31,), kwargs = {})\n",
      "    %conv2d_38 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_29, %p_decoder_up_blocks_0_resnets_2_conv1_weight, %p_decoder_up_blocks_0_resnets_2_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_32 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_38, 32, %p_decoder_up_blocks_0_resnets_2_norm2_weight, %p_decoder_up_blocks_0_resnets_2_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_30 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_32,), kwargs = {})\n",
      "    %dropout_16 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_30, 0.0, False), kwargs = {})\n",
      "    %conv2d_39 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_16, %p_decoder_up_blocks_0_resnets_2_conv2_weight, %p_decoder_up_blocks_0_resnets_2_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_16 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_15, %conv2d_39), kwargs = {})\n",
      "    %div_16 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_16, 1.0), kwargs = {})\n",
      "    %upsample_nearest2d : [num_users=1] = call_function[target=torch.ops.aten.upsample_nearest2d.vec](args = (%div_16, None, [2.0, 2.0]), kwargs = {})\n",
      "    %conv2d_40 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%upsample_nearest2d, %p_decoder_up_blocks_0_upsamplers_0_conv_weight, %p_decoder_up_blocks_0_upsamplers_0_conv_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_33 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_40, 32, %p_decoder_up_blocks_1_resnets_0_norm1_weight, %p_decoder_up_blocks_1_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_31 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_33,), kwargs = {})\n",
      "    %conv2d_41 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_31, %p_decoder_up_blocks_1_resnets_0_conv1_weight, %p_decoder_up_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_34 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_41, 32, %p_decoder_up_blocks_1_resnets_0_norm2_weight, %p_decoder_up_blocks_1_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_32 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_34,), kwargs = {})\n",
      "    %dropout_17 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_32, 0.0, False), kwargs = {})\n",
      "    %conv2d_42 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_17, %p_decoder_up_blocks_1_resnets_0_conv2_weight, %p_decoder_up_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_17 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_40, %conv2d_42), kwargs = {})\n",
      "    %div_17 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_17, 1.0), kwargs = {})\n",
      "    %group_norm_35 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_17, 32, %p_decoder_up_blocks_1_resnets_1_norm1_weight, %p_decoder_up_blocks_1_resnets_1_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_33 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_35,), kwargs = {})\n",
      "    %conv2d_43 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_33, %p_decoder_up_blocks_1_resnets_1_conv1_weight, %p_decoder_up_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_36 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_43, 32, %p_decoder_up_blocks_1_resnets_1_norm2_weight, %p_decoder_up_blocks_1_resnets_1_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_34 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_36,), kwargs = {})\n",
      "    %dropout_18 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_34, 0.0, False), kwargs = {})\n",
      "    %conv2d_44 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_18, %p_decoder_up_blocks_1_resnets_1_conv2_weight, %p_decoder_up_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_18 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_17, %conv2d_44), kwargs = {})\n",
      "    %div_18 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_18, 1.0), kwargs = {})\n",
      "    %group_norm_37 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_18, 32, %p_decoder_up_blocks_1_resnets_2_norm1_weight, %p_decoder_up_blocks_1_resnets_2_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_35 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_37,), kwargs = {})\n",
      "    %conv2d_45 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_35, %p_decoder_up_blocks_1_resnets_2_conv1_weight, %p_decoder_up_blocks_1_resnets_2_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_38 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_45, 32, %p_decoder_up_blocks_1_resnets_2_norm2_weight, %p_decoder_up_blocks_1_resnets_2_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_36 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_38,), kwargs = {})\n",
      "    %dropout_19 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_36, 0.0, False), kwargs = {})\n",
      "    %conv2d_46 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_19, %p_decoder_up_blocks_1_resnets_2_conv2_weight, %p_decoder_up_blocks_1_resnets_2_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_19 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_18, %conv2d_46), kwargs = {})\n",
      "    %div_19 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_19, 1.0), kwargs = {})\n",
      "    %upsample_nearest2d_1 : [num_users=1] = call_function[target=torch.ops.aten.upsample_nearest2d.vec](args = (%div_19, None, [2.0, 2.0]), kwargs = {})\n",
      "    %conv2d_47 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%upsample_nearest2d_1, %p_decoder_up_blocks_1_upsamplers_0_conv_weight, %p_decoder_up_blocks_1_upsamplers_0_conv_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_39 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_47, 32, %p_decoder_up_blocks_2_resnets_0_norm1_weight, %p_decoder_up_blocks_2_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_37 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_39,), kwargs = {})\n",
      "    %conv2d_48 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_37, %p_decoder_up_blocks_2_resnets_0_conv1_weight, %p_decoder_up_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_40 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_48, 32, %p_decoder_up_blocks_2_resnets_0_norm2_weight, %p_decoder_up_blocks_2_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_38 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_40,), kwargs = {})\n",
      "    %dropout_20 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_38, 0.0, False), kwargs = {})\n",
      "    %conv2d_49 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_20, %p_decoder_up_blocks_2_resnets_0_conv2_weight, %p_decoder_up_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%conv2d_47,), kwargs = {memory_format: torch.contiguous_format})\n",
      "    %conv2d_50 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%clone, %p_decoder_up_blocks_2_resnets_0_conv_shortcut_weight, %p_decoder_up_blocks_2_resnets_0_conv_shortcut_bias), kwargs = {})\n",
      "    %add_20 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_50, %conv2d_49), kwargs = {})\n",
      "    %div_20 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_20, 1.0), kwargs = {})\n",
      "    %group_norm_41 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_20, 32, %p_decoder_up_blocks_2_resnets_1_norm1_weight, %p_decoder_up_blocks_2_resnets_1_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_39 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_41,), kwargs = {})\n",
      "    %conv2d_51 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_39, %p_decoder_up_blocks_2_resnets_1_conv1_weight, %p_decoder_up_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_42 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_51, 32, %p_decoder_up_blocks_2_resnets_1_norm2_weight, %p_decoder_up_blocks_2_resnets_1_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_40 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_42,), kwargs = {})\n",
      "    %dropout_21 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_40, 0.0, False), kwargs = {})\n",
      "    %conv2d_52 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_21, %p_decoder_up_blocks_2_resnets_1_conv2_weight, %p_decoder_up_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_21 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_20, %conv2d_52), kwargs = {})\n",
      "    %div_21 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_21, 1.0), kwargs = {})\n",
      "    %group_norm_43 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_21, 32, %p_decoder_up_blocks_2_resnets_2_norm1_weight, %p_decoder_up_blocks_2_resnets_2_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_41 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_43,), kwargs = {})\n",
      "    %conv2d_53 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_41, %p_decoder_up_blocks_2_resnets_2_conv1_weight, %p_decoder_up_blocks_2_resnets_2_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_44 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_53, 32, %p_decoder_up_blocks_2_resnets_2_norm2_weight, %p_decoder_up_blocks_2_resnets_2_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_42 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_44,), kwargs = {})\n",
      "    %dropout_22 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_42, 0.0, False), kwargs = {})\n",
      "    %conv2d_54 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_22, %p_decoder_up_blocks_2_resnets_2_conv2_weight, %p_decoder_up_blocks_2_resnets_2_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_22 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_21, %conv2d_54), kwargs = {})\n",
      "    %div_22 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_22, 1.0), kwargs = {})\n",
      "    %upsample_nearest2d_2 : [num_users=1] = call_function[target=torch.ops.aten.upsample_nearest2d.vec](args = (%div_22, None, [2.0, 2.0]), kwargs = {})\n",
      "    %conv2d_55 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%upsample_nearest2d_2, %p_decoder_up_blocks_2_upsamplers_0_conv_weight, %p_decoder_up_blocks_2_upsamplers_0_conv_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_45 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_55, 32, %p_decoder_up_blocks_3_resnets_0_norm1_weight, %p_decoder_up_blocks_3_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_43 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_45,), kwargs = {})\n",
      "    %conv2d_56 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_43, %p_decoder_up_blocks_3_resnets_0_conv1_weight, %p_decoder_up_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_46 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_56, 32, %p_decoder_up_blocks_3_resnets_0_norm2_weight, %p_decoder_up_blocks_3_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_44 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_46,), kwargs = {})\n",
      "    %dropout_23 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_44, 0.0, False), kwargs = {})\n",
      "    %conv2d_57 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_23, %p_decoder_up_blocks_3_resnets_0_conv2_weight, %p_decoder_up_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_58 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%conv2d_55, %p_decoder_up_blocks_3_resnets_0_conv_shortcut_weight, %p_decoder_up_blocks_3_resnets_0_conv_shortcut_bias), kwargs = {})\n",
      "    %add_23 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_58, %conv2d_57), kwargs = {})\n",
      "    %div_23 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_23, 1.0), kwargs = {})\n",
      "    %group_norm_47 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_23, 32, %p_decoder_up_blocks_3_resnets_1_norm1_weight, %p_decoder_up_blocks_3_resnets_1_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_45 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_47,), kwargs = {})\n",
      "    %conv2d_59 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_45, %p_decoder_up_blocks_3_resnets_1_conv1_weight, %p_decoder_up_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_48 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_59, 32, %p_decoder_up_blocks_3_resnets_1_norm2_weight, %p_decoder_up_blocks_3_resnets_1_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_46 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_48,), kwargs = {})\n",
      "    %dropout_24 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_46, 0.0, False), kwargs = {})\n",
      "    %conv2d_60 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_24, %p_decoder_up_blocks_3_resnets_1_conv2_weight, %p_decoder_up_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_24 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_23, %conv2d_60), kwargs = {})\n",
      "    %div_24 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_24, 1.0), kwargs = {})\n",
      "    %group_norm_49 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_24, 32, %p_decoder_up_blocks_3_resnets_2_norm1_weight, %p_decoder_up_blocks_3_resnets_2_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_47 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_49,), kwargs = {})\n",
      "    %conv2d_61 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_47, %p_decoder_up_blocks_3_resnets_2_conv1_weight, %p_decoder_up_blocks_3_resnets_2_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_50 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_61, 32, %p_decoder_up_blocks_3_resnets_2_norm2_weight, %p_decoder_up_blocks_3_resnets_2_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_48 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_50,), kwargs = {})\n",
      "    %dropout_25 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_48, 0.0, False), kwargs = {})\n",
      "    %conv2d_62 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_25, %p_decoder_up_blocks_3_resnets_2_conv2_weight, %p_decoder_up_blocks_3_resnets_2_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_25 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_24, %conv2d_62), kwargs = {})\n",
      "    %div_25 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_25, 1.0), kwargs = {})\n",
      "    %group_norm_51 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_25, 32, %p_decoder_conv_norm_out_weight, %p_decoder_conv_norm_out_bias, 1e-06), kwargs = {})\n",
      "    %silu_49 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_51,), kwargs = {})\n",
      "    %conv2d_63 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_49, %p_decoder_conv_out_weight, %p_decoder_conv_out_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    return (conv2d_63,)\n",
      "VAE exported ➜  vae_fp32.ep\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from diffusers import AutoencoderKL\n",
    "# from torch.export import export\n",
    "# import inspect\n",
    "\n",
    "# CKPT = \"prs-eth/marigold-depth-v1-1\"\n",
    "\n",
    "# vae = AutoencoderKL.from_pretrained(CKPT, subfolder=\"vae\").cpu()\n",
    "# vae.eval()                                   # switch off dropout\n",
    "# class VAEEncodeDecode(torch.nn.Module):\n",
    "#     def __init__(self, core):\n",
    "#         super().__init__()\n",
    "#         self.core = core\n",
    "#     def forward(self, x):\n",
    "#         # Encode RGB → latent\n",
    "#         lat = self.core.encode(x).latent_dist.sample()\n",
    "#         # Decode latent → reconstruction\n",
    "#         img = self.core.decode(lat).sample\n",
    "#         return lat, img            # returns a tuple\n",
    "\n",
    "# wrapper = VAEEncodeDecode(vae)\n",
    "# rgb = torch.randn(1, 3, 512, 512)\n",
    "# gm = export(wrapper, (rgb,))        # gm is already a GraphModule\n",
    "# gm.graph.print_tabular()            # pretty table\n",
    "# # or just:\n",
    "# print(gm.graph)                   # raw text\n",
    "\n",
    "  \n",
    "import torch\n",
    "from diffusers import AutoencoderKL\n",
    "from torch.export import export\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "        \"prs-eth/marigold-depth-v1-1\", subfolder=\"vae\").cpu().eval()\n",
    "\n",
    "rgb = torch.randn(1, 3, 512, 512)\n",
    "\n",
    "# AutoencoderKL.forward does: encode → (optionally sample) → decode\n",
    "gm_vae = export(vae, (rgb,))            # default sample_posterior = False\n",
    "gm_vae.graph.print_tabular()            # full encoder + decoder graph\n",
    "print(gm_vae.graph)\n",
    "\n",
    "save(gm_vae, \"vae_fp32.ep\")\n",
    "print(\"VAE exported ➜  vae_fp32.ep\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "428e44e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max |Δ| = tensor(0., grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "ep_loaded = load(\"vae_fp32.ep\")   # the ExportedProgram\n",
    "\n",
    "gm = ep_loaded.module()           # ← call with no args, returns GraphModule\n",
    "out_graph = gm(rgb)               # run the graph\n",
    "out_eager = vae(rgb)              # original eager result\n",
    "\n",
    "print(\"max |Δ| =\", (out_graph.sample - out_eager.sample).abs().max())  # ≈ 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d4abe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marigold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
